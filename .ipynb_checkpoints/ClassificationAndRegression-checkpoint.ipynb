{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYS-STK4155 Project #2 - Classification and Regression\n",
    "\n",
    "Evaluation of Project number: 1 <br />\n",
    "Name: Lennart Lehmann (ERASMUS Student - UiO Code lennarl)\n",
    "\n",
    "All the Code and data can be found in [my Github Repository](https://github.com/lenlehm/Classification-and-Regression). <br />\n",
    "\n",
    "Comment your results and give a critical discussion of the results obtained\n",
    "with the Logistic Regression code and your own Neural Network code. Make\n",
    "an analysis of the regularization parameters and the learning rates employed to\n",
    "find the optimal accurary score.\n",
    "\n",
    "\n",
    "## Abstract \n",
    "\n",
    "** DO NOT FORGTE THIS SHIT HERE **\n",
    "\n",
    "\n",
    "## 1.) Introduction\n",
    "\n",
    "Neural networks have often been in the news and literally experienced a hype because of that. Many companies and people fear that Artificial Intelligence (AI) will eventually lead to machines taking over the world. This scenario is called *technological singularity* [1] and many famous people like Elon Musk or the deceased Stephen Hawkins warn about AI. [2] There also have been many popular movies depicting this machine dominating future such as *I Robot, Terminator, Ex Machina* and co. \n",
    "However, the most frequent use-case of neural networks are still classification and regression problems. <br />\n",
    "The rise of Deep Neural Networks emerged in 2012, where the winner of 2012 scored tremendous results on the *[ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://image-net.org)* by almost halving the Top-5-Error. [3] <br /> \n",
    "Keep in mind that the Error for this challenge almost remained constant over the last years. This challenge focuses on object classification and localization with over 200 distinct objects, in fact an image recognition task. This breakthrough in 2012 lead to rapid advances in AI, especially Image Classification, where only 3 years later, in 2015, human performance was already achieved. [3] <br />\n",
    "This rapid progress was utilized for similiar fields such as Regression or classification of non-images across every industry branch. [3] <br />\n",
    "Nowadays, these algorithms are used to detect cancer in medical images, or to perceive the environment of various agents, such as Autonomous Cars/ Drones or even robots. [3] <br />\n",
    "Classification is different from regression problems in the sense that algorithm's result can only take values across the classes that means to be classified. Hence, Linear Regression is not suitable for classification tasks, that is why this work focuses on Logistic Regression as well as Neural Networks (NN) for the classification of Credit Card data. <br />\n",
    "Neural networks are a subfield of Artifical Intelligence and these algorithms model a complex function to represent the dataset that was given during training. During training the model learns the correlation of the input and its corresponding output. These trained models are then used to predict new, unknown datapoints that need to be classified. <br />\n",
    "Another advantage of Neural Networks is that they are also neat for Regression problems, thus this work will make use of Neural Networks as well Logistic Regression for Classification and Regression. <br />\n",
    "Both of these methods are supervised learning techniques, where a dataset comprising inputs and its corresponding targets/ outputs are necessary. The algorithm learns from the input corresponding target for each of the datapoints given. The targets differentiate for classification problems and regression problems, i.e. classification targets are discrete variables like the class names, such as *Cat* or *Dog*, when classifying images for cat or dog images repectively. Targets for regression problems are numerical values such as Stock market prices or housing prices.\n",
    "\n",
    "** EXPLAIN THE CLASSIFICATION AND REGRESSION DATASETS AND TASKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Theory\n",
    "\n",
    "Since this work will make us of *Logistic Regression* as well as *Neural Networks*, the mathematical foundations for each of this algorithms is explained in the following section. <br />\n",
    "Furthermore, the second part of this project deals with Regression, where the two above mentioned algorithms are tested against the common Regression methods such as *Ordinary Least Squares, Ridge* and *Lasso*. I explained those in my [first project](https://github.com/lenlehm/Regression-and-Resampling/blob/master/RegressionAnalysisAndResampling.ipynb), so feel free to refresh those if you feel like it. <br />\n",
    "\n",
    "### 2.1) Logistic Regression\n",
    "\n",
    "Logistic regression can be considered a special case of linear regression with the neat benefit of simple probabilistic results for classification. Such a model specifies that an appropriate function of the fitted probability of the event is a linear function of the observed values $X$ of the available predictors $p$.\n",
    "Unfortunately, this simple model cannot properly deal with the problems of non-linear and interactive effects of the predictor variables, such as normalization of the data. [4]\n",
    "<br />\n",
    "Assuming we have a dataset $\\mathcal{D} = \\{(\\boldsymbol{x^{(i)}}, y_i)\\}^n_{i=1}$, where we have $p$ predictors for each data sample $\\boldsymbol{x^{i}} = \\{x^{(i)}_1, \\ldots, x^{(i)}_p\\}$. <br />\n",
    "The output $y_i$ are discrete values and can take values from $k = 0, 1, \\ldots, K-1$, for $K$ classes. <br />\n",
    "Classification problems try to predict the output classes $k_i$ for the given $n$ samples comprising the $p$ predictors. <br />\n",
    "Logistic Regression usually handles *binary classification* problems by assigning a probability to each of the two classes, thus meaning there are only two possible outcomes/ classes $y_i \\in \\{0, 1\\}$ with each a probability of $p(y|x) \\in \\{0, \\ldots, 1\\}$. The following of this section will be according to the *binary classification* problem.\n",
    "Let $p(y|x)$ denote the probabilty of the outcome $y$ given $x$, then the logistic model reads as follows: <br />\n",
    "\n",
    "$$\n",
    "p(y=1 | \\boldsymbol{x}, \\beta) = \\frac{1}{1 + e^{-\\beta \\cdot \\boldsymbol{x}}}, \\tag{1}\n",
    "$$\n",
    "<br />\n",
    "$$\n",
    "p(y=0 | \\boldsymbol{x}, \\beta) = 1 - p(y=1 | \\boldsymbol{x}, \\beta). \\tag{2}\n",
    "$$\n",
    "\n",
    "Note that $1-p(x) = p(-x)$. <br />\n",
    "The paramters of the mode are denoted with $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)$. \n",
    "The term $\\beta \\cdot \\boldsymbol{x} = \\beta_0 + \\sum_{k=1}^p \\beta_k x_k$ is known as the log-odds and the following function is called the *sigmoid* of $x$: <br />\n",
    "\n",
    "$$\n",
    "\\sigma(\\boldsymbol{x}) = \\frac{1}{1 + e^{-\\boldsymbol{x}}}, \\tag{3}\n",
    "$$\n",
    "\n",
    "Now the logistic model can predict a class $\\hat{y}_i$ by utilizing the estimated probabilities $p(y | x)$: <br />\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\begin{cases} 1 & if \\quad p(y=1 | \\boldsymbol{x}^{(i)} \\geq 0.5 \\\\\n",
    "0 & if \\quad p(y=1 | \\boldsymbol{x}^{(i)} < 0.5.\n",
    "\\end{cases} \\tag{4}\n",
    "$$\n",
    "\n",
    "<br />\n",
    "For training of the logistic model, *maximum likelihood* is used. Under the i.i.d. assumption (identically independent distributed) the likelohood is given by: [6] <br />\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i:y_i=1} p(y_i = 1 | \\boldsymbol{x}^{(i)}) \\quad \\prod_{i:y_i=0} p(y_i = 0 | \\boldsymbol{x}^{(i)}) \\tag{5}\n",
    "$$\n",
    "$$\n",
    " = \\prod_{i=1}^n p_i^{y_i}(1 - p_i)^{1-y_i}, \\tag{6}\n",
    "$$\n",
    "\n",
    "where $p_i = p(y_i = 1 | \\boldsymbol{x}^{(i)}) = \\sigma(\\beta \\boldsymbol{x}^{(i)}).$\n",
    "\n",
    "The parameters $\\beta$ are chosen to maximize the likelihood.\n",
    "For the sake of simplicity, the maximum likelihood is often rewritten to the *log-likelihood* to turn the productions into summations: <br />\n",
    "\n",
    "$$\n",
    "\\ell{(\\beta)} = log(L(\\beta)) = \\sum_{i=1}^n y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i). \\tag{7}\n",
    "$$\n",
    "\n",
    "Since the logarithmus function is monotonous, maximizing the logarithm of a function is equivalent to maximizing the function itself. Hence $\\beta$ maximizes both, the log-likelihood along with the likelihood itself. <br />\n",
    "Lastly, the loss function for logistic regression is defined by the *binary cross-entropy* which is denoted as follows: <br />\n",
    "\n",
    "$$\n",
    "C(\\beta) = - \\sum_{i=1}^n y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i). \\tag{8}\n",
    "$$\n",
    "\n",
    "Minimizing the binary cross-entropy loss (equation (8)) yield the optimal paramters $\\beta$. One can extend the binary cross-entropy equation by regularizing it with the $L^1, L^2$ or $L^{\\infty}$ - Norm. Considering the most common $L^2$ Regularization, (8) can be rewritten as: <br />\n",
    "\n",
    "$$\n",
    "C_{L^2}(\\beta) =  - \\sum_{i=1}^n y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i) + \\frac{\\lambda}{2}||\\beta||^2, \\tag{9}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularizing parameter and needs to follow: $\\lambda > 0$. $L^2$ Regularization is more stable than its counterpart $L^1$, since it has a continuous derivative. However, since we square the differences, [outliers are more sensitive](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0) in the $L^2$ regularization. [12]<br />\n",
    "However, finding an analytical solution for the minimization problem is not possible. Making use of numerical optimization algorihtms like the notorious *(stochastic) gradient descent* eradicates this problem. <br /> \n",
    "Looking at the gradient of the binary cross-entropy: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(\\beta)}{\\partial \\beta_j} = - \\sum_{i=1}^n \\boldsymbol{x}_j^{(i)} \\cdot (y^{(i)} - p_i), \\tag{10}\n",
    "$$\n",
    "\n",
    "which can be written in matrix form: <br />\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta C(\\beta) = -X^T \\cdot (\\boldsymbol{y - p}). \\quad \\quad X \\in \\mathbb{R}^{n\\times(p+1)} \\tag{11}\n",
    "$$\n",
    "\n",
    "$X$ being the design-matrix containing $\\boldsymbol{x}^{(i)}$ as its i-th row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convexity\n",
    "\n",
    "A neat feature of convex function is that any local minimm is also a global minimum. Hence, if this function exhibits a minimum, one can say that this minimum is global and thus resulting in the optimal solution. Convexity is guaranteed (for multivariate functions) if the corresponding Hessian matrix of the second partial derivatives is PSD (positive semi-definite).<br />\n",
    "Showing that binary cross-entropy's (8) second partial derivatives is PSD, proves that this cost function is convex: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 C(\\beta)}{\\partial \\beta_k \\partial \\beta_j} = - \\sum_{i=1}^n \\boldsymbol{x}_j^{(i)} \\cdot \\frac{\\partial (y^{(i)} - p_i)}{\\partial \\beta_k}, \\tag{12}\n",
    "$$\n",
    "\n",
    "$$\n",
    " = - \\sum_{i=1}^n \\boldsymbol{x}_j^{(i)} \\cdot \\boldsymbol{x}_k^{(i)} \\cdot p_i(p_i - 1). \\tag{13}\n",
    "$$\n",
    "\n",
    "Again (13) can be rewritten in matrix form: <br />\n",
    "\n",
    "$$\n",
    "\\nabla^2_\\beta C(\\beta) =  - \\sum_{i=1}^n \\boldsymbol{x}^{(i)} \\cdot (\\boldsymbol{x}^{(i)})^T \\cdot p_i(p_i - 1). \\tag{14}\n",
    "$$\n",
    "\n",
    "A matrix $M \\in \\mathbb{R}^{n \\times n}$ is PSD, iff  $\\boldsymbol{z}^T Az \\geq 0, \\quad \\forall \\boldsymbol{z} \\in \\mathbb{R}^n$.\n",
    "\n",
    "Thus, we get following expression: <br />\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z}^T\\nabla_\\beta^2 C(\\beta)\\boldsymbol{z} =  - \\sum_{i=1}^n \\boldsymbol{z}^T \\cdot \\boldsymbol{x}^{(i)} \\cdot (\\boldsymbol{x}^{(i)})^T \\cdot \\boldsymbol{z} \\cdot p_i(p_i - 1) \\tag{15}\n",
    "$$\n",
    "\n",
    "$$\n",
    " = \\sum_{i=1}^n || (\\boldsymbol{x}^{(i)})^T \\boldsymbol{z} ||^2 \\cdot p_i(1 - p_i) \\geq 0. \\tag{16}\n",
    "$$\n",
    "\n",
    "The inequality follwos due to the sum of non-negative terms. \n",
    "Hence, the Hessian of the binary cross-entropy is a convex function meaning that a local minimum is also a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Neural Networks\n",
    "\n",
    "Neural networks, as the name already hint, are supposed to mimic the human brain. Recent advances in Neural networks, especially Deep Neural Networks (DNN), was a breakthrough of these algorithms. These networks consist of an input layer, where the data is fed in, arbitrary many hidden layers as well as an output layer (see Fig. 1).\n",
    "![DNN Architecture](plots/deepLearning.PNG \"DNN architecture\")\n",
    "*Fig. 1 Systematic architecture of a DNN with three hidden layers (blue rectangles) and four neurons or units per hidden layer (white circles inside blue rectangle). (Source: [7](https://github.com/lenlehm/Classification-and-Regression/blob/master/E04-Deep_Learning.pdf))* <br />\n",
    "A neural net is considered *deep*, when it has multiple hidden layers, thus the depicted network is considered a *deep neural network* since it utilizes three hidden layers. <br /> \n",
    "Each of the inputs and neurons respectively have so-called *weights* $\\boldsymbol{W}$ along with biases $b$ per neuron that it counts to optimize. In Fig. 1 each of the edges you see from one neuron to another has a specific weight $w_{l,i}$, where $l \\in (0, 1, \\cdots, L)$ denotes the current layer of maximum layer size $L$ and $i \\in (0, 1, \\cdots, N)$ describes the respective neuron in that layer, where $N$ is the last Neuron in that specified layer $l$. The biases are not depicted in Fig. 1, but there is one bias value per neuron, thus resulting in $b \\in (0, 1, \\cdots, N)$ Biases per layer $l$ with $N$ neurons.<br /> \n",
    "This architecture is also called *Mulit-layered Perceptrons (MLP)* where a MLP is build from layers of connected neurons.\n",
    "The input of the network is propagated through the layers and processed by each neuron in the network. That is also the reason why these networks are called *feed forward neural networks (FFNN)*, because the information flows through the network in forward direction (from input through layers to output - see Fig. 1). <br /> \n",
    "The network itself outputs a value for a single neuron output i.e. binary classification or regression, or a vector for multi-class classification i.e. 200 classes in the ImageNet challenge (ILSRVC, see Introduction). <br /> \n",
    "As you probably could already tell, the parameters of these network explode, the deeper (more layers) and wider (more neurons per layer) we get. For instance the DNN in Fig. 1 has a 3D input $(x_1, x_2, x_3)$ and a 2D output $(y_1, y_2)$. In between it has 3 hidden layers with 4 neurons each layer, thus resulting in 64 parameters: <br />\n",
    "$ (3 \\times 4)_{\\boldsymbol{W_0}} \\cdot (4 \\times 4)_{\\boldsymbol{W_1}} \\cdot (4 \\times 4)_{\\boldsymbol{W_2}} \\cdot (4 \\times 2)_{\\boldsymbol{W_3}} + (3 \\times 4)_{\\boldsymbol{B}} = 64.$\n",
    "\n",
    "Note, that the addition at the end depicts the Bias vector, each neuron has a single bias value. Since we have 3 (layers) $\\times$ 4 (neurons in each layer), we have to add 12 parameters for the biases. <br /> \n",
    "This parameter space blows up pretty fast, especially considering complex tasks, where the input is higher dimensional such as 100-D input, which is not uncommon for real-world applications. <br />\n",
    "In order to optimize for the desired weights and biases an efficient algorithm for calculating the gradient of the entire function with repsect to the parameters is necessary. <br /> \n",
    "Here comes the *backpropagation* very handy.\n",
    "\n",
    "#### Backpropagation \n",
    "\n",
    "As the name already states, there is not only a forward propagation, but also a *backpropagation*. This can olny be applied when a forward pass was done previously, since the calculated outut is necessary to start the backward propagation. During the forward pass the network calculates the outputs of each layer with respect to the activation function $A$: <br />\n",
    "\n",
    "$$\n",
    "O_1 = A(\\boldsymbol{W_0}^T \\cdot I), \\quad \\quad where \\quad I = Input. \\tag{17}\n",
    "$$\n",
    "\n",
    "The activation function \"activates\" the neurons, which can also be rewritten with: <br /> \n",
    "\n",
    "$$\n",
    "z_j^l = \\sum_{i=1}^n w_{ij}^l\\cdot x_i + b_j^l \\tag{18} \n",
    "$$ \n",
    "\n",
    "as <br /> \n",
    "\n",
    "$$\n",
    "a_j^l = f_l(z_j^l) = f_l(\\sum_{i=1}^n w_{ij}^l \\cdot x_i + b_j^l). \\tag{19}\n",
    "$$\n",
    "\n",
    "This is done for each layer, until arriving at the output layer. After the forward pass is done, the cost function is calculated along with its derivative w.r.t. the weights and biases in the output layer $W^L$. <br /> \n",
    "Luckily, through the Chain rule of the gradients this allows us to chain the following gradients in the following manner: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial w_{jk}^L} = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L)}{\\partial w_{jk}^L} \\\\\n",
    "\\quad = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L)}{\\partial z_j^L} \\cdot \\frac{\\partial z_j^L)}{\\partial w_{jk}^L} \\\\\n",
    "\\quad = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot f'_L(z_j^L)a_k^{L-1}, \\tag{20}\n",
    "$$\n",
    "\n",
    "where <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial a_{j}^L} = \\frac{\\partial}{\\partial a_{j}^L} \\cdot \\left[\\frac{1}{2} \\sum_{i=1}^N (a_j^L - t_j)^2\\right] \\\\\n",
    "= a_j^L - t_j, \\tag{21}\n",
    "$$\n",
    "\n",
    "and <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^L_j}{\\partial w_{jk}^L} = \\frac{\\partial}{\\partial w_{jk}^L} \\cdot \\left[\\sum_{p=1}^N w_{jp}^L \\cdot a^{L-1}_j + b_j^L\\right] \\\\\n",
    "= a_j^{L-1}. \\tag{22}\n",
    "$$\n",
    "\n",
    "Defining everything except $a_k^{L-1}$ in (20) as $\\delta_j^L$ we end up in : <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial w_{jk}^L} = \\delta_j^L \\cdot a_k^{L-1}. \\tag{23}\n",
    "$$\n",
    "\n",
    "Guess what's up next: the lovely Chain rule ... again (applied to $\\delta_j^L$). <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^L = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\frac{\\partial f^L}{\\partial z_j^L} = \\frac{\\partial C(W^L)}{\\partial a_j^L} \\frac{\\partial a_j^L}{\\partial z_j^L} \\\\ \n",
    "= \\frac{\\partial C(W^L)}{\\partial z_{j}^L} = \\frac{\\partial C(W^L)}{\\partial b_{j}^L} \\frac{\\partial b_j^L}{\\partial z_{j}^L} \\\\\n",
    "= \\frac{\\partial C(W^L)}{\\partial b_{j}^L}, \\tag{24}\n",
    "$$\n",
    "\n",
    "where making use of: <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_j^L}{\\partial z_{j}^L} = \\left[\\frac{\\partial z_j^l}{\\partial b_j^L} \\right]^{-1} \\\\\n",
    "= \\left[\\frac{\\partial}{\\partial b_j^L} \\sum_{i=1}^N{L-1} w_{ij}^L \\cdot a_i^{L-1} + b_j^L \\right]^{-1} \\\\\n",
    "= 1. \\tag{25}\n",
    "$$\n",
    "\n",
    "These are the derivatives of the cost function w.r.t. both weights (20) and biases (25) in the output layer $W^L$ and $\\boldsymbol{b}^L$. <br /> \n",
    "The following equation holds for any layer, except the output layer: <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l}. \\tag{26}\n",
    "$$\n",
    "\n",
    "Connecting this to the derivatives w.r.t. the next layer $l+1$: <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l} = \\sum_k \\frac{\\partial C}{\\partial z_k^{l+1}} \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n",
    "= \\sum_k \\delta_k^{l+1} \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n",
    "= \\sum_k \\delta_k^{l+1} \\cdot w^{l+1}_{kj} \\cdot \\frac{\\partial f^l}{\\partial z_j^l}, \\tag{27}\n",
    "$$\n",
    "\n",
    "with <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} = \\frac{\\partial}{\\partial z_j^l} \\left[\\sum_{i=1}^{N_l} w^{l+1}_{ik} a_k^l + b_k^{l+1} \\right] \\\\\n",
    "= \\frac{\\partial}{\\partial z_j^l} \\left[\\sum_{i=1}^{N_l} w^{l+1}_{ik} f^l(z_k^l) + b_k^{l+1} \\right] \\\\\n",
    "= w_{jk}^{l+1} \\cdot f^l(z_j^l). \\tag{28}\n",
    "$$\n",
    "\n",
    "Backpropagation is usually iterating Equation (27) and computing the gradients $\\partial C / \\partial w_{ij}^l$ and $\\partial C / \\partial b_i^l$ for each layer. <br /> \n",
    "\n",
    "#### Cost Functions\n",
    "\n",
    "All the above mentioned formulas share the same cost function C. \n",
    "The cost function is really important in the learning step since it is directly correlated with the accuracy. Finding an optimal cost function is often not easy. <br /> \n",
    "The cost function is an indicator of how well the network is doing. High loss indicates that the model is far away from the true values. Likewise, a low loss means that the model can fit well on the data and thus a low loss is desirable. <br />\n",
    "Most popular and common cost functions are *Mean-Squarred Error*, which is defined as: [6] <br />\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2, \\tag{29}\n",
    "$$\n",
    "\n",
    "and  *Cross Entropy*: [8] <br />\n",
    "\n",
    "$$\n",
    "CE = - \\sum_{x \\in \\mathcal{X}} p(x) log(q(x)), \\tag{30}\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are discrete probability distribution with the same $\\mathcal{X}$. <br />\n",
    "When dealing with a binary classification problem, where only two classes are present one could simplify (30) to: <br />\n",
    "\n",
    "$$\n",
    "CE_{binary} = - \\frac{1}{N} \\sum_{i = 1}^N y_i log(p(y_i)) + (1-y_i) log( 1 - p(y_i)). \\tag{31}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "When the gradients are known, the paramters can be updated by gradient descent according to following formula: <br /> \n",
    "\n",
    "$$\\\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\tau \\nabla C(\\theta^{(t)}), \\quad \\tau > 0 \\tag{32}\n",
    "$$\n",
    "\n",
    "for timestep $t$ and $\\tau$ being the well-known and headache provoking *learning-rate*. Too small values for $\\tau$ (< 0.000001) will take years until convergence is reached. However, too big values for $\\tau$ (> 0.01) and again no convergence is reached, since the updates are too big and the algorithm jitters around. [3]\n",
    "Since the aim is to minimize and go towards the negative gradient (descent) we have to put a \"minus\" in the update formula (32). Gradient descent is the most common optimization algorithm in machine learning, due to its performance and simplicity. A model $m(\\theta)$ is fit on a dataset $X$ with a cost function $C(X, m(\\theta))$, which evaluates the model on the underlying observations $X$. The model is fit by calculating the gradients and thus finding the optimal parameters $\\theta$ that minimize the cost function $C(X, m(\\theta)$. <br /> \n",
    "As everything in life this method also has its drawbacks. There are usually more non-convex, high-dimensional cost functions that often result in local minima instead of a global minimum. Here the inital guess (timestep $t = 0$) of $\\theta^0$ is crucial for the performance of the Gradient descent and thus sensitive to the inital guess. <br /> \n",
    "As mentioned earlier, Gradient Descent is very sensitive for the learning rate $\\tau$. There is a lot of research to find the optimal learning rate. Most of the recent published work, utilizes an adaptive learning rate, where the initial learning rate is high, but with each epoch the learning rate is reduced until it gets very small. <br /> \n",
    "Additionally the gradient is a function of $\\boldsymbol{x} = (x_1, \\ldots, x_n)$, which makes it expensive to compute numerically. <br /> \n",
    "One can alleviate the shortcomings by introducing randomness, i.e. when training in batches such as the Stochastic Gradient Descent (SGD). <br /> \n",
    "\n",
    "#### Stochastic Gradient Descent \n",
    "\n",
    "Stochastic Gradient Descent (SGD) address the drawbacks of vanilla gradient descent. The idea behind SGD is that the cost function can mostly be rewritten as a sum over datapoints: <br />\n",
    "\n",
    "$$\n",
    "C(\\theta) = \\sum_{i=1}^n c_i(\\boldsymbol{x}_i, \\theta). \\tag{33}\n",
    "$$\n",
    "\n",
    "Since the gradient is the working horse, this can also be computed as the sum over i-gradients: <br /> \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta C(\\theta) = \\sum_i^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta). \\tag{34}\n",
    "$$\n",
    "\n",
    "Randomness is added by only taking the gradient on a subset of the data, often referred to *minibatches*. Assuming $n$ datapoints and the size of minibatches $M$, there will be $\\frac{n}{M}$ minibatches. In the following of this report, the minibatches are denoted by $B_k$ where $k = 1, \\ldots, \\frac{n}{M}$. For instance one chooses $M = n$, yieldig a single datapoint in the minibatch: $B_k = \\boldsymbol{x}_k$, or $M=1$, then there is only one (Mini-)Batch $B_1$ containing all datapoints. <br /> \n",
    "Approximating the gradient by replacing the sum over all datapoints by the sum over a randomly picked minibatch in each gradient descent step: <br /> \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta C(\\theta) = \\sum_{i=1}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta) \\to \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta). \\tag{35}\n",
    "$$\n",
    "\n",
    "Accordingly an update step now looks as follows: \n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\tau \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta), \\tag{36}\n",
    "$$\n",
    "\n",
    "where each minibatch $B_k$ is picked randomly with equal probability from the interval $[1, \\frac{n}{M}]$. One iteration over all minibatches is known as *epoch*. Hence, it is common to choose a number of epochs instead of iterating over minibatches. <br /> \n",
    "Taking the gradient on a subset of the data introduces not only radomness, which decreases the chances to get stuck in a local minimum, but also has some computational benefits, if the minibatch size are relatively small to the number of datapoints. Common sizes of Minibatches start from [16, 32, 64, 128, 256, 512], depending on the dataset at hand.\n",
    "\n",
    "#### Adaptive Moment Estimation (ADAM)\n",
    "\n",
    "[ADAM](https://arxiv.org/abs/1412.6980) is another optimization algorithm, that was introduced in 2015 on the International Conference on Learning Representations (ICLR). It was a huge achievement, by boosting the performance of the optimization problems. <br />\n",
    "SGD maintains a single learning rate $\\tau$ for all update steps and thus the learning rate does not change during training.\n",
    "ADAM instead computes adaptive learning rates for different parameters from estimates of first and second moments (*momentum term*) of the gradients. <br />\n",
    "By keeping a part of the change at the previous timestep, thus giving the optimization a momentum to accelerate the minimization in parameter space directions in which the gradient is not steep, but consistently has a small value steadily in one direction. Each minibatch changes to: [9] <br /> \n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\left[\\eta \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta^{(t-1)} \\right] - \\tau \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta^{(t)}), \\tag{37}\n",
    "$$\n",
    "\n",
    "with the momentum parameter $\\eta$, usually close to 1.0, i.e. $\\eta = 0.95$. In general, using past moments of the previously calculated iterations as a guide for the current gradient step to enhance the performance. <br /> \n",
    "ADAM uses a exponentially decaying average of the first and second memoments of the gradient to compute individual adaptive learning rates for each parameter independently. [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "The previous Section dealt with the Backpropagation and Gradient descent. However, to calculate a proper gradient, we utilize an activation function $f_l(x)$. In order to introduce non-linearities so that the model can also represent complex datasets, employing a non-linear activation function is essential. <br /> \n",
    "It is required that these functions are continous and differentiable in order for the backpropagation to work. [8] <br >\n",
    "Since the feedforward is just a nested function of the inputs times the activation functions, it can be written as: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{j=1}^M w_{1j}^L f_L \\left(\\sum_{i=1}^M w_{ji}^{L-1} f_{L-1} \\left(\\sum_{k=1}^M w_{ik}^{L-2} f_{L-2} \\left( \\ldots f_1(w_{m1}^1 x_1 + b_m^1) \\ldots \\right) +b_k^{L-2} \\right) + b_i^{L-1} \\right) + b_1^L. \\tag{38}\n",
    "$$\n",
    "\n",
    "The simplest activation function would be the identity transformation $f_I(x) = x$, which is often used for regression networks in the output layer.\n",
    "Common activation functions comprise the *sigmoid, ReLU, tanh, leaky ReLU* and $ELU$ function. \n",
    "The sigmoid function is commonly used as hidden layer activations, or as the output layer for binary classification tasks, since this function squeezes its input values to a range from $[0, \\ldots, 1]$ : <br /> \n",
    "\n",
    "$$\n",
    "f_{sigmoid}(x) = \\frac{1}{1 + e^{-x}}. \\tag{39}\n",
    "$$\n",
    "\n",
    "Next in line is the tangent hyperbolicus, that squeezes the values to a range from $[-1, \\ldots, 1]$: <br /> \n",
    "\n",
    "$$\n",
    "f_{tanh}(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}. \\tag{40}\n",
    "$$\n",
    "\n",
    "Simpler than both of the other activation functions and usually achieving better results are the Rectified Linear Units (ReLU). It consists of a piecewise linear function, when it reaches the origin of a coordinate system: <br /> \n",
    "\n",
    "$$\n",
    "f_{ReLU}(x) = max(0, x). \\tag{41}\n",
    "$$\n",
    "\n",
    "Many variants of ReLU exist, which are not 0 in the negative space, such as the leaky ReLU: <br /> \n",
    "\n",
    "$$\n",
    "f_{leaky ReLU}(x, \\alpha) = \\begin{cases} x & if \\quad x \\geq 0 \\\\\n",
    "\\alpha x & if \\quad x < 0.\n",
    "\\end{cases}, \\tag{42}\n",
    "$$\n",
    "\n",
    "and the exponential linear unit (ELU): <br /> \n",
    "\n",
    "$$\n",
    "f_{ELU}(x, \\alpha) = \\begin{cases} x & if \\quad x \\geq 0 \\\\\n",
    "\\alpha (e^x - 1) & if \\quad x < 0.\n",
    "\\end{cases}. \\tag{43}\n",
    "$$\n",
    "\n",
    "Fig. 2 visualizes each of these activation functions. <br /> \n",
    "![Activation Functions](plots/ActivationFunctions.png \"Activation Functions\")\n",
    "*Fig. 2 Different, common Activation functions used in Neural Networks. Retrieved from [Hackernoon](https://hackernoon.com/how-to-debug-neural-networks-manual-dc2a200f10f2) on the 8th October, 2019.*\n",
    "\n",
    "#### Exploding and vanishing gradients\n",
    "\n",
    "Looking at Fig. 2, one can see that the functions on the left hand-side (Sigmoid, tanh and ReLU) are only constant on a specific interval. For instance ReLU is 0 before the origin and then linearly increasing, tanh and sigmoid are only differentiable in an interval from approximately [-3, 3]. <br /> Thus, those activation functions could squeeze the input to 0 beyond the intervals, i.e. a fully saturated neuron with input $z_j >> 1$, or a dead neuron with input $z_j << -1$ will exhibit very small gradients or none at all. Resulting in wasted neurons, since they cannot learn anything. [3]\n",
    "To avoid this problem, it is important to initialize the weights and biases correctly. One method to initialize the weights and biases is the [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). Initialize the weights in such a way that the variance remains the same for x and y as we pass thorugh the layer. [10] <br /> \n",
    "Picking the weights from a Gaussian distribution, $\\mathcal{N}(0, \\frac{1}{N})$, with zero mean and a variance of $1/N$, where $N$ specifies the number of input neurons: <br /> \n",
    "\n",
    "$$\n",
    "var(w_i) = \\frac{1}{N}. \\tag{44}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "name = 'deepLearning.png'\n",
    "file = os.path.join( os.path.join(os.getcwd(), \"plots\"), name)\n",
    "#Image(filename=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The following work will focus on the *[UCI Credit Card dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)*, which consists of 24 columns, where 23 of them are the predictor variables $p$ and one refers to the target $y$. The dataset has 30.000 entries, or individuals respectively, thus resulting in a dataset matrix $X \\in \\mathbb{R}^{30000 \\times 23}$ along with the targets $\\boldsymbol{y} = \\{y_1, y_2, \\ldots, y_{30000}\\}$. <br />\n",
    "This dataset represents the defaults of Taiwanese credit card clients of the early 2000's, based on the aforementioned 23 predictor variables like education, sex, payment history, age along with 19 others (below is a small excerpt plotted of the dataset). During this time there prevailed a cash and credit card debt crisis. An insanely smart decision [IRONY] from Taiwanese banks was to issue cash and credit cards even to unqualified applicants just to increase their market share. <br /> \n",
    "When taking a closer look to the dataset at hand, one can notice that there are only numerical columns, so there are no written words in any of the columns, and every column is filled with a proper value, which is usually never the case for other datasets. <br />\n",
    "Furthermore, the numeric values vastly range from binary values like *sex (1 = male, 2 = female)* over *age ([21, ..., 79])* until *payment amount ([0, ..., 1.684.259])*. Hence there is a need of normalizing the entire dataset to decrease the variance in our model as well as being suitable for our activation functions (see Chapter \"Vanishing and Exploding Gradients\"). <br /> \n",
    "There are mostly two common techniques: **Min-Max Normalization**, as well as **Standardization**. However, when the minimum and maximum values are not known in the features, it is obviously not possible to apply the first technique, which reads as follows: <br /> \n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min{(x)}}{\\max{(x)} - \\min{(x)}}. \\tag{42}\n",
    "$$\n",
    "\n",
    "Equation (42) squeezes all the values in the respective column to a range from [0, ..., 1]. Additionally to the aforementioned problem, considerable outliers will affect this method greatly, where the outlier takes either the value $1$ when being the biggest value or $0$ when it is super small.  <br /> \n",
    "That is the reason why this work makes use of *Standardization*: <br />\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma_x}, \\tag{43}\n",
    "$$\n",
    "\n",
    "where $\\bar{x}$ is the mean/ average value of the feature vector and $\\sigma{x}$ is its standard deviation. <br />\n",
    "\n",
    "Additional to the findings above, this dataset is also heavily unbalanced: It has almost by a factor of 5 more samples labeled as $0$, which means no default. Only 22.12% (6.636 out of the 30.000) samples are default samples. In order to not have a biased classifier, this work uses *undersampling* as technique of balancing the dataset, i.e. picking randomly the same amount of the minority class (in our case 6.636). Since using *oversampling* would not work due to the complexity and variances in the dataset, there is no way of upsampling the minority class to the size of the majority class. <br /> \n",
    "\n",
    "Moreover, when comparing the documentation of the dataset with the dataset itself, one can identify mismatches in the documentation and the actual dataset. The documentation states that the features *Education* as well as *Marriage* have ranges from $[1, 2, 3, 4]$ and $[1, 2, 3]$, respectively. <br > \n",
    "However, the dataset yields values of $0, 5$ and $6$ as well. Meaning that these values are not documented and probably have been wrongly labeled. One common way is to declare these non-documented values as *NaN* values and deleting these entries. Unfortunately, we would only keep $13.5\\%$ of the initial dataset (4.061 out of 30.000). Obviously this is too much data, which is being lost when applying this method. I decided to declare all the non-documented values as a sepearate class. Hence, *Marriage* is not going to change since there was only one different, non-documented value. *Education* however, is getting another label, $5$, representing the new label *unknown*.\n",
    "\n",
    "[READ THIS PAPER](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Code and Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the lovely libraries that saved my ass here\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a plot directory\n",
    "if os.path.isdir(os.path.join(os.getcwd(), 'plots')): \n",
    "    plot_dir = os.path.join(os.getcwd(), 'plots')\n",
    "else :\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'plots'))\n",
    "    plot_dir = os.path.join(os.getcwd(), 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my libraries\n",
    "from LogisticRegression import get_data, LogisticRegression \n",
    "from neuralNet import NeuralNetwork\n",
    "\n",
    "# read the data\n",
    "filename = \"default of credit card clients.xls\"\n",
    "datapath = os.getcwd() + '\\\\data'\n",
    "filePath = os.path.join(datapath, filename)\n",
    "X, y = get_data(filePath, standardized=False, normalized=False)\n",
    "stdX, stdY = get_data(filePath, standardized=True, normalized=False)\n",
    "normX, normY = get_data(filePath, standardized=False, normalized=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(X.isnull().values.any()) # there are no NaN values - AMAZING\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate( [ [name, mini, maxi, stdmini, stdmaxi, normmin, normmax] for name, mini, maxi, stdmini, stdmaxi, normmin, normmax in zip(X.columns, X.min(axis=0), X.max(axis=0), stdX.min(axis=0), stdX.max(axis=0), normX.min(axis=0), normX.max(axis=0))], headers=['Feature Name', 'Min Value', 'Max Value', 'Standardized Min', 'Standardized Max', 'Normalized Min', 'Normalized Max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize paramters\n",
    "epochs = 100 # after 150 runs already at maximum, so let's keep it rather short for time reasons\n",
    "batches = 64\n",
    "\n",
    "# run Logistic Regression on it and my neural network implementation\n",
    "print(\"\\nNow the Original     Dataset:\")\n",
    "logistic = LogisticRegression(X, y, test_size=0.2)\n",
    "logistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=0.001, plot_training=False)\n",
    "\n",
    "print(\"\\nNow the Standardized Dataset:\")\n",
    "stdLogistic = LogisticRegression(stdX, stdY, test_size=0.2)\n",
    "stdLogistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=0.001, plot_training=False)\n",
    "\n",
    "print(\"\\nNow the Normalized   Dataset:\")\n",
    "normLogistic = LogisticRegression(normX, normY, test_size=0.2)\n",
    "normLogistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=0.001, plot_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-5,5, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best regularization stregth on raw data (non-standardized and non-normalized)\n",
    "lambdas = np.logspace(-4,4,9) # array([0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000., 10000.]\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "for lamda in lambdas: \n",
    "    logistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=lamda, plot_training=False)\n",
    "    test_acc.append(logistic.test_accuracy)\n",
    "    train_acc.append(logistic.train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate( [ [lmds, test, train] for lmds, test, train in zip(lambdas, np.mean(test_acc, axis=1), np.mean(train_acc, axis=1))], headers=['λ Values', 'Test Accuracy', 'Train Accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lovely accuracies\n",
    "txt = \"Fig. 3 Effects of regularization parameter λ on Logistic Regression.\"\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(lambdas, np.mean(np.array(test_acc), axis=1), label=\"Test Accuracy\")\n",
    "plt.plot(lambdas, np.mean(np.array(train_acc), axis=1), label=\"Train Accuracy\")\n",
    "plt.title(\"Accuracies for Logistic Regression for different λ - values\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"λ parametrization values\")\n",
    "plt.xticks(lambdas)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "fig.text(.1, 0,txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Logistic Regression Lambdas.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAPT AS SOON AS RIGHT PLOTS SHOW\n",
    "As you can see in Fig. 3, the higher the $\\lambda$-values, the worse the accuracy becomes. However, very small $\\lambda$ will also \"destroy\" the algorihtm as it basically isn't penalizing any weights anymore. <br /> \n",
    "The sweet spot for the parametrization of $\\lambda$ is around $[0.0001, ..., 0.001]$. One could fine-tune the parametrization and makes a new, more fine granulated grid for the $\\lambda$-values. <br /> \n",
    "The takeaway from Fig. 3 is that the algorithm is really sensitive to the $\\lambda$ parametrization, so be careful which values to use. A great approach is using Cross Validation to figure out a great value of $\\lambda$, however since I have severe time problems, this work isn't focusing on the perfect Logistic Regression parameters for this dataset. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get our hands dirty with the Neural Network now. The following section will train a Neural Network to fit the Credit Card data set and classifies it. Afterwards a comparison among Logistic Regression and Neural Network will take pace. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup new Training data and Test data if wanted, but then keep in mind that we can't compare LR to NN\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "new_data = False\n",
    "\n",
    "## Get new data potentially otherwise use the versions from the Log Reg\n",
    "if new_data:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2912)\n",
    "    X_train_std,  X_test_std,  y_train_std,  y_test_std  = train_test_split(stdX, stdY, test_size=0.2, random_state=2912)\n",
    "    X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(normX, normY, test_size=0.2, random_state=2912)\n",
    "else: \n",
    "    X_train, X_test, y_train, y_test = logistic.X_train, logistic.X_test, logistic.y_train, logistic.y_test\n",
    "    X_train_std,  X_test_std,  y_train_std,  y_test_std  = stdLogistic.X_train, stdLogistic.X_test, stdLogistic.y_train, stdLogistic.y_test\n",
    "    X_train_norm, X_test_norm, y_train_norm, y_test_norm = normLogistic.X_train, normLogistic.X_test, normLogistic.y_train, normLogistic.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# One run with one classifier takes about 1m 20s\n",
    "clf      = MLPClassifier(hidden_layer_sizes=[128, 128, 128, 64], learning_rate_init=0.001).fit(X_train, y_train)\n",
    "clf_std  = MLPClassifier(hidden_layer_sizes=[128, 128, 128, 64], learning_rate_init=0.001).fit(X_train_std, y_train_std)\n",
    "clf_norm = MLPClassifier(hidden_layer_sizes=[128, 128, 128, 64], learning_rate_init=0.001).fit(X_train_norm, y_train_norm)\n",
    "print(\"SKLearn Accuracy on Original   Dataset: {}\".format(clf.score(X_test, y_test)))\n",
    "print(\"SKLearn Accuracy on Stdized    Dataset: {}\".format(clf_std.score(X_test_std, y_test_std)))\n",
    "print(\"SKLearn Accuracy on Normalized Dataset: {}\".format(clf_norm.score(X_test_norm, y_test_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Init my Neural Network\n",
    "number_of_classes = 2\n",
    "epochs            = 100\n",
    "batch_size        = 64\n",
    "learning_rate     = 0.0001\n",
    "reg_strength      = 0.0 # lambda parameter for the cost function\n",
    "\n",
    "nn = NeuralNetwork(inputs             = X_train.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train.T, y_train.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "y_pred = nn.predict(X_test.T)\n",
    "y_pred = np.argmax(y_pred, axis=0)\n",
    "acc = np.sum(y_test.astype(int) == y_pred.astype(int)) / len(y_test)\n",
    "\n",
    "print(\"Neural Network Accuracy on Test data: {}\\n\".format(acc))\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STANDARDIZED VERSION \n",
    "nn = NeuralNetwork(inputs             = X_train_std.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train_std.T, y_train_std.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "y_pred = nn.predict(X_test_std.T)\n",
    "y_pred = np.argmax(y_pred, axis=0)\n",
    "acc = np.sum(y_test_std.astype(int) == y_pred.astype(int)) / len(y_test)\n",
    "\n",
    "print(\"Neural Network Accuracy on Standardized Test data: {}\\n\".format(acc))\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NORMALIZED DATASET\n",
    "nn = NeuralNetwork(inputs             = X_train_norm.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train_norm.T, y_train_norm.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "y_pred = nn.predict(X_test_norm.T)\n",
    "y_pred = np.argmax(y_pred, axis=0)\n",
    "acc = np.sum(y_test_norm.astype(int) == y_pred.astype(int)) / len(y_test)\n",
    "\n",
    "print(\"Neural Network Accuracy on Normalized Test data: {}\\n\".format(acc))\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOW TEST THE DIFFERENT LEARNING RATES\n",
    "learning_rates_to_test = [0.000001, 0.001, 1, 1000, 100000]\n",
    "accuracies = []\n",
    "\n",
    "for tau in learning_rates_to_test: \n",
    "    nn = NeuralNetwork(inputs             = X_train_std.shape[1],\n",
    "                       outputs            = number_of_classes, \n",
    "                       cost_function_name = 'cross_entropy')\n",
    "\n",
    "    ## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "    ## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "    nn.addLayer(activations = 'relu', neurons = 128)\n",
    "    nn.addLayer(activations = 'relu', neurons = 128)\n",
    "    nn.addLayer(activations = 'relu', neurons = 128)\n",
    "    nn.addLayer(activations = 'relu', neurons = 64)\n",
    "    nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "    nn.train_network(X_train_std.T, y_train_std.T,\n",
    "           batch_size           = batch_size,\n",
    "           learning_rate        = tau,\n",
    "           epochs               = epochs,\n",
    "           val_size             = 0.2,\n",
    "           val_stepwidth        = 10,\n",
    "           optimizer            = 'adam',\n",
    "           lmbda                = reg_strength)\n",
    "\n",
    "    y_pred = nn.predict(X_test_std.T)\n",
    "    y_pred = np.argmax(y_pred, axis=0)\n",
    "    acc    = np.sum(y_test_norm.astype(int) == y_pred.astype(int)) / len(y_test)\n",
    "\n",
    "    print(\"Neural Network Accuracy on Normalized Test data: {}\\n\".format(acc))\n",
    "    print(np.unique(y_pred, return_counts=True))\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuNet = NeuralNet(X, y, nodes=neural_setup, activations=activation_functions, cost_function='cross_entropy')\n",
    "neuNet.split_data(test_size=0.2)\n",
    "scores = []\n",
    "learning_rates_to_test = [0.00001, 0.001, 0.1, 1, 10]\n",
    "for tau in learning_rates_to_test:\n",
    "    neuNet.trainingNetwork(epochs=100, batchSize=64, tau=tau)\n",
    "    ypred_test = neuNet.feed_forward(neuNet.xTest, isTraining=False)\n",
    "    acc = neuNet.accuracy(neuNet.yTest, ypred_test)\n",
    "    scores.append( (neuNet.eta, acc) )\n",
    "    print(\"Learning Rate: {}, Accuracy: {}\".format(tau, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employ regularization to the cost function with adaptive (decaying) learning rate, started with 0.001\n",
    "scores = []\n",
    "lambdas_to_test = [0.00001, 0.001, 0.1, 1, 10]\n",
    "\n",
    "for lmda in lambdas_to_test:\n",
    "    neuNet = NeuralNet(X, y, nodes=neural_setup, activations=activation_functions, cost_function='cross_entropy', regularization='l2', lamda=lmda)\n",
    "    neuNet.split_data(test_size=0.2)\n",
    "\n",
    "    neuNet.trainingNetwork(epochs=100, batchSize=64, tau='schedule')\n",
    "    ypred_test = neuNet.feed_forward(neuNet.xTest, isTraining=False)\n",
    "    acc = neuNet.accuracy(neuNet.yTest, ypred_test)\n",
    "    scores.append( (neuNet.lamda, acc) )\n",
    "    print(\"Learning Rate: {}, Reg. strength: {}, Accuracy: {}\".format(neuNet.eta, neuNet.lamda, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuNet = NeuralNet(X, y, nodes=neural_setup, activations=activation_functions, cost_function='cross_entropy', regularization='l1', lamda=100)\n",
    "neuNet.split_data(test_size=0.2)\n",
    "\n",
    "neuNet.trainingNetwork(epochs=100, batchSize=64, tau=10)\n",
    "ypred_test = neuNet.feed_forward(neuNet.xTest, isTraining=False)\n",
    "acc = neuNet.accuracy(neuNet.yTest, ypred_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neuNet.Weights['W1'])\n",
    "#print(neuNet.yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.0001\n",
    "# compare the cost functions\n",
    "neuNetCE = NeuralNet(X, y, nodes=neural_setup, activations=activation_functions, cost_function='cross_entropy')\n",
    "neuNetCE.split_data(test_size=0.2)\n",
    "neuNetCE.trainingNetwork(epochs=100, batchSize=64, tau=tau)\n",
    "ypred_test = neuNetCE.feed_forward(neuNetCE.xTest, isTraining=False)\n",
    "accCE = neuNetCE.accuracy(neuNetCE.yTest, ypred_test)\n",
    "    \n",
    "neuNetMSE = NeuralNet(X, y, nodes=neural_setup, activations=activation_functions, cost_function='mse')\n",
    "neuNetMSE.split_data(test_size=0.2)\n",
    "neuNetMSE.trainingNetwork(epochs=100, batchSize=64, tau=tau)\n",
    "ypred_test_mse = neuNetMSE.feed_forward(neuNetMSE.xTest, isTraining=False)\n",
    "accMSE = neuNetMSE.accuracy(neuNetMSE.yTest, ypred_test_mse)\n",
    "\n",
    "print(\"Score on CE:  {}\\nScore on MSE: {}\".format(accCE, accMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a mistake sneaked in anywhere, but I can't find it. <br /> \n",
    "The Accuracies across different learning rates and regularization parameters should change drastically, especially with high values for both parameters. <br /> \n",
    "## CHECK FOR ERRORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "Discuss again your choice of cost function.\n",
    "Train your network and compare the results with those from your Logistic\n",
    "Regression code. You can test your results against a similar code using Scikit-\n",
    "Learn (see the examples in the above lecture notes) or tensorflow/keras.\n",
    "Comment your results and give a critical discussion of the results obtained\n",
    "with the Logistic Regression code and your own Neural Network code. **Make\n",
    "an analysis of the regularization parameters and the learning rates employed to\n",
    "find the optimal accurary score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the theoretical part of Neural Networks, they can either be used for Regression or Classification (as done above). The Classification utilizes the cross entropy, equation (30), as a loss function. The neat benefit of the cross-entropy loss is that it is a convex function, meaning a local minimum is also automatically a global minimum. This is awesome for our minimization problem, since it is sure to find the global minimum of our function and thus the best result. <br /> \n",
    "**Using cross entropy as loss function for Regression would be possible but, computationally more expensive ????**\n",
    "Usually, mean-squared error (equation (29)) is used when it comes to Regression, since we want continuous values to predict. <br /> Additionally, the common Regression methods such as Ordinary Least Squares (OLS) and Ridge Regression are also evaluated on the MSE scores. <br />\n",
    "https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "<br />\n",
    "https://developers.google.com/machine-learning/crash-course/ml-intro\n",
    "<br />\n",
    "\n",
    "Due to the easy and efficient MSE, also its linear derivative, it is easier to implement as well as more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Part D\n",
    "\n",
    "Here we will change\n",
    "the cost function for our neural network code developed in part c) in order to\n",
    "perform a regression (fitting a function or some data set) analysis. As stated\n",
    "above, our default data sets could be either the Franke function or the terrain\n",
    "data from project 1.\n",
    "Compare you results from the neural network regression analysis (with\n",
    "a discussion of learning rates and regularization parameters) with those you\n",
    "obtained in project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to Regression Analysis with the neural network. As mentioned perviously, the cost function changed from Cross Entropy Loss (for classification) to Mean Squared Error for Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the image libraries we need\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from imageio import imread\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "def plot_terrain(file_number=2):\n",
    "    # caption the plot\n",
    "    if file_number == 2:\n",
    "        area_name = 'the Møsvatn Austfjell area'\n",
    "        plot_number = 4\n",
    "    else:\n",
    "        raise ValueError(\"There is only the Møsvatn area, which is the second file, so only call it with '2'.\")\n",
    "    txt = 'Fig. ' + str(plot_number) + ' Terrain data used within this project. The above figure illustrates ' + area_name + ' in Norway. Data taken from USGS EarthExplorer [1].'\n",
    "    # read the file and generate the data\n",
    "    fileName = os.path.join(os.getcwd(), 'SRTM_data_Norway_' + str(file_number) + '.tif')\n",
    "    image = Image.open(fileName, mode='r')\n",
    "    image.mode = 'I'\n",
    "    #print(image.size) # width, height\n",
    "    x = np.linspace(0, 1, image.size[0])\n",
    "    y = np.linspace(0, 1, image.size[1])\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = np.array(image)\n",
    "    Z = Z - np.min(Z)\n",
    "    Z = Z / np.max(Z)\n",
    "    \n",
    "    # plot the figure\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y,Z,cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    ax.set_zlim(-0.10, 1.20)\n",
    "    ax.set_title(\"Terrain Data of \" + area_name + \", Norway\")\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    ax.view_init(30, 45+90)\n",
    "    fig.text(.1,.1,txt)\n",
    "    #plt.savefig(os.path.join(plot_dir, 'terrain_' + area_name + '.png'), transparent=True, bbox_inches='tight')\n",
    "    plt.show()\n",
    "plot_terrain(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terrain_to_x_y_z(self, filenumber=2):\n",
    "    #setting up data points for real data\n",
    "    z = imread('SRTM_data_Norway_' + str(filenumber) + '.tif')\n",
    "    x = np.linspace(0, 1, len(z[1])).reshape(len(z[1]), 1)\n",
    "    y = np.linspace(0, 1, len(z)).reshape(len(z),1) # normalized data from 0 to 1\n",
    "    terrain_x, terrain_y = np.meshgrid(x,y)\n",
    "    terrain_z = z/np.max(z) # normalize\n",
    "    return terrain_x, terrain_y, terrain_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## massaging the Terrain data to the right shape\n",
    "mosvatn_x, mosvatn_y, mosvatn_z = terrain_to_x_y_z(2)\n",
    "X = np.vstack([mosvatn_x.ravel(), mosvatn_y.ravel()]).T\n",
    "y = np.reshape(mosvatn_z, X.shape[0])\n",
    "\n",
    "## for our split we will have to enable the shuffling, since it could just learn the position by time.\n",
    "## to get random samples from the entire space and not spatial correlated once we just enable the shuffling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# bring both training data to same shape \n",
    "y_train = np.vstack([y_train, y_train]).T\n",
    "y_test =  np.vstack([y_test, y_test]).T\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regression using the Neural Network with the Mean Squared Error\n",
    "print(\"Now the Neural Network as Regressor dataset\\n\")\n",
    "activation_fcts  = ['relu', 'relu', None] # needs to be one smaller than neural_nodes\n",
    "neural_nodes     = [X.shape[1], 128, 64, 1]\n",
    "\n",
    "regressorNet = NeuralNet(X, y, nodes=neural_nodes, activations=activation_fcts, cost_function='mse')\n",
    "regressorNet.split_data(test_size=0.2)\n",
    "regressorNet.trainingNetwork(epochs=10, batchSize=64, tau=0.01)\n",
    "\n",
    "ypred_test = regressorNet.feed_forward(regressorNet.xTest, isTraining=False)\n",
    "acc = regressorNet.accuracy(regressorNet.yTest, ypred_test)\n",
    "print(\"My      Accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run Logistic Regression on the terrain data\n",
    "logistic = LogisticRegression(X, y, test_size=0.2)\n",
    "logistic.optimize(batch_size=128, regularization='l2', epochs=150, lamda=0.001, plot_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is from [project 1](https://github.com/lenlehm/Regression-and-Resampling). The function used is in the Appendix, but is the exact copy of project 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lamda = 0.001\n",
    "mse_OLS_train, mse_OLS_test, r2_OLS_train, r2_OLS_test, mse_ridge_train, mse_ridge_test, r2_ridge_train, r2_ridge_test, mse_lasso_train, mse_lasso_test, r2_lasso_train, r2_lasso_test = Regression_Methods(X, y, lamda=lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE Scores for the Regression methods, Lasso and Ridge used a λ = {}\\n\".format(lamda))\n",
    "print(tabulate([ [i, olsTest, olsTrain, ridgeTest, ridgeTrain, lassoTest, lassoTrain] for i, olsTest, olsTrain, ridgeTest, ridgeTrain, lassoTest, lassoTrain in zip(np.arange(len(mse_OLS_test)), mse_OLS_test, mse_OLS_train, mse_ridge_test, mse_ridge_train, mse_lasso_test, mse_lasso_train)], headers=['Degree', 'OLS Test', 'OLS Train','RIDGE Test','RIDGE Train','LASSO Test', 'LASSO Train'], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the table above, OLS outperforms Ridge and Lasso Regression siginificantly. <br /> \n",
    "All regression methods share their effect on the polynomial degree: The lower the degree the better the MSE score (excluding the 0th degree). <br /> \n",
    "OLS achieves by far the best results, however Ridge also gets great results and Lasso Regression is not doing excellent on this dataset. <br /> \n",
    "Since OLS and Ridge are doing a quite comparable job and Lasso is doing worse, the figure below (Fig. 5) seems almost constant over the degrees, but considering the table above, each degree added will reduce the MSE score by approximately a tenth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot that stuff\n",
    "txt = \"Fig. 5 Different Regression methods and their respective MSE score on the Terrain data set displayed in Fig. 4\\n Ridge and Lasso utilized a regularization parameter λ = \" + str(lamda)\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(np.arange(max_poly_degree)[1:], mse_OLS_test[1:], label=\"Test Accuracy OLS\")\n",
    "plt.plot(np.arange(max_poly_degree)[1:], mse_ridge_test[1:], label=\"Test Accuracy RIDGE\")\n",
    "plt.plot(np.arange(max_poly_degree)[1:], mse_lasso_test[1: ], label=\"Test Accuracy LASSO\")\n",
    "plt.title(\"MSE Accuracies across the different Regression techniques\")\n",
    "plt.ylabel(\"Accuracy Score (MSE)\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.xticks(np.arange(max_poly_degree))\n",
    "plt.legend()\n",
    "fig.text(.1, 0, txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Regression on Terrain.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Part E\n",
    "\n",
    "**Critical evaluation of the various algorithms**. After all these\n",
    "glorious calculations, you should now summarize the various algorithms and\n",
    "come with a critical evaluation of their pros and cons. Which algorithm works\n",
    "best for the regression case and which is best for the classification case. These\n",
    "codes can also be part of your final project 3, but now applied to other data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work expanded on Logistic Regression and Neural Networks, especially Multi-Layered Perceptrons (MLP). <br />\n",
    "As seen in the first part of this work, where credit card data has been classified, MLP outperformed Logistic Regression. <br /> \n",
    "Taken into account that my Notebook has limited computing power - Intel Core i7, 7th generation, 16GB RAM and 512GB SSD - there was no hyperparameter search done for the MLP. Hyperparameters to be tuned are the learning rate $\\tau$, batch sizes, regularization parameter $\\lambda$, amount of hidden layers, amount of neurons within each layer, activation functions, etc. <br /> \n",
    "Hence, there is even more potential for MLP with the given credit card dataset. <br />\n",
    "However, deep neural networks experienced a hype since the last couple years and sometimes perform worse than traditional methods. It needs to be mentioned, that neural network definitely achieved tremendous successes but are not a universal tool for every problem at hand now. There still are problems where traditional methods outperform neural networks. <br /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Regression functionality of the neural networks, one can see that these models are quite flexible and adaptive. The exact same code can either classify credit card data or predict a continuous value - in our case the elevation of a region in Norway. <br />\n",
    "Comparing the traditional Regression methods like OLS and Ridge with the modern approach of a neural network, it is possible to say that there is not such a big difference in these methods. <br /> \n",
    "While OLS is way faster and does not need expensive hyperparameter searches, OLS and Ridge are definitely standing up against neural networks with respect to this data set. <br /> \n",
    "\n",
    "## EXPAND MORE ON THESE GUYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Analysis\n",
    "\n",
    "Considering that all three regression algorithms implemented in the previous chapter 3.) Code and Implementation had the same Training and Testing data, we further can compare all testing scores acorss the algorithms. <br />\n",
    "Please note, that only tested several different regularization parameters were tested. Attached in the Appendix (at the very end) you can find the MSE and R2 scores for different $\\lambda$ parametrizations acorss the algorihtms. However, each of the tested parameter still does not beat the vanilla OLS method. <br /> \n",
    "Furthermore, the ordinary Least Squares method achieved slightly better scores than its derivation Ridge regression and outperformed Lasso Regression. <br /> \n",
    "Nonetheless, Ridge regression still had a comparably good performance with respect to the terrain data for low $\\lambda$ values, but Lasso was worse than the other both. <br />\n",
    "The higher the $\\lambda$ values (see Appendix for $\\lambda \\geq 10$), the worse the performance of Ridge and Lasso respectively that's why the focus is on smaller $\\lambda$ values. <br /> \n",
    "Having more computation power at hand, one can still do an optimal hyperparameter search for $\\lambda$ by using cross validation or random search. <br />\n",
    "Additionally, there is a positive correlation among the polynomial degree and the Error measurement: the higher the polynomial degree, the smaller the MSE or the higher the R2 score, respectively. <br /> \n",
    "Comparing the scores with the generated Franke function data, we could observe similiar behavior for the algorithms. Ridge and OLS can basically achieve same scores on both datasets unlike Lasso. Lasso was always worse than both methods, sometimes even significant deviations between Lasso and the other two types. <br />\n",
    "\n",
    "Moreover, by shuffling the data one could ameliorate the model performance from 0.00238 to 0.00145 (values taken from Appendix). <br /> \n",
    "Nevertheless, a MSE of 0.00238 is already pretty good, but by enabling data shuffling it is possible to achieve an about 50% better score of the model with no real effort. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Conclusions\n",
    "\n",
    "This work shows that it is possible to reproduce the results for the Logistic Regression method and the Artificial Neural Network model of Yeh and Lien. [4] <br /> \n",
    "**DID NEURAL NETWORK OUTPERFORM LOGISTIC REGRESSION**, \n",
    "<br /> \n",
    "**HOW DID BOTH BEHAVE FOR REGRESSION TASK AT HAND**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Further Work \n",
    "\n",
    "As the section about the Dataset in the Introduction already states, the dataset itself has some values that are not documented. Manually checking and cleaning the dataset that it follows its documentation would most probably lead to better results. Going further, one could perform feature engineering to enhance the dataset by adding some more predictor variables to it and thus boosting the accuracies for the classification algorithms at hand. <br /> \n",
    "Leveraging this technique along with utilizing other classification algorithms such as Gradient Boosting Machines, i.e. AdaBoost, it would be interesting to see how this would affect the accuracies. According to the authors [4], these methods revealed promising results and digging deeper into these techniques could yield even better results. <br /> \n",
    "Another interesting approach would be combining this dataset with another credit card default dataset, such as the [German one from before 2000](https://github.com/olethrosdc/ml-society-science/tree/master/data/credit) to end up in one huge dataset, or comparing the different nations to one another and deriving some insights with respect to the banks handing out credits. <br /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Shanahan, M. (2015). *The Techonological Singularity*. Camebridge: The MIT Press <br />\n",
    "[2] Saintano, M. (2015). *Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence*. Retrieved from [Observer](https://observer.com/2015/08/stephen-hawking-elon-musk-and-bill-gates-warn-about-artificial-intelligence/) on 8th October 2019 <br />\n",
    "[3] Baeuml, B. (2019). *Advanced Deep Learning for Robotics*. Munich: [Lecture at Technical University Munich (TUM)](https://github.com/bbaeuml/ss19-advanced-dl-for-robotics/blob/master/docs/adlr-2-advanced-networks.pdf), Summer Term 2019 (password: TUM19ADLR) <br />\n",
    "[4] Yeh, C. et al. (2009). *The comparisons of data mining techniques for the predictive\n",
    "accuracy of probability of default of credit card clients*. ELSEVIER, [Source](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf) Retrieved from ELSEVIER on 8th October 2019 <br />\n",
    "[5] Hjorth-Jensen, M. (2019). *Applied Data Analysis and Machine Learning*. Oslo: Lecture hold at University of Oslo (UiO), [Source](https://compphysics.github.io/MachineLearning/doc/web/course.html) <br />\n",
    "[6] Hastie, T. et al. (2009). *The Elements of Statistical Learning*. Camebridge: Springer <br />\n",
    "[7] Knoll, A. (2019). *Cognitive Systems*. Lecture hold at Technical University Munich (TUM), [Source](https://github.com/lenlehm/Classification-and-Regression/blob/master/E04-Deep_Learning.pdf) included in Github Repository <br />\n",
    "[8] Murphy, K. P. et al. (2007). *Machine Learning: A probabilistic Perspective*. Camebridge: The MIT Press. <br />\n",
    "[9] Diederik K. et al. (2015). *Adam: A Method for Stochastic Optimization*. Cornell University, ArXiv: https://arxiv.org/abs/1412.6980, retrieved 8th October, 2019 <br />\n",
    "[10] Glorot, X. et al. (2010). *Understanding the difficulty of training deep feedforward neural networks*. Cornell University, ArXiv: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf, retrieved 8th October, 2019. <br />\n",
    "[10] Bishop, C.M. et al. (2011). *Pattern Recognition and Machine Learning*. Cambridge: Springer. <br />\n",
    "[11] EarthExplorer website: https://earthexplorer.usgs.gov/, Used Dataset: Norway, last visited 05.09.2019 <br />\n",
    "[12] Grover P. (2018): *5 Regression Loss Functions All Machine Learners Should Know*. Medium: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0, retrieved 28th October, 2019. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "Some more plots for the terrain data $\\lambda$ parametrization. <br /> \n",
    "All of the plots below are showing the Mean Squared Error (MSE) of Ridge or Lasso, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the code, which has been used in project 1 only without noisy targets. <br />\n",
    "This code was used for the Terrain data Regression Analysis, which was shown towards the end of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from RegressionMethods import CreateDesignMatrix_X\n",
    "from sklearn import linear_model, metrics\n",
    "\n",
    "def Regression_Methods(X, y, max_poly_degree=7, lamda=0.001):\n",
    "    '''\n",
    "    INPUT:\n",
    "    ----------\n",
    "    X: numpy ndarray or pandas Dataframe\n",
    "        entire input data which it counts to analyse using regression\n",
    "    y: numpy array or pandas Series\n",
    "        corresponding targets for the input data rows\n",
    "    max_ploy_degree: integer (default = 7 - too long computation time and no accuracy benefit)\n",
    "        indicator of which polynomial degree to fit to the dataset\n",
    "    lamda: float (default = 0.001 - empirical test have proven this to be quite good)\n",
    "        regularization strength parameter for Ridge and Lasso Regression\n",
    "    \n",
    "    OUTPUT:\n",
    "    ----------\n",
    "    train/test_metric: numpy ndarray\n",
    "        Metrics (either MSE or R2) for the corresponding regression method\n",
    "        There are in total 12 return values with train and test accuracy for all the 3 methods\n",
    "    '''\n",
    "    # OLS train and test scores\n",
    "    mse_OLS_train = np.zeros(max_poly_degree)\n",
    "    mse_OLS_test  = np.zeros(max_poly_degree)\n",
    "    r2_OLS_train  = np.zeros(max_poly_degree)\n",
    "    r2_OLS_test   = np.zeros(max_poly_degree)\n",
    "\n",
    "    # Ridge train and test scores\n",
    "    mse_ridge_train = np.zeros(max_poly_degree)\n",
    "    mse_ridge_test  = np.zeros(max_poly_degree)\n",
    "    r2_ridge_train  = np.zeros(max_poly_degree)\n",
    "    r2_ridge_test   = np.zeros(max_poly_degree)\n",
    "\n",
    "    # Lasso train and test scores\n",
    "    mse_lasso_train = np.zeros(max_poly_degree)\n",
    "    mse_lasso_test  = np.zeros(max_poly_degree)\n",
    "    r2_lasso_train  = np.zeros(max_poly_degree)\n",
    "    r2_lasso_test   = np.zeros(max_poly_degree)\n",
    "    \n",
    "    # Split the Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # bring both training data to same shape \n",
    "    y_train = np.vstack([y_train, y_train]).T\n",
    "    y_test =  np.vstack([y_test, y_test]).T\n",
    "\n",
    "    # now check the proper parametrization and benchmark the algorithms against each other\n",
    "    for degree in range(max_poly_degree):\n",
    "        # put everything that DOES NOT depend on lamda in this loop --> entire OLS & design Matrices\n",
    "        print(\"Evaluate degree, now at: \" + str(degree) + ' out of: ' + str(max_poly_degree-1))\n",
    "        # Train design Matrix to fit our regression model\n",
    "        X = CreateDesignMatrix_X(X_train.ravel(), y_train.ravel(), degree)\n",
    "        # Test design Matrix to evaluate the Test Set\n",
    "        designX_test = CreateDesignMatrix_X(X_test.ravel(), y_test.ravel(), degree)\n",
    "\n",
    "        beta_OLS = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y_train.ravel())#reshape(-1,1))\n",
    "        beta_ridge = np.dot(np.linalg.inv(np.dot(np.transpose(X),X) + lamda * np.eye(X.shape[1])), np.dot(np.transpose(X), y_train.ravel()))#reshape(-1, 1)))\n",
    "\n",
    "        ## OLS prediction\n",
    "        train_pred_OLS = X.dot(beta_OLS)\n",
    "        test_pred_OLS  = designX_test.dot(beta_OLS)\n",
    "\n",
    "        ## Ridge Prediction\n",
    "        train_pred_ridge = X.dot(beta_ridge)\n",
    "        test_pred_ridge = designX_test.dot(beta_ridge)\n",
    "\n",
    "        ## LASSO\n",
    "        polynom    = PolynomialFeatures(degree=degree)\n",
    "        XY         = polynom.fit_transform(np.array([X_train.ravel(), y_train.ravel()]).T)\n",
    "        lasso      = linear_model.Lasso(fit_intercept=True, alpha=lamda)\n",
    "        lasso.fit(XY, y_train.reshape(-1, 1))\n",
    "        test_feed  = polynom.fit_transform(np.array([X_test.ravel(), y_test.ravel()]).T)\n",
    "        train_feed = polynom.fit_transform(np.array([X_train.ravel(), y_train.ravel()]).T)\n",
    "        ## prediction\n",
    "        train_pred_lasso = lasso.predict(train_feed)\n",
    "        test_pred_lasso  = lasso.predict(test_feed)    \n",
    "\n",
    "\n",
    "        ## GET THE SCORES - MSE AND R2 ---------------- OLS\n",
    "        mse_OLS_train[degree] = metrics.mean_squared_error(y_train.ravel(), train_pred_OLS)\n",
    "        ## OLS is doing this one:  np.mean( np.mean( (train_target.ravel() - train_pred_OLS)**2, axis=1, keepdims=True) )\n",
    "        mse_OLS_test[degree]  = metrics.mean_squared_error(y_test.ravel(), test_pred_OLS)\n",
    "\n",
    "        r2_OLS_train[degree]  = metrics.r2_score(y_train.ravel(), train_pred_OLS)\n",
    "        r2_OLS_test[degree]   = metrics.r2_score(y_test.ravel(), test_pred_OLS)\n",
    "\n",
    "        ## RIDGE\n",
    "        mse_ridge_train[degree] = metrics.mean_squared_error(y_train.ravel(), train_pred_ridge)\n",
    "        mse_ridge_test[degree]  = metrics.mean_squared_error(y_test.ravel(), test_pred_ridge)\n",
    "        r2_ridge_train[degree]  = metrics.r2_score(y_train.ravel(), train_pred_ridge)\n",
    "        r2_ridge_test[degree]   = metrics.r2_score(y_test.ravel(), test_pred_ridge)\n",
    "\n",
    "        ## LASSO\n",
    "        mse_lasso_train[degree] = metrics.mean_squared_error(y_train.ravel(), train_pred_lasso)\n",
    "        mse_lasso_test[degree]  = metrics.mean_squared_error(y_test.ravel(), test_pred_lasso)\n",
    "        r2_lasso_train[degree]  = metrics.r2_score(y_train.ravel(), train_pred_lasso)\n",
    "        r2_lasso_test[degree]   = metrics.r2_score(y_test.ravel(), test_pred_lasso)\n",
    "        \n",
    "    return mse_OLS_train, mse_OLS_test, r2_OLS_train, r2_OLS_test, mse_ridge_train, mse_ridge_test, r2_ridge_train, r2_ridge_test, mse_lasso_train, mse_lasso_test, r2_lasso_train, r2_lasso_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
