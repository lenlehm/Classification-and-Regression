{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYS-STK4155 Project #2 - Classification and Regression\n",
    "\n",
    "Evaluation of Project number: 1 <br />\n",
    "Name: Lennart Lehmann (ERASMUS Student - UiO Code lennarl)\n",
    "\n",
    "All the Code and data can be found in [my Github Repository](https://github.com/lenlehm/Classification-and-Regression). <br />\n",
    "\n",
    "Comment your results and give a critical discussion of the results obtained\n",
    "with the Logistic Regression code and your own Neural Network code. Make\n",
    "an analysis of the regularization parameters and the learning rates employed to\n",
    "find the optimal accurary score.\n",
    "\n",
    "\n",
    "## Abstract \n",
    "\n",
    "** DO NOT FORGTE THIS SHIT HERE **\n",
    "\n",
    "\n",
    "## 1.) Introduction\n",
    "\n",
    "Neural networks have often been in the news and literally experienced a hype because of that. Many companies and people fear that Artificial Intelligence (AI) will eventually lead to machines taking over the world. This scenario is called *technological singularity* [1] and many famous people like Elon Musk or the deceased Stephen Hawkins warn about AI. [2] There also have been many popular movies depicting this machine dominating future such as *I Robot, Terminator, Ex Machina* and co. \n",
    "However, the most frequent use-case of neural networks are still classification and regression problems. <br />\n",
    "The rise of Deep Neural Networks emerged in 2012, where the winner of 2012 scored tremendous results on the *[ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://image-net.org)* by almost halving the Top-5-Error. [3] <br /> \n",
    "Keep in mind that the Error for this challenge almost remained constant over the last years. This challenge focuses on object classification and localization with over 200 distinct objects, in fact an image recognition task. This breakthrough in 2012 lead to rapid advances in AI, especially Image Classification, where only 3 years later, in 2015, human performance was already achieved. [3] <br />\n",
    "This rapid progress was utilized for similiar fields such as Regression or classification of non-images across every industry branch. [3] <br />\n",
    "Nowadays, these algorithms are used to detect cancer in medical images, or to perceive the environment of various agents, such as Autonomous Cars/ Drones or even robots. [3] <br />\n",
    "Classification is different from regression problems in the sense that algorithm's result can only take values across the classes that means to be classified. Hence, Linear Regression is not suitable for classification tasks, that is why this work focuses on Logistic Regression as well as Neural Networks (NN) for the classification of Credit Card data. <br />\n",
    "Neural networks are a subfield of Artifical Intelligence and these algorithms model a complex function to represent the dataset that was given during training. During training the model learns the correlation of the input and its corresponding output. These trained models are then used to predict new, unknown datapoints that need to be classified. <br />\n",
    "Another advantage of Neural Networks is that they are also neat for Regression problems, thus this work will make use of Neural Networks as well Logistic Regression for Classification and Regression. <br />\n",
    "Both of these methods are supervised learning techniques, where a dataset comprising inputs and its corresponding targets/ outputs are necessary. The algorithm learns from the input corresponding target for each of the datapoints given. The targets differentiate for classification problems and regression problems, i.e. classification targets are discrete variables like the class names, such as *Cat* or *Dog*, when classifying images for cat or dog images repectively. Targets for regression problems are numerical values such as Stock market prices or housing prices.\n",
    "\n",
    "** EXPLAIN THE CLASSIFICATION AND REGRESSION DATASETS AND TASKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Theory\n",
    "\n",
    "Since this work will make us of *Logistic Regression* as well as *Neural Networks*, the mathematical foundations for each of this algorithms is explained in the following section. <br />\n",
    "Furthermore, the second part of this project deals with Regression, where the two above mentioned algorithms are tested against the common Regression methods such as *Ordinary Least Squares, Ridge* and *Lasso*. I explained those in my [first project](https://github.com/lenlehm/Regression-and-Resampling/blob/master/RegressionAnalysisAndResampling.ipynb), so feel free to refresh those if you feel like it. <br />\n",
    "\n",
    "### 2.1) Logistic Regression\n",
    "\n",
    "Logistic regression can be considered a special case of linear regression with the neat benefit of simple probabilistic results for classification. Such a model specifies that an appropriate function of the fitted probability of the event is a linear function of the observed values $X$ of the available predictors $p$.\n",
    "Unfortunately, this simple model cannot properly deal with the problems of non-linear and interactive effects of the predictor variables, such as normalization of the data. [4]\n",
    "<br />\n",
    "Assuming we have a dataset $\\mathcal{D} = \\{(\\boldsymbol{x^{(i)}}, y_i)\\}^n_{i=1}$, where we have $p$ predictors for each data sample $\\boldsymbol{x^{i}} = \\{x^{(i)}_1, \\ldots, x^{(i)}_p\\}$. <br />\n",
    "The output $y_i$ are discrete values and can take values from $k = 0, 1, \\ldots, K-1$, for $K$ classes. <br />\n",
    "Classification problems try to predict the output classes $k_i$ for the given $n$ samples comprising the $p$ predictors. <br />\n",
    "Logistic Regression usually handles *binary classification* problems by assigning a probability to each of the two classes, thus meaning there are only two possible outcomes/ classes $y_i \\in \\{0, 1\\}$ with each a probability of $p(y|x) \\in \\{0, \\ldots, 1\\}$. The following of this section will be according to the *binary classification* problem.\n",
    "Let $p(y|x)$ denote the probabilty of the outcome $y$ given $x$, then the logistic model reads as follows: <br />\n",
    "\n",
    "$$\n",
    "p(y=1 | \\boldsymbol{x}, \\beta) = \\frac{1}{1 + e^{-\\beta \\cdot \\boldsymbol{x}}}, \\tag{1}\n",
    "$$\n",
    "<br />\n",
    "$$\n",
    "p(y=0 | \\boldsymbol{x}, \\beta) = 1 - p(y=1 | \\boldsymbol{x}, \\beta). \\tag{2}\n",
    "$$\n",
    "\n",
    "Note that $1-p(x) = p(-x)$. <br />\n",
    "The paramters of the mode are denoted with $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)$. \n",
    "The term $\\beta \\cdot \\boldsymbol{x} = \\beta_0 + \\sum_{k=1}^p \\beta_k x_k$ is known as the log-odds and the following function is called the *sigmoid* of $x$: <br />\n",
    "\n",
    "$$\n",
    "\\sigma(\\boldsymbol{x}) = \\frac{1}{1 + e^{-\\boldsymbol{x}}}, \\tag{3}\n",
    "$$\n",
    "\n",
    "Now the logistic model can predict a class $\\hat{y}_i$ by utilizing the estimated probabilities $p(y | x)$: <br />\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\begin{cases} 1 & if \\quad p(y=1 | \\boldsymbol{x}^{(i)} \\geq 0.5 \\\\\n",
    "0 & if \\quad p(y=1 | \\boldsymbol{x}^{(i)} < 0.5.\n",
    "\\end{cases} \\tag{4}\n",
    "$$\n",
    "\n",
    "<br />\n",
    "For training of the logistic model, *maximum likelihood* is used. Under the i.i.d. assumption (identically independent distributed) the likelohood is given by: [6] <br />\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i:y_i=1} p(y_i = 1 | \\boldsymbol{x}^{(i)}) \\quad \\prod_{i:y_i=0} p(y_i = 0 | \\boldsymbol{x}^{(i)}) \\tag{5}\n",
    "$$\n",
    "$$\n",
    " = \\prod_{i=1}^n p_i^{y_i}(1 - p_i)^{1-y_i}, \\tag{6}\n",
    "$$\n",
    "\n",
    "where $p_i = p(y_i = 1 | \\boldsymbol{x}^{(i)}) = \\sigma(\\beta \\boldsymbol{x}^{(i)}).$\n",
    "\n",
    "The parameters $\\beta$ are chosen to maximize the likelihood.\n",
    "For the sake of simplicity, the maximum likelihood is often rewritten to the *log-likelihood* to turn the productions into summations: <br />\n",
    "\n",
    "$$\n",
    "\\ell{(\\beta)} = log(L(\\beta)) = \\sum_{i=1}^n y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i). \\tag{7}\n",
    "$$\n",
    "\n",
    "Since the logarithmus function is monotonous, maximizing the logarithm of a function is equivalent to maximizing the function itself. Hence $\\beta$ maximizes both, the log-likelihood along with the likelihood itself. <br />\n",
    "Lastly, the loss function for logistic regression is defined by the *binary cross-entropy* which is denoted as follows: <br />\n",
    "\n",
    "$$\n",
    "C(\\beta) = - \\sum_{i=1}^n y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i). \\tag{8}\n",
    "$$\n",
    "\n",
    "Minimizing the binary cross-entropy loss (equation (8)) yield the optimal paramters $\\beta$. One can extend the binary cross-entropy equation by regularizing it with the $L^1, L^2$ or $L^{\\infty}$ - Norm. Considering the most common $L^2$ Regularization, (8) can be rewritten as: <br />\n",
    "\n",
    "$$\n",
    "C_{L^2}(\\beta) =  - \\sum_{i=1}^n y_i \\cdot log(p_i) + (1 - y_i) \\cdot log(1 - p_i) + \\frac{\\lambda}{2}||\\beta||^2, \\tag{9}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularizing parameter and needs to follow: $\\lambda > 0$. $L^2$ Regularization is more stable than its counterpart $L^1$, since it has a continuous derivative. However, since we square the differences, [outliers are more sensitive](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0) in the $L^2$ regularization. [12]<br />\n",
    "However, finding an analytical solution for the minimization problem is not possible. Making use of numerical optimization algorihtms like the notorious *(stochastic) gradient descent* eradicates this problem. <br /> \n",
    "Looking at the gradient of the binary cross-entropy: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(\\beta)}{\\partial \\beta_j} = - \\sum_{i=1}^n \\boldsymbol{x}_j^{(i)} \\cdot (y^{(i)} - p_i), \\tag{10}\n",
    "$$\n",
    "\n",
    "which can be written in matrix form: <br />\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta C(\\beta) = -X^T \\cdot (\\boldsymbol{y - p}). \\quad \\quad X \\in \\mathbb{R}^{n\\times(p+1)} \\tag{11}\n",
    "$$\n",
    "\n",
    "$X$ being the design-matrix containing $\\boldsymbol{x}^{(i)}$ as its i-th row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convexity\n",
    "\n",
    "A neat feature of convex function is that any local minimm is also a global minimum. Hence, if this function exhibits a minimum, one can say that this minimum is global and thus resulting in the optimal solution. Convexity is guaranteed (for multivariate functions) if the corresponding Hessian matrix of the second partial derivatives is PSD (positive semi-definite).<br />\n",
    "Showing that binary cross-entropy's (8) second partial derivatives is PSD, proves that this cost function is convex: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 C(\\beta)}{\\partial \\beta_k \\partial \\beta_j} = - \\sum_{i=1}^n \\boldsymbol{x}_j^{(i)} \\cdot \\frac{\\partial (y^{(i)} - p_i)}{\\partial \\beta_k}, \\tag{12}\n",
    "$$\n",
    "\n",
    "$$\n",
    " = - \\sum_{i=1}^n \\boldsymbol{x}_j^{(i)} \\cdot \\boldsymbol{x}_k^{(i)} \\cdot p_i(p_i - 1). \\tag{13}\n",
    "$$\n",
    "\n",
    "Again (13) can be rewritten in matrix form: <br />\n",
    "\n",
    "$$\n",
    "\\nabla^2_\\beta C(\\beta) =  - \\sum_{i=1}^n \\boldsymbol{x}^{(i)} \\cdot (\\boldsymbol{x}^{(i)})^T \\cdot p_i(p_i - 1). \\tag{14}\n",
    "$$\n",
    "\n",
    "A matrix $M \\in \\mathbb{R}^{n \\times n}$ is PSD, iff  $\\boldsymbol{z}^T Az \\geq 0, \\quad \\forall \\boldsymbol{z} \\in \\mathbb{R}^n$.\n",
    "\n",
    "Thus, we get following expression: <br />\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z}^T\\nabla_\\beta^2 C(\\beta)\\boldsymbol{z} =  - \\sum_{i=1}^n \\boldsymbol{z}^T \\cdot \\boldsymbol{x}^{(i)} \\cdot (\\boldsymbol{x}^{(i)})^T \\cdot \\boldsymbol{z} \\cdot p_i(p_i - 1) \\tag{15}\n",
    "$$\n",
    "\n",
    "$$\n",
    " = \\sum_{i=1}^n || (\\boldsymbol{x}^{(i)})^T \\boldsymbol{z} ||^2 \\cdot p_i(1 - p_i) \\geq 0. \\tag{16}\n",
    "$$\n",
    "\n",
    "The inequality follwos due to the sum of non-negative terms. \n",
    "Hence, the Hessian of the binary cross-entropy is a convex function meaning that a local minimum is also a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Neural Networks\n",
    "\n",
    "Neural networks, as the name already hint, are supposed to mimic the human brain. Recent advances in Neural networks, especially Deep Neural Networks (DNN), was a breakthrough of these algorithms. These networks consist of an input layer, where the data is fed in, arbitrary many hidden layers as well as an output layer (see Fig. 1).\n",
    "![DNN Architecture](plots/deepLearning.PNG \"DNN architecture\")\n",
    "*Fig. 1 Systematic architecture of a DNN with three hidden layers (blue rectangles) and four neurons or units per hidden layer (white circles inside blue rectangle). (Source: [7](https://github.com/lenlehm/Classification-and-Regression/blob/master/E04-Deep_Learning.pdf))* <br />\n",
    "A neural net is considered *deep*, when it has multiple hidden layers, thus the depicted network is considered a *deep neural network* since it utilizes three hidden layers. <br /> \n",
    "Each of the inputs and neurons respectively have so-called *weights* $\\boldsymbol{W}$ along with biases $b$ per neuron that it counts to optimize. In Fig. 1 each of the edges you see from one neuron to another has a specific weight $w_{l,i}$, where $l \\in (0, 1, \\cdots, L)$ denotes the current layer of maximum layer size $L$ and $i \\in (0, 1, \\cdots, N)$ describes the respective neuron in that layer, where $N$ is the last Neuron in that specified layer $l$. The biases are not depicted in Fig. 1, but there is one bias value per neuron, thus resulting in $b \\in (0, 1, \\cdots, N)$ Biases per layer $l$ with $N$ neurons.<br /> \n",
    "This architecture is also called *Mulit-layered Perceptrons (MLP)* where a MLP is build from layers of connected neurons.\n",
    "The input of the network is propagated through the layers and processed by each neuron in the network. That is also the reason why these networks are called *feed forward neural networks (FFNN)*, because the information flows through the network in forward direction (from input through layers to output - see Fig. 1). <br /> \n",
    "The network itself outputs a value for a single neuron output i.e. binary classification or regression, or a vector for multi-class classification i.e. 200 classes in the ImageNet challenge (ILSRVC, see Introduction). <br /> \n",
    "As you probably could already tell, the parameters of these network explode, the deeper (more layers) and wider (more neurons per layer) we get. For instance the DNN in Fig. 1 has a 3D input $(x_1, x_2, x_3)$ and a 2D output $(y_1, y_2)$. In between it has 3 hidden layers with 4 neurons each layer, thus resulting in 64 parameters: <br />\n",
    "$ (3 \\times 4)_{\\boldsymbol{W_0}} \\cdot (4 \\times 4)_{\\boldsymbol{W_1}} \\cdot (4 \\times 4)_{\\boldsymbol{W_2}} \\cdot (4 \\times 2)_{\\boldsymbol{W_3}} + (3 \\times 4)_{\\boldsymbol{B}} = 64.$\n",
    "\n",
    "Note, that the addition at the end depicts the Bias vector, each neuron has a single bias value. Since we have 3 (layers) $\\times$ 4 (neurons in each layer), we have to add 12 parameters for the biases. <br /> \n",
    "This parameter space blows up pretty fast, especially considering complex tasks, where the input is higher dimensional such as 100-D input, which is not uncommon for real-world applications. <br />\n",
    "In order to optimize for the desired weights and biases an efficient algorithm for calculating the gradient of the entire function with repsect to the parameters is necessary. <br /> \n",
    "Here comes the *backpropagation* very handy.\n",
    "\n",
    "#### Backpropagation \n",
    "\n",
    "As the name already states, there is not only a forward propagation, but also a *backpropagation*. This can olny be applied when a forward pass was done previously, since the calculated outut is necessary to start the backward propagation. During the forward pass the network calculates the outputs of each layer with respect to the activation function $A$: <br />\n",
    "\n",
    "$$\n",
    "O_1 = A(\\boldsymbol{W_0}^T \\cdot I), \\quad \\quad where \\quad I = Input. \\tag{17}\n",
    "$$\n",
    "\n",
    "The activation function \"activates\" the neurons, which can also be rewritten with: <br /> \n",
    "\n",
    "$$\n",
    "z_j^l = \\sum_{i=1}^n w_{ij}^l\\cdot x_i + b_j^l \\tag{18} \n",
    "$$ \n",
    "\n",
    "as <br /> \n",
    "\n",
    "$$\n",
    "a_j^l = f_l(z_j^l) = f_l(\\sum_{i=1}^n w_{ij}^l \\cdot x_i + b_j^l). \\tag{19}\n",
    "$$\n",
    "\n",
    "This is done for each layer, until arriving at the output layer. After the forward pass is done, the cost function is calculated along with its derivative w.r.t. the weights and biases in the output layer $W^L$. <br /> \n",
    "Luckily, through the Chain rule of the gradients this allows us to chain the following gradients in the following manner: <br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial w_{jk}^L} = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L)}{\\partial w_{jk}^L} \\\\\n",
    "\\quad = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot \\frac{\\partial a_j^L)}{\\partial z_j^L} \\cdot \\frac{\\partial z_j^L)}{\\partial w_{jk}^L} \\\\\n",
    "\\quad = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\cdot f'_L(z_j^L)a_k^{L-1}, \\tag{20}\n",
    "$$\n",
    "\n",
    "where <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial a_{j}^L} = \\frac{\\partial}{\\partial a_{j}^L} \\cdot \\left[\\frac{1}{2} \\sum_{i=1}^N (a_j^L - t_j)^2\\right] \\\\\n",
    "= a_j^L - t_j, \\tag{21}\n",
    "$$\n",
    "\n",
    "and <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^L_j}{\\partial w_{jk}^L} = \\frac{\\partial}{\\partial w_{jk}^L} \\cdot \\left[\\sum_{p=1}^N w_{jp}^L \\cdot a^{L-1}_j + b_j^L\\right] \\\\\n",
    "= a_j^{L-1}. \\tag{22}\n",
    "$$\n",
    "\n",
    "Defining everything except $a_k^{L-1}$ in (20) as $\\delta_j^L$ we end up in : <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C(W^L)}{\\partial w_{jk}^L} = \\delta_j^L \\cdot a_k^{L-1}. \\tag{23}\n",
    "$$\n",
    "\n",
    "Guess what's up next: the lovely Chain rule ... again (applied to $\\delta_j^L$). <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^L = \\frac{\\partial C(W^L)}{\\partial a_{j}^L} \\frac{\\partial f^L}{\\partial z_j^L} = \\frac{\\partial C(W^L)}{\\partial a_j^L} \\frac{\\partial a_j^L}{\\partial z_j^L} \\\\ \n",
    "= \\frac{\\partial C(W^L)}{\\partial z_{j}^L} = \\frac{\\partial C(W^L)}{\\partial b_{j}^L} \\frac{\\partial b_j^L}{\\partial z_{j}^L} \\\\\n",
    "= \\frac{\\partial C(W^L)}{\\partial b_{j}^L}, \\tag{24}\n",
    "$$\n",
    "\n",
    "where making use of: <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_j^L}{\\partial z_{j}^L} = \\left[\\frac{\\partial z_j^l}{\\partial b_j^L} \\right]^{-1} \\\\\n",
    "= \\left[\\frac{\\partial}{\\partial b_j^L} \\sum_{i=1}^N{L-1} w_{ij}^L \\cdot a_i^{L-1} + b_j^L \\right]^{-1} \\\\\n",
    "= 1. \\tag{25}\n",
    "$$\n",
    "\n",
    "These are the derivatives of the cost function w.r.t. both weights (20) and biases (25) in the output layer $W^L$ and $\\boldsymbol{b}^L$. <br /> \n",
    "The following equation holds for any layer, except the output layer: <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l}. \\tag{26}\n",
    "$$\n",
    "\n",
    "Connecting this to the derivatives w.r.t. the next layer $l+1$: <br /> \n",
    "\n",
    "$$\n",
    "\\delta_j^l = \\frac{\\partial C}{\\partial z_j^l} = \\sum_k \\frac{\\partial C}{\\partial z_k^{l+1}} \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n",
    "= \\sum_k \\delta_k^{l+1} \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n",
    "= \\sum_k \\delta_k^{l+1} \\cdot w^{l+1}_{kj} \\cdot \\frac{\\partial f^l}{\\partial z_j^l}, \\tag{27}\n",
    "$$\n",
    "\n",
    "with <br /> \n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_k^{l+1}}{\\partial z_j^l} = \\frac{\\partial}{\\partial z_j^l} \\left[\\sum_{i=1}^{N_l} w^{l+1}_{ik} a_k^l + b_k^{l+1} \\right] \\\\\n",
    "= \\frac{\\partial}{\\partial z_j^l} \\left[\\sum_{i=1}^{N_l} w^{l+1}_{ik} f^l(z_k^l) + b_k^{l+1} \\right] \\\\\n",
    "= w_{jk}^{l+1} \\cdot f^l(z_j^l). \\tag{28}\n",
    "$$\n",
    "\n",
    "Backpropagation is usually iterating Equation (27) and computing the gradients $\\partial C / \\partial w_{ij}^l$ and $\\partial C / \\partial b_i^l$ for each layer. <br /> \n",
    "\n",
    "#### Cost Functions\n",
    "\n",
    "All the above mentioned formulas share the same cost function C. \n",
    "The cost function is really important in the learning step since it is directly correlated with the accuracy. Finding an optimal cost function is often not easy. <br /> \n",
    "The cost function is an indicator of how well the network is doing. High loss indicates that the model is far away from the true values. Likewise, a low loss means that the model can fit well on the data and thus a low loss is desirable. <br />\n",
    "Most popular and common cost functions are *Mean-Squarred Error*, which is defined as: [6] <br />\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2. \\tag{29}\n",
    "$$\n",
    "\n",
    "MSE is used in Regression tasks, since it gives a great score of how far away the predicted value from the true value is. <br />\n",
    "Beside MSE, there is also the popular Loss function called *Cross Entropy*, which is used in classification, because there is a known set of possible outcomes i.e. the classes. *Cross Entropy* is defined as: [8] <br />\n",
    "\n",
    "$$\n",
    "CE = - \\sum_{x \\in \\mathcal{X}} p(x) log(q(x)), \\tag{30}\n",
    "$$\n",
    "\n",
    "where $p$ and $q$ are discrete probability distribution with the same $\\mathcal{X}$. <br />\n",
    "When dealing with a binary classification problem, where only two classes are present (thus $y_i \\in \\{0, 1\\}$) one could simplify (30) to: <br />\n",
    "\n",
    "$$\n",
    "CE_{binary} = - \\frac{1}{N} \\sum_{i = 1}^N y_i log(p(y_i)) + (1-y_i) log( 1 - p(y_i)). \\tag{31}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "When the gradients are known, the paramters can be updated by gradient descent according to following formula: <br /> \n",
    "\n",
    "$$\\\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\tau \\nabla C(\\theta^{(t)}), \\quad \\tau > 0 \\tag{32}\n",
    "$$\n",
    "\n",
    "for timestep $t$ and $\\tau$ being the well-known and headache provoking *learning-rate*. Too small values for $\\tau$ (< 0.000001) will take years until convergence is reached. However, too big values for $\\tau$ (> 0.01) and again no convergence is reached, since the updates are too big and the algorithm jitters around. [3]\n",
    "Since the aim is to minimize and go towards the negative gradient (descent) we have to put a \"minus\" in the update formula (32). Gradient descent is the most common optimization algorithm in machine learning, due to its performance and simplicity. A model $m(\\theta)$ is fit on a dataset $X$ with a cost function $C(X, m(\\theta))$, which evaluates the model on the underlying observations $X$. The model is fit by calculating the gradients and thus finding the optimal parameters $\\theta$ that minimize the cost function $C(X, m(\\theta)$. <br /> \n",
    "As everything in life this method also has its drawbacks. There are usually more non-convex, high-dimensional cost functions that often result in local minima instead of a global minimum. Here the inital guess (timestep $t = 0$) of $\\theta^0$ is crucial for the performance of the Gradient descent and thus sensitive to the inital guess. <br /> \n",
    "As mentioned earlier, Gradient Descent is very sensitive for the learning rate $\\tau$. There is a lot of research to find the optimal learning rate. Most of the recent published work, utilizes an adaptive learning rate, where the initial learning rate is high, but with each epoch the learning rate is reduced until it gets very small. <br /> \n",
    "Additionally the gradient is a function of $\\boldsymbol{x} = (x_1, \\ldots, x_n)$, which makes it expensive to compute numerically. <br /> \n",
    "One can alleviate the shortcomings by introducing randomness, i.e. when training in batches such as the Stochastic Gradient Descent (SGD). <br /> \n",
    "\n",
    "#### Stochastic Gradient Descent \n",
    "\n",
    "Stochastic Gradient Descent (SGD) address the drawbacks of vanilla gradient descent. The idea behind SGD is that the cost function can mostly be rewritten as a sum over datapoints: <br />\n",
    "\n",
    "$$\n",
    "C(\\theta) = \\sum_{i=1}^n c_i(\\boldsymbol{x}_i, \\theta). \\tag{33}\n",
    "$$\n",
    "\n",
    "Since the gradient is the working horse, this can also be computed as the sum over i-gradients: <br /> \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta C(\\theta) = \\sum_i^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta). \\tag{34}\n",
    "$$\n",
    "\n",
    "Randomness is added by only taking the gradient on a subset of the data, often referred to *minibatches*. Assuming $n$ datapoints and the size of minibatches $M$, there will be $\\frac{n}{M}$ minibatches. In the following of this report, the minibatches are denoted by $B_k$ where $k = 1, \\ldots, \\frac{n}{M}$. For instance one chooses $M = n$, yieldig a single datapoint in the minibatch: $B_k = \\boldsymbol{x}_k$, or $M=1$, then there is only one (Mini-)Batch $B_1$ containing all datapoints. <br /> \n",
    "Approximating the gradient by replacing the sum over all datapoints by the sum over a randomly picked minibatch in each gradient descent step: <br /> \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta C(\\theta) = \\sum_{i=1}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta) \\to \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta). \\tag{35}\n",
    "$$\n",
    "\n",
    "Accordingly an update step now looks as follows: \n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\tau \\sum_{i \\in B_k}^n \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta), \\tag{36}\n",
    "$$\n",
    "\n",
    "where each minibatch $B_k$ is picked randomly with equal probability from the interval $[1, \\frac{n}{M}]$. One iteration over all minibatches is known as *epoch*. Hence, it is common to choose a number of epochs instead of iterating over minibatches. <br /> \n",
    "Taking the gradient on a subset of the data introduces not only radomness, which decreases the chances to get stuck in a local minimum, but also has some computational benefits, if the minibatch size are relatively small to the number of datapoints. Common sizes of Minibatches start from [16, 32, 64, 128, 256, 512], depending on the dataset at hand.\n",
    "\n",
    "#### Adaptive Moment Estimation (ADAM)\n",
    "\n",
    "[ADAM](https://arxiv.org/abs/1412.6980) is another optimization algorithm, that was introduced in 2015 on the International Conference on Learning Representations (ICLR). It was a huge achievement, by boosting the performance of the optimization problems. <br />\n",
    "SGD maintains a single learning rate $\\tau$ for all update steps and thus the learning rate does not change during training.\n",
    "ADAM instead computes adaptive learning rates for different parameters from estimates of first and second moments (*momentum term*) of the gradients. <br />\n",
    "By keeping a part of the change at the previous timestep, thus giving the optimization a momentum to accelerate the minimization in parameter space directions in which the gradient is not steep, but consistently has a small value steadily in one direction. Each minibatch changes to: [9] <br /> \n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\left[\\eta \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta^{(t-1)} \\right] - \\tau \\nabla_\\theta c_i(\\boldsymbol{x}_i, \\theta^{(t)}), \\tag{37}\n",
    "$$\n",
    "\n",
    "with the momentum parameter $\\eta$, usually close to 1.0, i.e. $\\eta = 0.95$. In general, using past moments of the previously calculated iterations as a guide for the current gradient step to enhance the performance. <br /> \n",
    "ADAM uses a exponentially decaying average of the first and second memoments of the gradient to compute individual adaptive learning rates for each parameter independently. [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "The previous Section dealt with the Backpropagation and Gradient descent. However, to calculate a proper gradient, we utilize an activation function $f_l(x)$. In order to introduce non-linearities so that the model can also represent complex datasets, employing a non-linear activation function is essential. <br /> \n",
    "It is required that these functions are continous and differentiable in order for the backpropagation to work. [8] <br >\n",
    "Since the feedforward is just a nested function of the inputs times the activation functions, it can be written as: \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{j=1}^M w_{1j}^L f_L \\left(\\sum_{i=1}^M w_{ji}^{L-1} f_{L-1} \\left(\\sum_{k=1}^M w_{ik}^{L-2} f_{L-2} \\left( \\ldots f_1(w_{m1}^1 x_1 + b_m^1) \\ldots \\right) +b_k^{L-2} \\right) + b_i^{L-1} \\right) + b_1^L. \\tag{38}\n",
    "$$\n",
    "\n",
    "The simplest activation function would be the identity transformation $f_I(x) = x$, which is often used for regression networks in the output layer.\n",
    "Common activation functions comprise the *sigmoid, ReLU, tanh, leaky ReLU* and $ELU$ function. \n",
    "The sigmoid function is commonly used as hidden layer activations, or as the output layer for binary classification tasks, since this function squeezes its input values to a range from $[0, \\ldots, 1]$ : <br /> \n",
    "\n",
    "$$\n",
    "f_{sigmoid}(x) = \\frac{1}{1 + e^{-x}}. \\tag{39}\n",
    "$$\n",
    "\n",
    "Next in line is the tangent hyperbolicus, that squeezes the values to a range from $[-1, \\ldots, 1]$: <br /> \n",
    "\n",
    "$$\n",
    "f_{tanh}(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}. \\tag{40}\n",
    "$$\n",
    "\n",
    "Simpler than both of the other activation functions and usually achieving better results are the Rectified Linear Units (ReLU). It consists of a piecewise linear function, when it reaches the origin of a coordinate system: <br /> \n",
    "\n",
    "$$\n",
    "f_{ReLU}(x) = max(0, x). \\tag{41}\n",
    "$$\n",
    "\n",
    "Many variants of ReLU exist, which are not 0 in the negative space, such as the leaky ReLU: <br /> \n",
    "\n",
    "$$\n",
    "f_{leaky ReLU}(x, \\alpha) = \\begin{cases} x & if \\quad x \\geq 0 \\\\\n",
    "\\alpha x & if \\quad x < 0.\n",
    "\\end{cases}, \\tag{42}\n",
    "$$\n",
    "\n",
    "and the exponential linear unit (ELU): <br /> \n",
    "\n",
    "$$\n",
    "f_{ELU}(x, \\alpha) = \\begin{cases} x & if \\quad x \\geq 0 \\\\\n",
    "\\alpha (e^x - 1) & if \\quad x < 0.\n",
    "\\end{cases}. \\tag{43}\n",
    "$$\n",
    "\n",
    "Fig. 2 visualizes each of these activation functions. <br /> \n",
    "![Activation Functions](plots/ActivationFunctions.png \"Activation Functions\")\n",
    "*Fig. 2 Different, common Activation functions used in Neural Networks. Retrieved from [Hackernoon](https://hackernoon.com/how-to-debug-neural-networks-manual-dc2a200f10f2) on the 8th October, 2019.*\n",
    "\n",
    "#### Exploding and vanishing gradients\n",
    "\n",
    "Looking at Fig. 2, one can see that the functions on the left hand-side (Sigmoid, tanh and ReLU) are only constant on a specific interval. For instance ReLU is 0 before the origin and then linearly increasing, tanh and sigmoid are only differentiable in an interval from approximately [-3, 3]. <br /> Thus, those activation functions could squeeze the input to 0 beyond the intervals, i.e. a fully saturated neuron with input $z_j >> 1$, or a dead neuron with input $z_j << -1$ will exhibit very small gradients or none at all. Resulting in wasted neurons, since they cannot learn anything. [3]\n",
    "To avoid this problem, it is important to initialize the weights and biases correctly. One method to initialize the weights and biases is the [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). Initialize the weights in such a way that the variance remains the same for x and y as we pass thorugh the layer. [10] <br /> \n",
    "Picking the weights from a Gaussian distribution, $\\mathcal{N}(0, \\frac{1}{N})$, with zero mean and a variance of $1/N$, where $N$ specifies the number of input neurons: <br /> \n",
    "\n",
    "$$\n",
    "var(w_i) = \\frac{1}{N}. \\tag{44}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "name = 'deepLearning.png'\n",
    "file = os.path.join( os.path.join(os.getcwd(), \"plots\"), name)\n",
    "#Image(filename=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The following work will focus on the *[UCI Credit Card dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)*, which consists of 24 columns, where 23 of them are the predictor variables $p$ and one refers to the target $y$. The dataset has 30.000 entries, or individuals respectively, thus resulting in a dataset matrix $X \\in \\mathbb{R}^{30000 \\times 23}$ along with the targets $\\boldsymbol{y} = \\{y_1, y_2, \\ldots, y_{30000}\\}$. <br />\n",
    "This dataset represents the defaults of Taiwanese credit card clients of the early 2000's, based on the aforementioned 23 predictor variables like education, sex, payment history, age along with 19 others (below is a small excerpt plotted of the dataset). During this time there prevailed a cash and credit card debt crisis. An insanely smart decision [IRONY] from Taiwanese banks was to issue cash and credit cards even to unqualified applicants just to increase their market share. <br /> \n",
    "When taking a closer look to the dataset at hand, one can notice that there are only numerical columns, so there are no written words in any of the columns, and every column is filled with a proper value, which is usually never the case for other datasets. <br />\n",
    "Furthermore, the numeric values vastly range from binary values like *sex (1 = male, 2 = female)* over *age ([21, ..., 79])* until *payment amount ([0, ..., 1.684.259])*. Hence there is a need of normalizing the entire dataset to decrease the variance in our model as well as being suitable for our activation functions (see Chapter \"Vanishing and Exploding Gradients\"). <br /> \n",
    "There are mostly two common techniques: **Min-Max Normalization**, as well as **Standardization**. However, when the minimum and maximum values are not known in the features, it is obviously not possible to apply the first technique, which reads as follows: <br /> \n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\min{(x)}}{\\max{(x)} - \\min{(x)}}. \\tag{42}\n",
    "$$\n",
    "\n",
    "Equation (42) squeezes all the values in the respective column to a range from [0, ..., 1]. Additionally to the aforementioned problem, considerable outliers will affect this method greatly, where the outlier takes either the value $1$ when being the biggest value or $0$ when it is super small.  <br /> \n",
    "That is the reason why this work makes use of *Standardization*: <br />\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma_x}, \\tag{43}\n",
    "$$\n",
    "\n",
    "where $\\bar{x}$ is the mean/ average value of the feature vector and $\\sigma{x}$ is its standard deviation. <br />\n",
    "\n",
    "Additional to the findings above, this dataset is also heavily unbalanced: It has almost by a factor of 5 more samples labeled as $0$, which means no default. Only 22.12% (6.636 out of the 30.000) samples are default samples. In order to not have a biased classifier, this work uses *undersampling* as technique of balancing the dataset, i.e. picking randomly the same amount of the minority class (in our case 6.636). Since using *oversampling* would not work due to the complexity and variances in the dataset, there is no way of upsampling the minority class to the size of the majority class. <br /> \n",
    "\n",
    "Moreover, when comparing the documentation of the dataset with the dataset itself, one can identify mismatches in the documentation and the actual dataset. The documentation states that the features *Education* as well as *Marriage* have ranges from $[1, 2, 3, 4]$ and $[1, 2, 3]$, respectively. <br > \n",
    "However, the dataset yields values of $0, 5$ and $6$ as well. Meaning that these values are not documented and probably have been wrongly labeled. One common way is to declare these non-documented values as *NaN* values and deleting these entries. Unfortunately, we would only keep $13.5\\%$ of the initial dataset (4.061 out of 30.000). Obviously this is too much data, which is being lost when applying this method. I decided to declare all the non-documented values as a sepearate class. Hence, *Marriage* is not going to change since there was only one different, non-documented value. *Education* however, is getting another label, $5$, representing the new label *unknown*.\n",
    "\n",
    "[READ THIS PAPER](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Code and Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the lovely libraries that saved my ass here\n",
    "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a plot directory\n",
    "if os.path.isdir(os.path.join(os.getcwd(), 'plots')): \n",
    "    plot_dir = os.path.join(os.getcwd(), 'plots')\n",
    "else :\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'plots'))\n",
    "    plot_dir = os.path.join(os.getcwd(), 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 23)\n"
     ]
    }
   ],
   "source": [
    "# import my libraries\n",
    "from LogisticRegression import get_data, LogisticRegression \n",
    "from neuralNet import NeuralNetwork\n",
    "\n",
    "# read the data\n",
    "filename = \"default of credit card clients.xls\"\n",
    "datapath = os.getcwd() + '\\\\data'\n",
    "filePath = os.path.join(datapath, filename)\n",
    "X, y = get_data(filePath, standardized=False, normalized=False)\n",
    "stdX, stdY = get_data(filePath, standardized=True, normalized=False)\n",
    "normX, normY = get_data(filePath, standardized=False, normalized=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values and target distribution\n",
    "print(X.isnull().values.any()) # there are no NaN values and only numerical values - AMAZING\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.hist()\n",
    "print(\"Label 0 has {} recordings\\nLabel 1 has {} recordings\\nPercentage: {:.2f}%\".format(y.value_counts()[0], y.value_counts()[1], (100* y.value_counts()[1] / ( y.value_counts()[0] + y.value_counts()[1])))) \n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, the dataset is heavily unbalanced. <br /> \n",
    "Label 1 accounts for only 22% of the entire dataset. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate( [ [name, mini, maxi, stdmini, stdmaxi, normmin, normmax] for name, mini, maxi, stdmini, stdmaxi, normmin, normmax in zip(X.columns, X.min(axis=0), X.max(axis=0), stdX.min(axis=0), stdX.max(axis=0), normX.min(axis=0), normX.max(axis=0))], headers=['Feature Name', 'Min Value', 'Max Value', 'Standardized Min', 'Standardized Max', 'Normalized Min', 'Normalized Max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize paramters\n",
    "epochs = 100 # after 150 runs already at maximum, so let's keep it rather short for time reasons\n",
    "batches = 64\n",
    "\n",
    "# run Logistic Regression on it and my neural network implementation\n",
    "print(\"\\nNow the Original     Dataset:\")\n",
    "logistic = LogisticRegression(X, y, test_size=0.2)\n",
    "logistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=0.001, plot_training=False)\n",
    "\n",
    "print(\"\\nNow the Standardized Dataset:\")\n",
    "stdLogistic = LogisticRegression(stdX, stdY, test_size=0.2)\n",
    "stdLogistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=0.001, plot_training=False)\n",
    "\n",
    "print(\"\\nNow the Normalized   Dataset:\")\n",
    "normLogistic = LogisticRegression(normX, normY, test_size=0.2)\n",
    "normLogistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=0.001, plot_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best regularization stregth on raw data (non-standardized and non-normalized)\n",
    "lambdas = np.logspace(-4,4,9) # array([0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000., 10000.]\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "for lamda in lambdas: \n",
    "    logistic.optimize(batch_size=batches, regularization='l2', epochs=epochs, lamda=lamda, plot_training=False)\n",
    "    test_acc.append(logistic.test_accuracy)\n",
    "    train_acc.append(logistic.train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Scikit Learn Logistic Regression also utilized the different regularization Strengths values. They mostly seem to match up, thus Logistic Regression is not too much affected of $\\lambda$ parametrization. At least not when making use of the $L^2$ regularization in the cost function. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate( [ [lmds, test, train] for lmds, test, train in zip(lambdas, np.mean(test_acc, axis=1), np.mean(train_acc, axis=1))], headers=['位 Values', 'Test Accuracy', 'Train Accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lovely accuracies\n",
    "txt = \"Fig. 3 Effects of regularization parameter 位 on Logistic Regression.\"\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(lambdas, np.mean(np.array(test_acc), axis=1), label=\"Test Accuracy\")\n",
    "plt.plot(lambdas, np.mean(np.array(train_acc), axis=1), label=\"Train Accuracy\")\n",
    "plt.title(\"Accuracies for Logistic Regression for different 位 - values\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"位 parametrization values\")\n",
    "plt.xticks(lambdas)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "fig.text(.1, 0,txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Logistic Regression Lambdas.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAPT AS SOON AS RIGHT PLOTS SHOW\n",
    "##  VERIFY THAT LR IS NOT TOO MUCH AFFECTED OF LAMBDA\n",
    "As you can see in Fig. 3, the higher the $\\lambda$-values, the worse the accuracy becomes. However, very small $\\lambda$ will also \"destroy\" the algorihtm as it basically isn't penalizing any weights anymore. <br /> \n",
    "The sweet spot for the parametrization of $\\lambda$ is around $[0.0001, ..., 0.001]$. One could fine-tune the parametrization and makes a new, more fine granulated grid for the $\\lambda$-values. <br /> \n",
    "The takeaway from Fig. 3 is that the algorithm is really sensitive to the $\\lambda$ parametrization, so be careful which values to use. A great approach is using Cross Validation to figure out a great value of $\\lambda$, however since I have severe time problems, this work isn't focusing on the perfect Logistic Regression parameters for this dataset. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic Curve (ROC)\n",
    "\n",
    "In order to compare the results from one algorithm to another, this work utilizes the ROC curve. A ROC curve is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: the *True Positive Rate (TPR)*, and the *False Positive Rate (FPR)*, which are defined as follows: <br />\n",
    "\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}, \\tag{44} \\quad \\textrm{where TP stands for True Positives and FN for False Negatives,}\n",
    "$$\n",
    "\n",
    "$$\n",
    "FPR = \\frac{FP}{FP + TN}, \\tag{45} \\quad \\textrm{where FP are False Positives and TN are True Negatives.}\n",
    "$$.\n",
    "\n",
    "A ROC curve plots TPR (y-axis) vs. FPR (x-axis) at different classification thresholds. Lowering the classification threshold, classifies more items as positive, thus increasing both False Positives and True Positives. <br /> \n",
    "Moreover, there is an addition to the ROC curve, calles *Area under ROC Curve (AUC)*, which represents the area underneath the 2D ROC curve (can be thought of as the integral of the ROC curve). <br /> \n",
    "AUC provides an aggregate measure of performance across all possible classification thresholds. <br />\n",
    "AUC is a commonly used metrics to evaluate a classification algorithm due to its scale - and classification threshold invariance. <br /> It measures how well predictions are ranked rather than their absolut values (scale-invariant) and the quality of the model's predictions irrespective of what classification threshold was chosen (classification-threshold-invariant). <br />\n",
    "The bigger AUC, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Setup new Training data and Test data if wanted, but then keep in mind that we can't compare LR to NN\n",
    "new_data = False\n",
    "\n",
    "## Get new data potentially otherwise use the versions from the Log Reg\n",
    "if new_data:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2912)\n",
    "    X_train_std,  X_test_std,  y_train_std,  y_test_std  = train_test_split(stdX, stdY, test_size=0.2, random_state=2912)\n",
    "    X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(normX, normY, test_size=0.2, random_state=2912)\n",
    "else: \n",
    "    X_train, X_test, y_train, y_test = logistic.X_train, logistic.X_test, logistic.y_train, logistic.y_test\n",
    "    X_train_std,  X_test_std,  y_train_std,  y_test_std  = stdLogistic.X_train, stdLogistic.X_test, stdLogistic.y_train, stdLogistic.y_test\n",
    "    X_train_norm, X_test_norm, y_train_norm, y_test_norm = normLogistic.X_train, normLogistic.X_test, normLogistic.y_train, normLogistic.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Claculate and Plot the ROC curve\n",
    "model = LR(solver='lbfgs').fit(X_train, y_train)\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "FPR, TPR, thresholds = roc_curve(y_test, probs) \n",
    "plot_roc_curve(FPR, TPR, 4, \"ROC curve of Logistic Regression on the original dataset\")\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seems to be similiar to the one in the paper of [Yeh, et al.](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf) <br />\n",
    "However, since they did not explicity mention how they transformed their data, there might be some deviations compared to my results. Moreover, they also do not mention how big their split between Training and Validation set is, which makes it really hard to reproduce their results. Considering a 20% split, there might still be other samples in the training and testing data, which in turn could also change the models parameters. <br />\n",
    "One could say that we got similiar results, but obtaining the exact same results will most probably not going to happen, since they didn't state their data preparation and also didn't mention their train test split size. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get our hands dirty with the Neural Network now. The following section will train a Neural Network to fit the Credit Card data set and classifies it. Afterwards a comparison among Logistic Regression and Neural Network will take pace. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# One run with one classifier takes about 1m 20s\n",
    "clf      = MLPClassifier(hidden_layer_sizes=[128, 128, 128, 64], learning_rate_init=0.001).fit(X_train, y_train)\n",
    "clf_std  = MLPClassifier(hidden_layer_sizes=[128, 128, 128, 64], learning_rate_init=0.001).fit(X_train_std, y_train_std)\n",
    "clf_norm = MLPClassifier(hidden_layer_sizes=[128, 128, 128, 64], learning_rate_init=0.001).fit(X_train_norm, y_train_norm)\n",
    "print(\"SKLearn Accuracy on Original   Dataset: {}\".format(clf.score(X_test, y_test)))\n",
    "print(\"SKLearn Accuracy on Stdized    Dataset: {}\".format(clf_std.score(X_test_std, y_test_std)))\n",
    "print(\"SKLearn Accuracy on Normalized Dataset: {}\".format(clf_norm.score(X_test_norm, y_test_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Claculate and Plot the ROC curve\n",
    "probs = clf_norm.predict_proba(X_test_norm)\n",
    "probs = probs[:, 1]\n",
    "FPR, TPR, thresholds = roc_curve(y_test_norm, probs) \n",
    "plot_roc_curve(FPR, TPR, 5, \"ROC curve of MLP on normalized data\")\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Init the neural networks Neural Network\n",
    "number_of_classes = 2\n",
    "epochs            = 70 # usually converged at around 100, but for the sake of time please allow me to run less epochs\n",
    "batch_size        = 64\n",
    "learning_rate     = 0.0001\n",
    "reg_strength      = 0.0 # lambda parameter for the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nn = NeuralNetwork(inputs             = X_train.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train.T, y_train.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy on Original Test data: {}\\n\".format(nn.accuracy(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time # took 28m 46s, same accuracy\n",
    "if False:\n",
    "nn = NeuralNetwork(inputs             = X_train.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'leakyrelu', neurons = 256)\n",
    "nn.addLayer(activations = 'leakyrelu', neurons = 512)\n",
    "nn.addLayer(activations = 'leakyrelu', neurons = 256)\n",
    "nn.addLayer(activations = 'leakyrelu', neurons = 128)\n",
    "nn.addLayer(activations = 'leakyrelu', neurons = 128)\n",
    "nn.addLayer(activations = 'leakyrelu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train.T, y_train.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs + 40,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy on Original Test data: {}\\n\".format(nn.accuracy(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two cell shows that there was no difference in the accuracy, when adding more layers and also training longer. However, it took waaaaay longer than the previous version. <br /> \n",
    "Here you see the computaional complexity of such models. Since one could not really anticipate the outcomes, when changing some parameters, you have to wait for quite a significant time until there will be a proper result. <br />\n",
    "Now when changing even more parameters, you can say that there will be a long time to evaluate all of those parameters. <br /> \n",
    "For the sake of my (precious) time and also the electricity bill (P.S. I am a super poor student), I rather prefer not blowing up the network too much and just focus on the effects of parametrization of the neural networks. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STANDARDIZED VERSION \n",
    "nn = NeuralNetwork(inputs             = X_train_std.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train_std.T, y_train_std.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy on Standardized Test data: {}\\n\".format(nn.accuracy(X_test_std, y_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NORMALIZED DATASET\n",
    "nn = NeuralNetwork(inputs             = X_train_norm.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train_norm.T, y_train_norm.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = learning_rate,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy on Normalized Test data: {}\\n\".format(nn.accuracy(X_test_norm, y_test_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NOW TEST THE DIFFERENT LEARNING RATES\n",
    "learning_rates_to_test = [0.000001, 0.001, 1, 1000, 1000000]\n",
    "epochs = 50 # lowering the epochs to speed up that stuff here\n",
    "accuracies = []\n",
    "\n",
    "nn = NeuralNetwork(inputs             = X_train.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "for tau in learning_rates_to_test: \n",
    "    ## Train the network\n",
    "    nn.train_network(X_train.T, y_train.T,\n",
    "           batch_size           = batch_size,\n",
    "           learning_rate        = tau,\n",
    "           epochs               = epochs,\n",
    "           val_size             = 0.2,\n",
    "           val_stepwidth        = 10,\n",
    "           optimizer            = 'adam',\n",
    "           lmbda                = reg_strength)\n",
    "    \n",
    "    acc    = nn.accuracy(X_test, y_test)\n",
    "\n",
    "    print(\"\\nNeural Network Accuracy on Original Test data: {}, tau: {}\".format(acc, tau))\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lovely accuracies\n",
    "txt = \"Fig. 6 Effects of learning rate on Neural Network using SGD as optimizer.\"\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(learning_rates_to_test, accuracies, label=\"Test Accuracy\")\n",
    "plt.title(\"Accuracies for Neural Network for different learning rates\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"位 parametrization values\")\n",
    "plt.xticks(learning_rates_to_test)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "fig.text(.1, 0,txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Neural Network learning rate.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just going with super extreme values to show the effects\n",
    "scores = []\n",
    "epochs = 60 # reduce the computational amount and also time, otherwise I will not finish in time\n",
    "lambdas_to_test = [0.000001, 0.001, 100, 100000]\n",
    "\n",
    "for lmda in lambdas_to_test:\n",
    "    \n",
    "    nn = NeuralNetwork(inputs             = X_train_std.shape[1],\n",
    "                       outputs            = number_of_classes, \n",
    "                       cost_function_name = 'cross_entropy')\n",
    "\n",
    "    ## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "    ## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "    nn.addLayer(activations = 'relu', neurons = 128)\n",
    "    nn.addLayer(activations = 'relu', neurons = 128)\n",
    "    nn.addLayer(activations = 'relu', neurons = 64)\n",
    "    nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "    nn.train_network(X_train_std.T, y_train_std.T,\n",
    "           batch_size           = batch_size,\n",
    "           learning_rate        = learning_rate,\n",
    "           epochs               = epochs,\n",
    "           val_size             = 0.2,\n",
    "           val_stepwidth        = 10,\n",
    "           optimizer            = 'adam',\n",
    "           lmbda                = lmda)\n",
    "\n",
    "    acc    = nn.accuracy(X_test_std, y_test_std)\n",
    "    scores.append(acc)\n",
    "    print(\"\\nNeural Network Accuracy on Standardized Test data: {}, lambda: {}\".format(acc, lmda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lovely accuracies\n",
    "txt = \"Fig. 7 Effects of regularization parameter 位 on Neural Network using SGD as optimizer.\"\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(lambdas_to_test, scores, label=\"Test Accuracy\")\n",
    "plt.title(\"Accuracies for Neural Network for different 位 - values\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"位 parametrization values\")\n",
    "plt.xticks(lambdas_to_test)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "fig.text(.1, 0,txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Neural Network Lambdas.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:\n",
    "## Showing the ffects of activation functions\n",
    "## STANDARDIZED VERSION \n",
    "nn = NeuralNetwork(inputs             = X_train_std.shape[1],\n",
    "                   outputs            = number_of_classes, \n",
    "                   cost_function_name = 'cross_entropy')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the sigmoid activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'sigmoid', neurons = 128)\n",
    "nn.addLayer(activations = 'tanh', neurons = 128)\n",
    "nn.addLayer(activations = 'tanh', neurons = 128)\n",
    "nn.addLayer(activations = 'sigmoid', neurons = 64)\n",
    "nn.addLayer(activations = 'softmax', neurons = number_of_classes, output = True)\n",
    "\n",
    "nn.train_network(X_train_std.T, y_train_std.T,\n",
    "       batch_size           = batch_size,\n",
    "       learning_rate        = 0.0004,\n",
    "       epochs               = epochs,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = reg_strength)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy on Normalized Test data: {}\\n\".format(nn.accuracy(X_test_std, y_test_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the classification task\n",
    "\n",
    "The above two cells wonderfully show how sensitive a neural network is by just changing some hyperparameters such as the activation function, learning rate $\\tau$ or regularization strength $\\lambda$ (see Fig. 6 and 7). <br /> \n",
    "In the plot above, everything remained the same, instead of using ReLU activations the anetwork above makes use of a combination of Sigmoid and tanh activation functions. Resulting in exploding and/or vanishing gradients, since the network didn't learn anything (accuracy of 18%). <br /> \n",
    "\n",
    "Likewise, the learning rate also determines the success of convergence for the neural network. If the learning rate $\\tau$ is too small, it takes years until it converges (if enough epochs are set). If the learning rate $\\tau$ is too high, the algotihm can't converge at all, since it is only bouncing around and not resulting in a (global) optimum. [8] <br />\n",
    "Analogously, when the regularization strength $\\lambda$ is too high, the algorithm is penalzing too much ending up in a bad model and thus bad accuracy. Obviously, if $\\lambda$ is too low it is like not penalzing at all and thus also getting a bad model with a bad accuracy score. <br />\n",
    "\n",
    "Those are just some few out of many parameters in a neural networks to tune. There are multiple more, like batch size, number of epochs, number of hidden layers, number of neurons and multiple more. <br /> \n",
    "Hence, one can conclude that neural networks are really sensitive to parameter changes along with a tedious search for optimal hyperparameters. <br /> \n",
    "\n",
    "When considering Logistic Regression in the beginning of this work, there were similiar results achieved without having to tune some critical parameters. <br /> \n",
    "Moreover, Logistic Regression's accuracy basically stayed the same even when changing its regularization parameter $\\lambda$, which makes it even more robust compared to the sensitive neural networks. <br />\n",
    "Due to its simplicity in implementation, faster convergence and no tedious hyperparameter search Logistic Regression should prefered over Neural Networks for this dataset. <br />\n",
    "\n",
    "However, there is to mention that when properly tuning the Neural Network hyperparameters with Bayesian Analysis [3], it is possible to further boost the Neural Network's accuracy, however this comes to the cost of a more complex implementation as well as computational power and time (= both of which I do not have). <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "\n",
    "As mentioned in the theoretical part of Neural Networks, they can either be used for Regression or Classification (as done above). The Classification utilizes the cross entropy, equation (30), as a loss function. The neat benefit of the cross-entropy loss is that it is a convex function, meaning a local minimum is also automatically a global minimum. This is awesome for our minimization problem, since it is sure to find the global minimum of our function and thus the best result. <br /> \n",
    "In classification there is a very particular set of possible output values, the possibles classes. Hence, Mean-Squared Error (MSE), equation (29), is badly defined, as it does not have this kind of knowledge and in turn penalizes errors in an incompatible way. However, in Regression where the goal is to predict a continuous value, MSE gives a great indicator how far apart the predicted value to the true value is. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Part D\n",
    "\n",
    "Here we will change\n",
    "the cost function for our neural network code developed in part c) in order to\n",
    "perform a regression (fitting a function or some data set) analysis. As stated\n",
    "above, our default data sets could be either the Franke function or the terrain\n",
    "data from project 1.\n",
    "Compare you results from the neural network regression analysis (with\n",
    "a discussion of learning rates and regularization parameters) with those you\n",
    "obtained in project 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to Regression Analysis with the neural network. As mentioned perviously, the cost function changed from Cross Entropy Loss (for classification) to Mean Squared Error for Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwAAAAGdCAYAAACl5hQ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXl8XGW9/z9nlsy+L2cme9K0adPSQheoCEUBQaosIrXIfqF6Ua4IylUUr3iRHyrXn4qXlyCCbAWpiqLID2VR2VRaCpS2dEnSNukya9bZ1+f3RzmnM5OZZCaZLc33/Xr11cx6nnPmzJzP53m+C8cYA0EQBEEQBEEQcwNJrQdAEARBEARBEET1IANAEARBEARBEHMIMgAEQRAEQRAEMYcgA0AQBEEQBEEQcwgyAARBEARBEAQxhyADQBAEQRAEQRBzCDIABEEQBEEQBDGHIANAEARBEARBEHMIMgAEQRAEQRAEMYcgA0AQBEEQBEEQcwhZhd+fVfj9CYIgCIKoLlytB0AQxMygFQCCIAiCIAiCmEOQASAIgiAIgiCIOQQZAIIgCIIgCIKYQ5ABIAiCIAiCIIg5BBkAgiAIgiAIgphDkAEgCIIgCIIgiDkEGQCCIAiCIAiCmEOQASAIgiAIgiCIOQQZAIIgCIIgCIKYQ5ABIAiCIAiCIIg5BBkAgiAIgiAIgphDkAEgCIIgCIIgiDkEGQCCIAiCIAiCmEOQASCqwjXXXIO777671sOoCMFgEOeddx70ej2uvPLKol6zevVqbNy4scIjI0rh17/+NZqamqDVarFr1y7MmzcP//znP6d83e7duyGTycTb9NkSBEEQ9Q4ZgDmAVqsV/0kkEqhUKvH2E088UZUxPPLII/ja1742rdeuXr0aSqUSOp0Oer0eq1atwg9/+EMkEomiXh+NRsFxHA4dOjSt7U/Fr371KwSDQYyMjODxxx+f8Pitt96KDRs2VGTbAOBwOKBSqTA2NpZ1/6JFi8BxHNxud8W2PRn3338/zj777Bm/z+joKFQqFS6++OIyjOooDocDr7/+etZ9N998M375y18iGAxi0aJF6O/vx4c+9KGybbPeuPXWW8FxHJ599lnxvmAwWNNzhiAIgqgOZADmAMFgUPzX2tqKZ599Vrx9+eWXl/ReyWSyqPvKzYMPPohAIIAjR47ge9/7Hh555BFceOGFFd9uMQwMDKC7uxtSqbRmY2htbcWvf/1r8fbmzZuRTqenfN3u3bsrOayysGnTJqjVajz33HMYGhqqyDYSiQSOHDmCxYsXV+T9p0M1vldmsxn/9V//BcbYjN+rGuMlCIIgygMZAAKpVArf/e530dnZCavVissvvxyjo6MAjoU3/OIXv0BLSwvWrl2b975kMolPf/rT4HkeRqMRH/3oR7Fnzx5xG5deeinuvPNOAMCf//xndHV14a677oLNZkNTU1PRKxFarRZnn302/vCHP+Dll1/GSy+9BAB44403cMopp8BgMKCxsRE333yzKEjWrFkDAOju7oZWq8UzzzwDn8+H8847DzabDWazGRdeeCFcLlfB7W7fvh2nn346jEYjli5diueffx4A8PWvfx133303Hn300bwrKs888wx+9KMfiY+ffPLJ4mP9/f1YvXo19Ho91q5di5GREfGx1157DaeccgqMRiOWL1+ON954Y9LjcuWVV+Kxxx4Tbz/22GO46qqrsp4zPDyMyy67DDabDc3Nzbjpppvw1a9+FYwx7N69G6eddhoMBgNsNpv42muuuQbf+ta3st7n3HPPxc9+9jMAwB133IGOjg7odDosWbIEzz33HADgnXfewU033YS///3v0Gq1cDgcAI6eBzfddBPOPfdc6HQ6fPjDH8bAwMCk+/boo4/ipptuwrx58/CrX/1KvD/fyk7meeZ2u/Hxj38cRqMRFosFZ555JgBg3bp18Hq9OOecc6DVanHPPffAZDIBOHqOCCYgc5Vgsu9IKUx2ngr7c99992HevHlYsmQJAGDHjh0488wzYTKZsGjRIjzzzDPi+/3+97/HsmXLoNfr0dbWhrvuuquk8VxwwQWIxWJZ5jGTzHOmo6MDd999t2gW7r//fpx55pm44YYbYDKZ8P3vfx8OhwM7d+4EcNS0cxyHffv2AQDuvfdeXHrppVMeh+uuuw633XZb1jg+9rGP4f777y9p3wiCIIhJYIxV8h9RZ7S1tbEXX3wx677vfe977LTTTmOHDx9mkUiEXX311eyaa65hjDG2a9cuBoBdd911LBQKsXA4nPe+RCLBHnnkERYIBFgkEmHXX389O+WUU8RtrF+/nn33u99ljDH2/PPPM5lMxu68804Wj8fZ7373O6bValkgEMg75lNOOYU9/vjjE+5ftWoV+/a3v80YY+zNN99kmzdvZslkkvX19bF58+ax++67jzHGWCQSYQDYwYMHxde63W72zDPPsHA4zEZHR9kFF1zA1q9fn3f7kUiEtba2sh/+8IcsHo+zP//5z0yj0bB9+/Yxxhj7+te/zq677rqCxzzf46eccgpbsGAB6+vrY8FgkH3oQx9it99+O2OMsf379zOz2cxefPFFlkql2HPPPcesVisbHh7O+/48z7NXX32Vtbe3s/7+fhaPx5nD4WD9/f0MAHO5XIwxxtatW8fWrVvHAoEAe+ONNxjHceyBBx5gjDF20UUXsf/5n/9h6XSahcNh9vrrrzPGGPvLX/7C5s2bJ27L4/EwpVLJfD4fY4yxp556ih05coSlUin22GOPMa1WKz523333sbPOOitrrOvXr2c2m41t3bqVxeNx9ulPf5pdffXVBY/d3r17GcdxrK+vj915551s1apVWZ9L7ueaeZ7ddNNN7MYbb2SJRILFYjH2yiuvZB2z1157bdL3ynzOVN8RqVQqvq7Q+cpYcefp2rVr2cjICAuHw2xsbIw5HA62ceNGlkwm2ebNm5nJZGK9vb2MMcZeeukltmPHDpZKpdjWrVuZyWRizz//fMHjmYlwXm7atIl1d3ezZDLJAoHAhHPmkksuYYFAgPX29rL29na2ceNGxtjRz1cqlbIHHniAJZNJFg6H2bp169i9997LGGPsyiuvZJ2dneyXv/yl+F4/+9nPpjwOr7zyCmtvb2fpdJoxxtjhw4eZSqViQ0NDRe0XURUqrR3oH/2jfxX+RysABH7+85/j+9//PhobG6FUKnH77bdj06ZNYOxYWMAdd9wBtVoNlUqV9z6ZTIarr74aWq1WfI/NmzcjGo3m3aZarcY3vvENyOVyfOpTnwLHcejr6ytp3I2NjRgeHgYAnHzyyVi1ahWkUinmzZuHDRs24JVXXin4Wp7nceGFF0KlUsFgMOAb3/hGwee/9tprAICvfOUrkMvlOPfcc/Gxj30MmzZtKmm8uXzuc5/DvHnzoNFocMkll+Ddd98FcHTG++KLL8bZZ58NiUSCtWvXoqenBy+88ELB9+I4Dpdffjkef/xxPPfcc1i5ciXsdrv4eCwWw9NPP4277roLcrkcd911F2677Tb85je/AQDI5XIcOHAAbrcbKpUKH/7whwEAZ511FoLBIDZv3gzgaDjORz/6UVitVgDA+vXr4XQ6IZFIcOWVV6KpqQlbt26ddL8/85nPYPny5ZDL5bjsssvE/c7Ho48+ipNPPhnz5s3DZZddhrfeegu7du0q4uge3acjR45gcHAQDQ0N4krQdCjmO1IMxZynt912G4xGI1QqFX7/+99jyZIluPzyyyGVSrFq1Sqcf/75ePrppwEc/XwWL14MiUSC5cuX4zOf+cyk530+1q1bB6VSmbWCBBw7Z37wgx9Aq9Wiq6sLN910U1aeS2dnJz73uc9BKpVCpVLhjDPOELf/+uuv49ZbbxVvv/rqqzjjjDOmPA6nn346OI4TV1+efPJJfPzjH4fZbC5pvwiCIIjCkAGY4zDGcPDgQaxduxZGoxFGoxEnnXQS0um0GG8tkUjQ2NiY9brc+5LJJG655RZ0dnZCr9dj4cKFYIwVjNm22WyQSI6dfmq1GsFgsKSxHz58WBQF77//Ps477zzwPA+9Xo877rgDfr+/4GsDgQCuvfZatLa2Qq/X45xzzin4/CNHjqC1tRUcx4n3tbW14fDhwyWNNxchLAbI3v+BgQFs3LhR/DyMRiPeeustHDlyZNL3u+qqq7Bx48a84T8ulwvpdBodHR34whe+gC9/+ctYtWqVGPb04x//GOFwGCeddBKWLl0qVrGRSqX4zGc+I4bePPnkk1l5Iw899BCWLl0qjrOvr2/S4z7ZfufCGMPjjz8ubq+jowOrV6/Go48+Oun7C9x2221obGzERz/6UXR1deFHP/pRUa/LN46pviPFUsx52tLSIv49MDCAV199NetcePrpp8XP7Y033sAZZ5wBm80Gg8GARx55ZMrjnwvHcfjud7+LO+64A/F4XLzf7XYjnU6jtbVVvC/3vM8cKwDRAAwODkKj0eDiiy/Gq6++it27dyOdTmPRokVTHgeO48RzGQA2btxYdHUtgiAIojjIAMxxOI5DU1MT/vrXv2J0dFT8F41GxVneTOGb+bpMHn74Ybzwwgv429/+hrGxMTG5tNQZ0mLZt28f3nvvPZx++ukAjs6mL1++HP39/RgfH8e3v/1tcdv5xv/9738fhw4dwpYtWzA+Po4XXnih4FgbGxsxODiYdd/g4CCampqKGmu+7U9GS0sLNmzYkPV5hEIh3HzzzZO+bsGCBbBYLPj73/+OCy64IOsxu90OiUSCb3zjG+ju7sbHPvYx9Pf3i/vQ1NSEX/7yl3C5XPjpT3+Ka6+9Vtznz372s9i0aRP6+vqwfft2XHTRRQCAvXv34ktf+hIeeOABDA8PY3R0FF1dXROOezHJyPn429/+hsHBQXznO9+Bw+GAw+HAtm3bsHHjRqTTaTQ0NEAulyMcDouvyaxeYzAYcM8992BgYABPP/007rzzTjGXopTPpJjvSLFMdp5mbk+gpaUF55xzTtZ2g8EgfvKTnwA4upqyfv16HDx4EGNjY7jmmmum9Z07//zz4XA48OCDD4r3ORwOSCSSrHM/97zPPY6LFy9GKpXC/fffjzPOOAMWiwVarRaPPfYY1qxZIz5/quNw1VVX4be//S22bt2KgwcP4hOf+ETJ+0QQBEEUhgwAgeuvvx633norDh48CADwer1ZpQGLIRAIQKlUwmKxIBQKTUgcLRehUAh//etfcdFFF+EjH/mIWGYyEAjAYDBAq9Vi586d+MUvfiG+RqFQwGAwiMmIwvPVajWMRiP8fr+YOJqP008/Hel0Gj/5yU+QTCbx4osv4oUXXsC6deuKGjPP89i/f3/Rwuzqq6/Gb37zG7z88stIpVKIRCJ4+eWXiyrN+Pjjj+Pll1+GQqHIul+tVuP888/H7373O9x4443o6+vDT3/6U3F2fdOmTThy5Ag4joPRaAQAsbb9hz70ISiVSnzhC1/A+eefD41GA+BodSmJRAKbzYZ0Oo37778ffX19YIwhnU7DYrFgcHAQfr8fwWAQ8Xi8JHH66KOP4pOf/CR27tyJd999F++++y62bduG4eFhvPzyy5BIJDjhhBPwxBNPIJVK4dlnn82q2//HP/5RPO4GgwFSqVSs1MTzfNb5MBXl+I4Ak5+n+bjooovwzjvvYNOmTUgkEojH4/jXv/6FvXv3gjGGYDAIi8UCpVKJf/zjH2JIl4DD4cBTTz1V1NjuvPNO/OAHPxBvKxQKfOpTn8I3v/lNhEIh9Pf345577sEVV1xR8D04jsOaNWtw7733iuE+Z5xxRtbtYo5DZ2cnenp68G//9m9Yv349GhoaitoHgiAIojjIABD42te+hrPPPhtnnnkmdDodTj31VLz99tslvcd1110Hm80Gh8OBE044AaeddlpZx7hhwwbodDo4HA7853/+Jy6//PIsAfbjH/8YDz74ILRaLW644QasX78+6/V33HEH1q1bB6PRiD/+8Y+45ZZb4Pf7YbFYcNppp2Ht2rUFt61UKvGnP/0Jv/3tb2GxWPCVr3wFmzZtwrx584oa+6WXXopwOAyz2YxTTz11yud3dnbi6aefxu233w6r1Yq2tjbcc889Rc2kz58/HyeddNKE+wcHBzEyMoIVK1agqakJZ511Fj7/+c+LYu6f//wnVqxYAa1Wi3Xr1uGBBx7ICvH67Gc/i5deegmXXXaZeN/y5ctx/fXXY+XKlXA6ndi3bx9WrFiBZDKJWCyGs846C62trViwYAG6u7sRDodFERuLxSbdn2AwiKeffho33nijOPvvcDjQ1dWFSy+9VAwDuvfee7Fp0yaYTCb8/ve/xyc/+UnxPXbt2oWPfOQj0Ol0WLNmDW655RasXr0awNHwICHW/t57753yuJbjOwJMfZ7mYjKZ8Je//AUPP/wwnE4nGhsb8a1vfQuJRAIcx+H+++/HLbfcAp1Oh7vvvjvLlIbDYQQCgazKU5Nx1llnYdmyZVn3/fznPwdwNPTnzDPPxIYNG6YsHXzGGWcgEAiIORe5t4s9DldffTW2b99O4T8EQRAVgKtUiMYHVPTNCYIojnA4DJ/Ph7a2trK9p1BJIJVKIZVKZYX+ZIb/RKNRcTVBeE06nQbHcZBKpWIoT2ZOSD1gt9vxpz/9qWgBXW+89NJLeOKJJ/Dwww/XeijT4oUXXsAXv/jFkosDEFWhtLhGgiDqDjIABEEUTTGiP5NYLIaxsTEYDIYJj9ezGXC5XGhra8PBgwfB83xNxzIXicfjuPjii7FmzZppdxAnKgoZAIKY5dTXlBtBEHWHINITiQRcLhc8Ho/YtEkikUAikWSJ+0QigcOHD2Pr1q1iEueWLVvQ29uL0dHRLNMgkUggk8kgkUiQTqcRiUSwY8cOBAKBKcOEKsVrr72Gnp4efP3rXyfxXwPeffddmEwmBAIB3HDDDbUeDkEQxHEJrQAQBDEBQfSn0+msmf7BwUHIZDI0NzdnPT8ej8Pr9cLtdiORSIDnefA8D5VKhWg0ColEgpGREXi9XoyPj8NkMsFms8FoNE5YGdi8eTNWrlwpblMqlUIul4srA6VWVSIIouzQl5AgZjmyWg+AIIj6IFP0CzP8wLGZeuFvgVgsBo/HA4/Hg1QqBZ7n0dPTA7VaLT5HmMGXSCSwWCywWCxIp9MYHR2Fx+PB3r17YTQaRTMgCHxhe8KYotEoIpHIhDAhMgMEQRAEUTq0AkAQcxhBYAsx/QKFYvr7+/sRCATEkp6ZM/35yE0Czvf42NgYvF4vRkdHYTAYMDIyglNOOWVCHkBmC3PgqKkgM0AQNYG+bAQxyyEDQBBzDCGJVxD+AoVEfyQSEWf6Y7GY2OlZqVROua2pDEDuuMbGxrB9+3Y0NDRAr9fDZrPBbDYXNANCAjGZAYKoKvQFI4hZDoUAEcQcoFTRHw6HRdEvlUrB8zxOPPFEeDwecBxXlPgvFaEJmVKpxMqVKzE+Pg6v14t9+/ZBq9WKZkAqlYrjlkgkohmIRCKIRqPgOA4KhQIymUx8LkEQBEEQxyADQBDHKZnlOjOr6Uwm+t1uNzweD2QyGRwOB5YvX16TLqwcx8FgMMBgMIAxhkAgAK/Xi/3790Oj0cBms8FisUwwA8DR/Y5Go2CMZa0MkBkgCIIgiKOQASCI44h8ol8QyPnEbzAYhMfjgdfrRUNDA3iex4oVKyYV/RUOG5wAx3HQ6/XQ6/VgjCEYDMLr9eLAgQNQq9WiGRDCjISeAsJYY7EYYrEYOI6DXC5HQ0MDmQGCIAhiTkMGgCBmOaWK/kAgIIp+pVIJnuexcuVKyOXyKbfFcVzVDUDu9nU6HXQ6HTo7OxEKheD1ejE4OAilUgmbzQar1VrQDMTjccTjcTIDBEEQxJyGDABBzEJKEf3CrLnb7YbP54NKpQLP8zj55JOLSs6tVziOg1arhVarzTID77zzDhQKhWgGBGMzlRmQy+WQyWRkBgiCIIjjntl79SeIOUZmY65iRP/4+Dg8Hg98Ph80Gg0cDgc6OjpmteifDI1Gg46ODnR0dCAUCsHn82Hbtm2Qy+Ww2Wyw2WxkBgiCIAgCZAAIom4RqtukUin4fD7odDpRlBYS/WNjY/B4PPD7/dBqtXA4HJg3b54oducKGo0GGo0G7e3tCIfDohmQSqWw2+2w2WxinkMhMxAIBKDRaLKOO0EQBEEcD5ABIIg6IlP0p1IpMd5+YGAACxcunJCcyxjD6Ogo3G43hoeHodPp4HA40NXVVRHRz3FcVkWh2YBarUZbWxva2toQiUTg8/mwfft2SCQScWVAoVAAyDYDXq8XFotFrC4krAzI5XIyAwRBEMSshgwAQdSYQqI/c6Y/U3AyxjAyMiKKfoPBAIfDge7u7gkNs4hsVCoVWltb0draimg0Cp/Ph507dwKAaAYyexxIpVJIpVIwxpBIJJBIJAAAMpkMDQ0NkMlkdMwJgiCIWQcZAIKoAcWI/lxGRkawf/9+jIyMwGg0wuFwYOHChSRAp4lSqURLSwtaWloQi8Xg8/mwa9cupNNp2Gw2JBIJ8XPIDRNKpVIIh8MAyAwQBEEQsw8yAARRJUoV/el0GsPDw3C73RgaGgLHcWhpacGiRYtqKjRrWQa0UigUCjQ3N6O5uRnxeBw+nw+jo6MIBoNwOByw2+1QqVQAshupCZ9nKBQCx3GQyWRimBCZAYIgCKJeIQNAEBVkMtGfTyCm02kMDQ3B7XZjbGwMZrMZjY2NSKVS6OzshE6nq/YuZDEXYt8bGhrQ1NSE8fFx8DyPaDSKvXv3IpFIwGq1wm63Q61WAzhmBiQSifg5CysHUqlU7EJMZoAgCIKoJ8gAEESZKVX0p1IpUfQHAgGYzWY0NzdjyZIlouA+fPjwcTnzXu/I5XLRhCUSCfj9fvT19SEWi4lmQKPRAJhoBtLpNCKRCCKRCJkBgiAIoq4gA0AQZWA6ot/n88Hj8SAYDMJqtaK1tRUGgyHvLHutO/DORRhjWZ+FXC6H0+mE0+lEMpmE3+/Hvn37EI1GYbFYRDOQG9JVyAzIZLI5V56VIAiCqA/IABDENBGEndCcayrRn0wmRdEfCoVgs9nQ3t4OvV4/ZWjNbDQAQrWi0dFR2O12sdTmbCHXAGQik8ngcDjgcDiQTCYxNDSEAwcOIBwOi2ZAq9VOagYYYxNyBuZCiBVBEARRe8gAEEQJCAJOmOkXmEz0e71eeDweRCIR2Gw2zJs3TxSHxVIvBmCqcWT2JRgaGoJer4dCoRBLbQpNuGaDGZjMAGQik8nA8zx4nhfDuQYHBxEKhWA2m2G326HT6QqagWg0mjdMiMwAQRAEUSnIABDEFEwm+vOJtEQiIYr+WCwGm82G+fPnQ6vVTnsM9WIA8pEr+jP7EgBANBpFR0cHYrEYvF7vrDED0zneQqdhu92OVCqF4eFhHDp0CMFgECaTCXa7XVzxyTUDjDFEIhFEo1FIJBIyAwRBEETFIANAEHkQ4vkF4S9QSPTH43FR9MfjcdjtdixYsGBGoj/fmOqFfDP9TqdzQjOyzK7BCoVCrLsvNOHasWMHOI4TRXNup+NaMxPhLZVKxeZiQknXI0eOYPfu3aIZEHI+chOI85kBIWeAzABBEAQxU8gAEMQHZCbxptNpbN++HQsWLIBSqSwo+j0eD9xuN1KpFOx2OxYtWiSWiCwn9SD6GGMIhUIYHh7G4cOHodfrp92BOLMJl2AGtm/fDolEgng8jng8XnMzUGwIUDFIJBJYrVZYrVak02mxk/OePXtgNBpFMyDM9meGlDHGsGfPHhgMBphMJnFlgMwAQRAEMV3IABBzmlzRDxyb5U+n0xNEYCwWg8fjgcfjQTqdht1ux+LFiysi+jOpVQgQYwxjY2PiTL9MJoNarcbixYvLVs4y1wxs3bpVNANCmFAtzEA5DUAmEokEFosFFosF6XQao6Oj8Hq92Lt3LwwGA+x2O4xGo3h8OY5DKpUSzUEsFkMsFgPHcZDL5WhoaCAzQBAEQZQEGQBizjGZ6M8UUYLojkajouhnjIHneZxwwglQKpVVG3M1DUCu6NfpdHA4HFiwYIGYzFypWvZKpRINDQ1YsWIFotEovF5vzcxApQxAJhKJBGazGWazWQyr8vl86O3thV6vh91uh8lkAmNMNABC6VDGmLhaQmaAIAiCKAUyAMScoFjRLxCJRBAOh7Ft2zax5OPSpUurKvozqbQBYIxhfHwcbrcbfr9fFP3z58+vWa16pVKJ1tZWtLa2IhKJZIUJVcMMVMMAZMJxHEwmkyj4x8bG4PP50NfXh3Q6DaVSCa1WK34eU5kBuVwOmUxGZoAgCIKYABkA4rilVNEfDofFmX6pVAqJRIKuri5YrdZqD30ClTAAuaJfq9XC4XCgq6ur7hpUqVSqqpuBWiZdcxwHo9EIo9EIxhi2bduGYDCIrVu3QqPRwG63w2w2kxkgCIIgpgUZAOK4IrMxVzGiPxQKwe12w+v1Qi6Xw+FwYPny5WhoaMDOnTshk9XHV6RcBmCmor8eypHmmgGv14v33nsPMplMNANyubws26oHwSyI+La2NqjVagQCAfh8Puzfvx9qtRp2ux0Wi2VSMyDkDAgJxGQGCIIg5jb1oW4IYpoIJROFmf7MbryFRH8wGITb7YbP50NDQwMcDgdWrlw5QTQKJRnrgZkI79k0018qKpUKbW1taGtrE82AELY1UzNQ7RCgycjMAdDr9dDr9ejs7EQwGITP58PAwACUSqVoBgTjWmhlAIC4MiCXy+tmPwmCIIjqQAaAmHWUKvoZY1miX6lUFhT9mQiVgOqBUg0AYwyBQAAulwt+vx8ajQZOp/O4EP2FKLcZqCcDkE6nJ4yF4zjodDrodDp0dHQgFArB5/NhcHAQCoUCdrsdVqu1oBlIJBJIJBIAyAwQBEHMNcgAELOC6Yj+QCAgznqrVCo4HA50dHQUHdYjkUhmlQHI3GefzweNRnPczPSXSqYZCIfD8Pl82LZtG+RyudicayozUE8GQFgBKATHcdBqtdBqtVlm4N1334VcLhfNgLDPk5kBmUwmNh6rVLUngiAIoraQASDqlumI/sxQF0EAd3Z2TiuWvx7i3TPJN5ZCon/evHlzTvQXQq1WZ5kBYWUgnzCuV/KtAEyGRqOBRqNBe3t7lgESVkNssDLXAAAgAElEQVSsVquYNJ1rBlKpFEKhEDiOg0wmE1cGyAwQBEEcP5ABIOqKyUR/PgGSr2Y9z/NlmfWutxUAASGkSQjvUavVMzI6pY6jnkxRqajVarS3t4vCeCozUE8rANMdS6YBmqqCUmYXYuF7mEgkRJMgJBGTGSAIgpjdkAEgao4g+kOhEABkxSwXEv2jo6Nwu90YHh6GXq8Hz/NYsGBBWYVJPSUBA0d7E/T29sLn80GlUsHpdFZF9B+vTGUG6umzT6fTZTm3MysoRaNR+Hw+7NixAwBEM6BQKABMNAPpdBqRSASRSATRaBQGgwEKhYLMAEEQxCyElANRE/LN9A8ODooVavI9f3h4GB6PB8PDwzAajeB5Ht3d3RUTILVOAs5MXj58+DDkcjk6OztLymMgiiOfGRAawQnCuJbHvBL5CEqlEi0tLWhpaUEsFoPP58P7778PxpiYJyE0vssMuWOMYc+ePVi8eDFisZi4MiCTySjsjCAIYpZAKoKoGsIsolCnPze8RyqVZgnudDotiv6RkRGYTCY4HA4sXLiwKrOOtVoByIzpz0xeBgCn01n18WRSLyExlUQwAz6fD11dXfB6vXjnnXfyVtapJpU89gqFAs3NzWhubkY8HofP58OuXbuQTqdFM6BSqcRxpNNpsWKQsDIAAFKpNCtnYC6cLwRBELMRMgBERckU/clkUrw/X3iPRCJBMpmEz+eDx+PB6OgozGYznE4nenp6qi4mqrkCIMT054p+QWgePnxYrNJSa+opNKbSaDQadHR0iJV16sUMVJKGhgY0NTWhqakJ8Xgcfr8fe/bsQTKZhNVqhd1uF5OSc1cG0uk0otEoIpHIhJwBMgMEQRD1w/F15SLqAkEICOE9AoUac6XTafj9frhcLkQiETgcDjQ1NWHx4sU1FQ0SiaSiolsI7/F6vWJvgkLhPbM9+RYA3j/nbPS88FKthzFt5qoZaGxsRGNjIxKJBPx+P3p7exEOh3HgwAHY7XZoNBoAE8OEGGNivoBEIiEzQBAEUUccX1cromYI8fyC8BcoJPpTqRT8fj/cbjeCwSAsFgusViskEgk6OzurOfSCVEJ05xP9J5988pTCcbYZAMHUvdd5NlR/uQ9Wq7XWQyorc9EMyOVyOJ1OOJ1ObN68GSqVCv39/YjFYrBYLKIZyFwZEMLoyAwQBEHUF8fXFYqoKqWK/mQyKYr+UCgEq9WK9vZ26PV6cBwn3l8vlKsM6HRFfyazwQCk02kMDQ3B7XZjfHwcFosFABA59wtQvf8nRAC89dZbsFqt4HlejCmf7WSagWAwKJoBpVIJu90Oi8Vy3JkBjuPgcDjgcDiQTCYxNDSE/fv3IxqNwmKxwGazQavVTjADwNHfjWg0img0Co7joFAoxARiMgMEQRDV4fi6KhEVJ7NyT6Y4nkz0+3w+uN1uRCIR2Gw2dHZ2QqfTTXh+PdXdB2Y2nmAwCI/HA4/HI4r+VatWTbvhVL0YgHzN14aHh+F2uzEyMgKz2Yzm5mYYjUa8rFsmPm9vzydhXWnEsmXL4PP5sGfPHqRSKdhsNtjtdrHazGxH6Mbb2dkpmoGBgQGoVKrj1gzIZDLwPA+e55FKpTA0NISBgQGEw2HRDGR+33Mbj0WjUbHTsbAyQGaAIAiishxfVyKiIuQT/YW68QJAIpEQRX80GoXNZkNXV5c4I1iI3CpAtaZU0R0KhcSZ/oaGhhmL/pmMpZKk02mMjIzA5XJheHhYrM5UTKK2XC4XY8ozq80IpSftdnuV9qLyCGYgM0wo0wxYrdbjrmymVCqF3W6H3W5HKpXC8PAwDh48iFAoBLPZDJvNJq74ARPNQCwWQywWA8dxkMvlaGhoIDNAEARRAcgAEHmZjuj3er1wu92Ix+Ow2+1YsGABtFpt0duUSCRZoUS1ppgVgHyif+XKlWUR/ZnUWgAJHZcPHjyI0dFRxONxOJ3OvCVZX9IuLeo9M6vNCHXod+7ciXA4jMOHD2d1qJ3NcBw3qRngeR4Wi2XWmIFijahUKhVLiAolfQ8fPow9e/bAZDLBZrPBYDAUNAPxeBzxeJzMAEEQRAUgA0CIlCr64/G4KPoTiQR4nsfChQvFqiClUm8hQIVm3asl+nOp9rFhjCEQCMDlcsHv90On00Gn00Gj0aC7uzvrudvP+QgA4IQX/o6eK7vx/uN7AADWlUb43xqdcluZdejffPNNpFIpbN++XZxRttlsFT++1SCfGfB4PDhw4ADUarUYJpRpBupl5UdgOl2JJRIJrFYrrFZr1gpSphkwGo1FmwG5XA6ZTEZmgCAIYpqQAZjjZDbmKkb0x2IxMbY9lUqB53n09PRArVbPeCz1FgKUaUjC4TDcbjc8Hk9VRb9ANYVOZk8CtVoNh8OBrq4uSKVS+Hw+DA8PZz1fEP+5LLp8Pnx7fFn3FVMKVCKRoLW1Fa2trYhEIvB4PNi2bRvkcjl4nj9uKuxkmoHMnIFcM1Dou1grpmMAMpFIJLBYLLBYLEin0xgdHYXH48HevXthNBpFMyBsI58ZiMVikEgkZAYIgiCmyey/ihIlIZTki8fjCAaDE2p457uIRqNRUfQzxsDzPJYsWVL2Ki71FgIUj8cxNjaGf/7zn5DL5VUX/ZlUOgdAWNUQkpadTmfengQzEVn+t0ZhXWks6TUqlQrt7e1ob29HOByGx+PJqrBzvMTRcxwnrrDkmgGVSoVkMolUKlUX+zpTA5CJRCKB2WyG2WwGYwyjo6Pwer3o7e2FXq+H3W6HyWSa1AzE43Gx0aDJZCIzQBAEUQRkAOYAgugXwnuEC+fu3buxcuXKvBdLYebV4/GA4zjwPI+lS5dWtFpLPYQAZc70C6ULly9fXvNY9EoYgHyrGqWWJy00+y9g67YVFQJUDGq1ekK5zYGBAajVavA8D7PZXBcCeabkmoHR0VHs2rULW7duLRgmVE3KaQAy4TgOJpMJJpNJzDnxer3o7++HVquF3W6H2WzOawai0Sj279+PRYsWAYC4MiCXy8kMEARB5IEMwHFKPtEPHJvpl8vlSKVSWRdHYYbV4/FAKpWC53mceOKJUCgUVRlzrQxA5n7LZDI4HA6sWLECiUQCvb29NRf/QPkMQDQahdvthtvthlQqhdPpxIoVK+piH0shM45eKLm6f//+vEJxOtRL3D3HcdBoNFCr1Vi2bFnWyoBGoxH3tZpmoFIGIBOO42A0GmE0GsEYw/j4OLxeL/bt25d3v9PpNKRSKaRSKRhjSCQSYhdvmUyGhoYGyGSyio+bIAhitkAG4DhiKtGfKfYFsS0kIXq9XrGed61mvKVSadVCgPKJ/tz9TiaTdSUEpzuWWCwmin6hgdNJJ500LWM3ndnURZfPx64neot+fin7mTlbnikU+/v7odfrwfN8Vjx5KWOol5ljoUZ+7spAIBCA1+vF/v37q2oGqmEAMuE4DgaDAQaDQUxMF/ZbWBERGokJz88ME0qlUgiHwwCOmgFhZYDMAEEQcxkyALOcyUR/oQucMGMaDAaxe/fumor+TCq9AhCJRMSQF6lUmlf0V3M8pVCqAYjH4/B4PHC73Uin0+B5HsuWLStLCJcwjsnCf3qu7Aar8rHLFYpjY2PweDzo7e2F0WiE3W7PqjQzGfVi/ICjgjt3zBzHQa/XQ6/XTxDFlTYD1TYAmeTut7Ai4vV6wXEcPB5PVrO1zIkP4XcykUiIJkFoPEZmgCCIuQYZgFlIqaJfuFAKM/1CZ1qNRoMVK1ZUe/gFqcSMa6miPxOJRFI3QrAYAyD0YnC5XEgmkxVL1p6MI19YN+P3KMd5kBlCkltpxmQygef5rIZUlRpHORBWAApRbTNQL8nImSsiWq0WY2NjCIVCGBwchEKhEJPEc82A8L1Op9MIh8NkBgiCmJOQAZglTEf0BwIBeDwe+Hw+qFSqCUmeBw4cqOYuVI2ZiP5MOI6r+xWAZDIp9mKIxWKw2+1YtGjRtHsxHI9kVpoRatALDanMZjPsdjt0Ol2W4K+nEKB8KwCFmMoMCMnSMxG5tVwBKEQ6nYZSqURrays6OzvFZmvvvPMOFAoFbDYbrFarWMErnxmIRCKIRCJkBgiCmBOQAahjhAuTUKe/GNE/Pj4Ot9sNv98PjUYDh8ORt5zj8UZm1SKJRAKe56cd5y5QbysAAqlUCj6fDy6XC5FIBDabreSuyzOhXo7JdMitQT80NISDBw8iFArBYrGA53loNJq6MgDTHUs+M+DxeNDf3w+dTjftZOl6NQCZqxIajUasGBUOh+H1esVeEkJ34lwzACCvGcjMGaiXc4IgCGKmHN+qcBaSKfqTyaR4/2Sif2xsDG63G0NDQ9BqtVmNmyZDmOGut4t5sQgVbTJFfzmrFtXTCoAQrrBt2zYEg0HYbDZ0dXVBq9VWVZTkbkvfaMD4kbGqbb+cSCQSUQymUikMDQ1h//79iEajMJlMdfXZz/Q7mmsGMpOlSzUD9fibkUqlCq7wqdVqsZdEJBKB1+vFe++9B4lEInaZFl6bzwxEo9G8KwNkBgiCmM2QAagDhAuNEN4jUKgxl9Awx+12Y3h4GDqdDg6HA/Pnzy8pNleoulNvF/PJyBT9QkWbSpUqrXTzrakQZqjdbjdGRkYAAIsWLZoydr0aDFxzIfSNhoKP5yYAn3bXOXj9my9UeljTRiqVwm63w263I5lMwu12w+VyYcuWLbDZbLDb7WXpdj0dyr0akZssnVlis5gyqvVqAIr57VOpVGhra0NbWxsikQh8Ph+2b9+eZQaF35JcM8AYQzQaRTQahUQiITNAEMSshgxAjRDi+QXhLzCZ6B8ZGRFFv8FggMPhQHd397QvxoIBqEVn28nIFTzVFP2Z1OKizhjD8PAwXC4XRkdHYbFY0NzcjM7OTuzduxcGQ2HRXQ8M9ftgmWer9TBmhEwmg81mw9DQEHp6euD3+9Hb24tEIiGahEo2xMulkoK7kBkQVgZ4ns/qxFvp8UyX6YxJpVKhtbUVra2tiMVi8Hq92LlzJwCIZkD4nPOZgUgkQmaAIIhZCxmAKlKq6BcSFoUZYKPRCIfDgYULF5blAlzNuvvFIsy6x2IxsYyl0Im4mk3JqkmuuTOZTHA6nVi8eLF4XkQikbqIvQ985d9Qm3nw6iKYULlcDqfTCafTKVZZ2rVrF9LptGgGKn1OVisfIZ8Z8Hg86Ovrg16vh91uF0Oj6i2naKaViRQKBVpaWtDS0oJYLAafzyd+zoIZEKpp5SYQZ5oBjuOgUCjEvgRkBgiCqFfq61f8OEQQ/blNpSYT/cPDw3C73RgdHYXZbIbD4cCiRYvKPutWbwYgGo0iHo9j8+bNYkx/uWrX1xtC7obL5cLQ0BAMBgOcTmdBc1frcKTjlf5PrwUAzHv6/2Xdn090y+VyNDU1oampCfF4PGvGmOf5rFjyclKLGfd8PRW8Xi/6+vrEqkr1tBJQztKkCoUCzc3NaG5uRjweh8/nw549e5BMJkUzIISDZZoBAGKYkFC6VS6Xo6GhgcwAQRB1BxmACpBZrjOdTqO3txcmkwk2m62g6BdivcfGxmCxWNDY2Jg1A1wJ6sEACF1qPR6PeNFcvHgxdDpdTcdVCYRKLC6XC36/X8zdKCaM63gyAO+fczZ6Xngp72PV3EdB/Bcax2TfvYaGBlEkRqNReL1ebN++XcwlyKwyM1NqXZEos6cCYwy7du1CMBjEli1bslYGamkGKtWboKGhQTR9iUQCPp9PDAezWq0TckNyuxBv3boV3d3dUCgUZAYIgqgryACUiVzRDxybHZLJZBNqeQtVR9xuNwKBgBjrvWTJkqpdHGplADLDexhjcDgcWLp0KZRKJd5+++26Cy+YKYFAAG63Gz6fD2q1Gk6ns6gqTZkcDwbAutIo/v3uu++Ks+b1+HmXIrqF+vOtra1ilRmh5KRgBmayj/U0085xHBoaGuBwOGAymbJWBvR6PXieh9ForPp4c8uAVgK5XI7GxkY0NjYikUjA7/ejr68PsVhMNAOZ/Tc4jhOrE3Ech3g8jng8LoaWkRkgCKKW1N+VdxYxmejP/FGXyWRIJpNi/XaPx4NgMAir1Yq2traaVXWppgHIFf08z4uiPxOJRFI35RdnQigUgsvlgtfrFZuwzaQfQz0bgJbVXTj4r76SXtPd3Q2Px4N33nkHSqUSPM/DYrHUjdCd7rHOrDITDoez9lHoTFuqUK31CkAugtjOXRkYHR0VZ8gNBgPsdnvVzEC1q5ll5oYkk0n4/X7s27cP0WgUFotFNAOCectdGcg1A3K5HDKZrK4+Z4Igjm/IAJRIZmOuyUS/QDKZRDAYxOjoKAYHB2G1WtHR0TGh82gtqLQBEES/x+NBOp0uKPozmc0GIBwOi+FMDQ0NcDqdWZ2XZ0I9G4DpsP/C89Hzwktob29HMBiEx+PBgQMHoNFokEwmqz7rnS8XYKbfT7VaLTajCgaD8Hq9GBgYgFqtFjvyFmMGhNC4eiGf2OY4DiaTCSaTSTQDXq+3amagUiFAxSCTyeBwOOBwOJBMJjE8PIwDBw4gHA4jHo8jGAxm9esgM0AQRD1ABqAEkskkEokEgKlFv9frhcfjQSQSgVqthtFoRE9PT139qFfCAMTjcVEEC6J/yZIlYgWNWoypkgglSt1uN6RSKZxOJ1auXFn20qq1NADxeBwejwculwt8Bd5fq9VCq9Wis7MTY2NjGB4expYtW2AwGMSQkmp/b8o96y7so2AGPB4P9u/fD41GI5qB2VJ3f6rx1MIM1ItJkslkYnWoVCqFN998E4ODgwiFQjCbzbDb7VmTP4XMwBVXXIFnn322rq4XBEEcX5ABmAb5LjRCiUC32414PA6bzYb58+dDq9XC7/fD7/fX3Y+5VCoVDc1MEASi2+2elujPpB5XAHLFoJC4LJQodTgcWL58eUUqwAhU2wAIJtblciGRSIirN54ZvOfCi0/G7t9tLvi40K1WpVJhxYoVGB0dhcfjwd69e2E2m8HzfNVWzioVdsNxHHQ6HXQ6nZgU7vF40N/fXzChth5DgIoV25OZAaPRCJvNBpPJVFf7Vy6kUinkcjkWL16MVCqFkZERHDp0CMFgECaTCXa7PSv8M9MMHDly5Lg8JgRB1A9kAEog9wdZKAXodrvFJkELFy7MSgQDjs4K1eOstlQqRTQandZrM0V/KpWakejPpN4MgFDrO5FITDA5x1uJ0lQqBb/fD5fLhXA4XPB8nglTmQCBTOEolMY9ePAgQqEQLBYLeJ6HVqud9jgmqwAEVEd0C4ZHr9dPKLWZufox21YACjGVGRBWBo4X4ZtZ+EEqlcJqtcJqtYr9XY4cOYLdu3eLFeIy9/14CvcjCKI+IQNQIrFYDH6/P0v49vT0ZJWCy0VIAq43Sg23qZTozx1TvRiARCKBeDyOrVu3Vmx/i6VSKwBCCVqXy4VAIACr1YrOzs66yFERkEgkongSTMq+ffsQi8Vgs9nA83zBzyR85xeRCMdguOsh8b4jGy6ZcpvFGoDei84DAMx/5vki9yY/uQm1IyMj4uqHIB7rZSWgHIYk1+BlrvYcL2agUF6CRCKBxWKBxWKZsO9//etfccIJJ8zq/SYIYnZABqAEOI5Df38/NBpNSUJwNhsAYZXD5XKJInjx4sWTGp6ZIJFIarpakhn6Eo/HAQDz58+H0Wic4pWVpZyCQJh9dblcGB4ehtlsRktLS8UEl2VxR0ljmwypVAqe58HzvFiKcc+ePUilUnk78ybCMcjVpXfqLUZsH7zqopLftxg4joPZbBabbe3atQsjIyPwer0wmUxVDYXKR7lXJITGYsL+lmoG6nW2PJlMTlkAIHffh4eH8dRTT2H//v3493//d1xyySX4yEc+MmlO0bXXXos//elPsNvt2LFjx4THn3jiCfzgBz8AcDQX5b777sOyZcsAAH/+85/x5S9/GalUChs2bMCtt946gz0mCGI2QQagRHp6ekq++M02A5AZ2pRMJisu+jOpRQiQUJ7V5XIhEonAbreju7sbWq0WW7durcmMf7lhjGF8fFzsPKzX6yftPJyPgWsuFP82tNoAAGODvpLHIoQBRX9wI5Rf/+mEx4sVtkIpxubXNiJy0ZfFzrwcx4k9BqZLMaJS2iADEJv2NopBIpFAqVSKdfdzQ6HsdntWhZlqUMmQpMnMQL5QmUqPZyaUWplIIpHgvPPOw4knnoibbroJl19+OX7729/iq1/9Kh555BEsX7487+uuueYa/Md//AeuuuqqvI93dHTglVdegclkwvPPP4/Pf/7zePPNN5FKpXDDDTfgxRdfRHNzM1atWoULLrgAPT0909pfgiBmF2QAqkC9VrbJHFeu6Lfb7VOGNlWCaq0ApNNp+Hw+uN1uBINB2Gw2dHV1TRBTHMfVTUjSdAgGg3C5XFlNyBYsWFA2wSQYgUwUBg2iI4FJX9d5VvlExtj7fcD7X0Lzd34udub1eDzYtm0b5n/wHGE2duyb1xX1nvFvfgHcLf8HI/95NUz/8yjc168HAKSTKaTiyQ/EP6CxahDyh9B70XklhQENXnHUTLVu/MOUzxUEbm4o1NDQEAYGBhCJRMS8iHLma0xGNQxHPjPgdrtFM2C322EwGOrWABSzApCPQCAAg8GANWvWYM2aNUin05Ma0jVr1uDAgQMFHz/11FPFv1evXo1Dhw4BADZv3oyuri50dnYCAC699FL84Q9/IANAEHMEMgAlMN2LXr3Gc6bTaQSDQbz11ltipZdaiP5MylWZKB/54t3b29snbcQmJAHPJiKRCFwuV1Y/gpk0Icuk5RNrcPC5V2E4eTnGNr997P5pNAMrN8Hv/Du03/k50j/8Ctq+9TO0tbVh7HdHH3v77behVqvRkvF8/oQWeLYfnPA+gtgXzomR/7w663FB/OcydPMVGN4/hPnPPD+pIRDEv/D3VCYgXziSVCoVQ56SySSGhobQ398v5kXY7faafo/LTa4ZGBkZgcvlwp49e6DT6USRXE+/tdPtTTA2NgaDwSDeLqe5eeihh3DeeUfzVg4fPoyWlmPfiObmZrz55ptl2xZBEPUNGYA5hlDNxuPxIBaLIZVK1Vz0Z1LuECAhrtbtdmN0dBQWi6WkePfZsgKQWZpUIpHA4XBgxYoVZSlNmv7N/4Vk3VfF262XnDvj9xQoFAZULNJf/0/+Bx65A5z82M/bqlWrEAwGEfCOATgq/gsR8gWgsZUWY2/usGT9X06mqnEvk8nEvIhkMil2400kEqIZOB7C2ARyk2hdLhdGR0exefPmrJWBWpuBmawA6PX6so/nb3/7Gx566CG8/vrrAPKHudX6mBEEUT3IAEyDeptpmorcHgU8z2PhwoVQKpXYsmVL3Yh/oDxVgIQqKm63G8PDwzCZTGhsbMTixYtL/tzqrSxpJoKZc7lcFSlNmnj4O2CJJGQmU1HPF1YBpgr/mYxSV1vG3i+86sASx/JuhPr7U40sszyo+u5vALrylnl1fX7dhPumWgXILCc5FTKZDE6nE06nE4lEAj6fD7t370Y6nc6bJD3bkUgk0Ol0MJvNWLBgQdbKgJAwPdkKXyWZ7gpAJQzAe++9hw0bNuD555+HxXLUpDY3N+PgwWMrYIcOHUJjY2NZt0sQRP1CBqAECnX+LZZqxqrmiv58Nd0ZY3UnbqebAyDUUReSXI1GIxwOR0lJrvmoZQfefKRSKbFKUSwWg91ur1iCdqaAriS5qwClfsc0TXaEDnsBHA0DksiOiS6hApBQDlTfbMH4oaGs17eu6cHgq++Lt63dTqRiccRD8aznpZPFnZfu69dD55go4PKJ/2KY7u+GXC5HY2MjGhsbxRyfnTt3AoBoBirZvK5aCEI7d2VAqLUvmIHcxluVJplMTuv4jo+PZ4UAzZTBwUFcfPHFePzxx7FgwQLx/lWrVqG3txf79+9HU1MTnnrqKTz55JNl2y5BEPUNGYAqITQDq6QBKEb0Z1KPqxilzLgLlW3cbjf8fj90Oh2cTie6u7vLdpzrYQUgnU7D7/cjHA7jX//6F+x2OxYsWDCjRlgAEP3fr0H5pbvF24mHvwP5v31H/LuSNBi0iI8Fy/6+meI/k0T4WLUeY7t9UjGfisULPlYKvi99FgBg+99fzeh9yrHi2NDQgObmZjQ3NyMWi8Hr9WL79u2QSCRixaTJSk1mjqXeyGeQcs3A8PAwDh8+XFUzMN0VgPHx8ZJm4j/72c/i73//O/x+P5qbm/Hf//3fYh7V9ddfjzvuuANDQ0P44he/CODoteitt96CTCbDvffei3PPPRepVArXXnstFi9eXPJ4CYKYnZABqBJCKdBiLrKlkFu3vhLdW6vJVIKbMZZV2Uaj0cDpdKKrq2taF9upqFUOAGMMw8PDYnyzxWKBQqHAqaeeWjHRkgqGIZydmbP/yZERVHqeuNhcAPn/uw+JtV8Qb2ua7EVvI+Qdg745O0Z/slwAmeLoz2MqPv2VEIVh5t/DcoccKhQKtLS0oKWlBZFIBF6vF9u2bYNcLofdbofNZisYu16P4Y9TCe3M6kmZZmD37t0wm80VMwMzrQJULL/61eQG88EHH8SDDz6Y97G1a9di7drJu2ITBHF8QgagRKYbElLOXgCC6He73WIYiFC3frZTqGRqMBiE2+2G1+uFSqWC0+nEvHnzKiL6M6lmFaDcMKbc3IV//OMfZRMp0f/9Wt7bmasAxWD40CoAwNg/t5RlXKUge/ZeZNaLKjT7D2DSZmAR3+iEMKCj7yeB0qRFyDMKAGjQKMBJjh7/WCA65fhSiaPG0fX5dXA+8Jspn1+ISoYOqlQqtLW1oa2tDeFwGF6vF++88w4UCgV4nofFYskSsfVYcrOUmfZCZiAzZ6BcTdbqKQeAIAgiFzIAVWKmvQAyRX80Gi1bGEi9kbkCEA6H4XK54PV6xXKWJ598clnKWU5nPJUiEAiIKxqVCGOajNwwoJnSsroLIc/ItF4r2/IMEid+omxjKYQQ/lNsTL/4XE+BW4gAACAASURBVMWx806hUxZlAgSE+P9EuPTwoqmqAJULtVqN9vZ2tLe3IxQKwePxYGBgAGq1Gna7XQynmc0GIJN8ZuDQoUMIBoPiysBMzEAymSQDQBBE3UIGoESEkJBSLwrTWQGohugX9qdeLurxeBzBYBD/+te/IJPJ4HA4sHLlyrKHThVLpZKABXPj8XiquqKRD2H2X6otnEicfOy7aPnEGgBAbP8BAIDSZj76YDwOw4plGNu6rSzjKea7JXv23rJsKxchATiTeKj4br+lmIpiGLr5CqSv/ErVw240Gg06OzvR0dGBYDAIr9eLAwcOQKlUIpFI1NVvRjqdnvGkQD4zIHRcnq4ZSKVS0xpXuZOACYIg8kEGoESmeyEu1gAIdbzdbjcikUjFZ/qFlYlaXswza9gLLF++vC4qlJRzBSAajYr7KZRrXLVqVdXNTfrpH1Xsvac7+68988yinhc/fAT4xX8BDn7CY+lkChKZFOpmx7TGYGq3TJoALIT/CCg+KBGqyCgVKoT9lJNaxt0L5VN1Oh06Ozvh8/nQ39+PLVu2QK/Xw263w2Qy1fT3Y7orAIXI7bg8XTMwkyRgWgEgCKLSkAGoEpMZgFzRb7PZ0NXVBZ1OV/FxCQag2iI0Ho+LYpgxBofDgRNPPBFSqRRbt26tC/EPzDwHIB6Pi7X6AcDhcNSNuSlEbgUgiXJi/Hz6gyojkV27AQCqeZ0AAMeaFXC/unXKbTSsXI34yy+VNK7cmH8AkNvtSI2PT/na0QPeCQnAAMDSRz/biG900tc3aKZfO1+mkCEZy/7u25a0AgB8Owaz7s/XE6BeZts5joNarYbBYMCiRYswNjYGr9eLvr4+GAwG0QxU26yU2wBkIpVKYbPZYLPZSjYDlANAEEQ9QwagSkil0iwDUEj0a7Xaql5AZ5qbUApC4yq3241kMgme53HCCSdkdSlNp9M1L7uZyXSqAOVWZnI4HBP2czbCqVRgkUjexyL9+9BgnLlhFZqa5SL/66NgABqaGo+uAuSgm3dUUIcGDgMApGoVUuH8Y1U7LAi7h/I+lg+tw4h4YOJ7yTVHZ/7D/vztxdTWo8dDYdAg+UEH4skYvOJC8e+hm6/IeqxeKu+k02lIpVJwHAej0Qij0QjGGEZHR+HxeNDb21v1bryVNACZTGYGLBYL7Hb7hN/v6ex/LBYrWzM/giCIQpABKJHpzsTJZDJEIhG43W64XK6aiv5MKm0AcsUwz/Po6ekp2Liq3hpvFRsClEql4Pf74XK5EA6HK1aOtVzhIA2NDsSPuCd9Tu7MfyHxX06SySSCwSDee+89OBwOWCyWbHGXLv7cCB/K3j+FSYfYB12Kc2P1k5HCcf6Tif9CqMzHV3K+QL5wQY7jYDKZYDKZxAZcQjdes9lc1so6+ajFCkk+MzAwMIBwOCyagXr6HSMIgsiFDECFSaVS8Pl8GBgYQCgUQktLS81FfyaVMAD58hiKLVNaD8ckk8lWAIRkQZfLhfHxcVitVnR2dlZM7AjmqNrHiMsRV3KzETGPryLbam5uxvj4ODo7O+HxeHDgwAGsVh2bPWcs57OQTDwWUnX2SovSZobDZkZ6Bg2+cuP/BQrN/ueSLwwok8gk5qrzyXsw9CRg+fHGorZVSaYS2/kacGWGzPA8X/bfvmqtABQi1wwMDQ2JZqC/vz/vygBBEEStIQNQAQTR73a7EQqFYLPZ0NzcjJGREcyfP7/Ww8uiXAYgcwY8FArBbrdXLY+hkkgkErGrJgAx3MHlcmF4eBhmsxnNzc0wGo2z+gKvWLAAySOHqrpN7VlnI/jyS9CedTaQI+y1Wi20Wi26h7cjjYmrKAm3B/I8icC5CJWKwi4flObsyiqshNWEUih19t/YboPvc5eKtwevuBAa28TvzdDNV9TcBJQy256bTJspjK1WK3ieL8sKWa0NQCZSqRR2ux1WqxVbt26FTqebsDIwmRmol3wPgiCOf8gAlEihH+5M0S9c4DJngwOBAIaGio87rhYzMQDpdBp+vx9utxuBQABWqxUdHR0V6apZK4QQoPHxcbhcLvj9fuj1ejidTixcuLCqF+vpJCSz3/0E3MU3TbifkzdA0daK2MBgVvlP6QfJh8mh4aznJ0bGAIxBbjZOur2YxzdlInDDytVFjz+tzC8QixH/ugUdH4x7+kSGxgvO/ucjU/yzNMt67WQ9AIztNoweqMyqSjmZrkAVhLHdbhcnC/r7+xGLxUQzUCgscCrqyQAICCVAM/c51wDZ7XZoNJqs38pgMHjc9XYhCKI+IQMwA/LN9BcKASlnJ+ByUqoBEJb13W43RkdHYbFY0NraWrWEv2oidB8eGRlBIBCA0+nE/PnzazpDV4oBYL/7Sd77OfmxCkQNvA2pUGja41HwNkAqRSwnn6DYakDas87Oup0b4iQd6EWqrYhVMwmXNz9A0dmJ2L59UJoNiA7PzAxMRTEz/+YuB4b7Js+9qGfKMUMtlUrB8zx4nhfDBXt7e5FIJGCz2WC320tKmK/HWfNkMpnVAyDXAA0NDeHAgQOIRCLiyoBGo6EeAARBVA0yANNgaGgIR44cQTAYLDruezYbAMaYmNg3MjICs9mMxsZGLF68+LgT/UKittvtRkNDA7RaLVQqFbq7u2s9tGknSGcagcDZ1yIzuIRTKIEPDICssRkseKykpkSjQXoqc5Bn5pVTKsGixXfJnTF5ZuiFCkASE/LG/mub7QgMeio+tHwIJUALkRv+o2u2AQACh3zYsWMHeJ6HxWKpiegtt9gW+mE4nU4kEgn4fD7s2bMHqVQKNpsNPM9DoZi8BGu1OiWXwmSrEvlWQ/bt24cbb7wRPT09kMlkNe39QBDE3KC+fjVnAYIIa29vx6mnnoru7u6iQl5mmwEQRP+uXbvwxhtv4MiRI3A4HDj11FPR09MDs9lc0QtUNStoxGIxDAwM4M0338SOHTsgk8mwYsUKrFixAhbLxNrxtaIcFZL27dt37MYH7yWZZLZVUuYqRgXhJv4Uyf/0s5LfRtPWVI7RlITKrJ109l9pOiroDS3Z55JUXtr8i67ZBudD38fY2Bjeeust7Nq1C8PDw1Utm1vJ2Xa5XI7GxkaceOKJOOGEEyCRSLBz5068/fbbOHToEOLx6SdxV5tiw5KE1ZClS5fimWeeQXNzM9577z2sXLkSt99+O3bu3Dnp66+99lrY7XYsWbIk7+OMMdx4443o6urC0qVL8fbbb4uPDQ4O4pxzzsGiRYvQ09ODAwcOlLSPBEHMbmgFYBpYrdaSL4L1OpsjlUoR/WC2ljEmxroPDQ1Bp9PB6XSiu7u76rHuQr3xSpHbk8DhcGDZsmUT6m+XsxPwTCnFALA/3pv3/mWDr0+4T6KaGHsts5gn3Jcb/69onNhxV9nWcuxvU20SwDmVCijQA6DaxMZCkMikiIyE0aBVIRVLQKpsKCj+1Zapw4i6urrAGMPY2FhW7X2e5yuef5NKparSxO7/s/emQZLc5bnvk1lZ+74vvW+z9OyrRggkZFlICKwQPtgefLGM8UqICIyN45hLBCEIzkUOCPAJi4jjY/sam2sk6xr7yoeDFRaLDEZoZiTNqtHM9Mz0bF3VVdXd1V37ksv9UJ1ZW2ZVZtbSNSJ/X2a6KivrX/v7/N/3fV6DwYDR0VGMjo6iVCohkUjg/PnzIElS2D0f9PBCJTSXAMnB5XLhyJEjAIDPfe5z+O53v4unn34af/3Xfy1ZFvSxj30Mn/zkJ/Hkk0+KXv9v//ZvWFhYwMLCAk6cOIFPfOITOHHiBADgySefxOc+9zk8/PDDyGazQ5dF0dDQ6C+aAFDJOyVFS5Ik8vk8rly5gmQyCZvNhlAohLm5uS1rrOOzEr2+f4ZhhJkEpVIJgUCg7UwCYLjmEnRaC03TwtThQ51OVn8eU+d6a+P4GNisPMvLeqwRn6LjjYsnATROQdXdXGi1/9ykvp9Bqg9gq7CGvADHgSlVWnb/5eKZn0IlnW25vH4QF++9v7S01OC93w/ryX4LczGMRiPGxsYwNjaGYrGIRCKBs2fPgqIoBIPBofl81tPtFGCbzYbjx4/j+PHjbY+///772+7cv/jii3jyySdBEASOHTsmOJilUinQNI2HH34YALTGYw2Nn0M0AaAQgiDeEYF/NptFLBZDLBYDAGzbtg0zMzND4abRy1133qkoFoshm83C7/djbm5Otj3psGcA+McXjUaFAWTz8/NA9LWBr69+958wmeD54PtRevOUonPoygUQhFNV+Y9c9AE/KoneOO5YfPYGO1HHVATpxeqk4lxsRdm5ZOz+i1HvvV8/lKpQKHTtsNPMVjfcmkwmjI+PY3x8HPl8HvF4HPl8HufOnRPsN5XuvPcDNRkAoCYAesXS0hLGxmqfy9HRUSwtLeHOnTtwuVz45V/+ZSwuLuIXf/EX8cwzzwzF97+GhsZg2Ppvyp8ztjJzkM/nEYvFkEgkYDQaEQqFsHv3bqG+f1joNujmOE4Y0MU7FU1MTKhyKhpGAcD3Z0SjUayvr7c0onP/+izQod+EKxWrDcAAUCkDekNDA7BwnMzpv2LlQBVnAMad8yi9fVHWOXgOQLlLDuX1obK8DJ3F3GJhapoaR3HxFgDAuXeH4nPX024CsGMq0tW5g/ftR/ynZySvJygdHJPtP6f1Q6lomsbKyorgsMOXzjSXuSlhqwVAPRaLBePj41hbW8PMzAwSiQROnz4Nk8kkNEpvVUDL24AqJZ1OY2JiomfrEMuOEAQBmqbxk5/8BKdPn8b4+Dh+7dd+Dd/85jfx27/92z27bw0NjeFGEwADRKfTgabpgdau8q428XgcFEUhFArh8OHDwhqy2WzPJwF3i06nUxx08zXRfP+Cy+VCOBzu2qloWEqAOI4DTdO4fv061tfXu3t8FitQbAzs2dQqiB68LwmbQ1RIyIGxOEEylY7H6QMB4f9cqSQE/5JrMvVm91sMsWFi9VkAKcyTYyjcuC387d4r7jIlVv4jF/7zHgqFUKlUkEgk8Pbbb4PjOEEMKK3nHyYBANRKbaxWK6ampjA5OYlcLidMkbZarQgEAgN3TaJpWpXQ6nUGYHR0FLdv195nd+7cQSQSQaVSwYEDBzA9PQ0AeOKJJ/Daa69pAkBD4+cITQCoQG1QSFEUGIbpuwAoFotC0E+SJEKhEA4ePCj6Y9+rScC9hCRJ2WvKZDKIxWJIJpN9aVre6gwAn7WJx+Mol8tCiY/U45Nq/hWuLxZagn8AIG12cKXNZvBKBYReD8LjB5LL4GS6r3CVClqkiNkqLwsg4gKkBqaHzb+2iA/ZaGMZj30ijMzNWMux9omw7PNWsgWsvX4BnoPzcKWk+yrcOyZBZ9XPaKhHr9djZGQEIyMjKJVKiMfjOH/+vOBC4/f7Ze1YD6sA4CEIQpgiPT09jUwmg0QigcXFRdhsNgSDQbjd7r4/hm56AHo5B+Dxxx/Hs88+i+PHj+PEiRNwOp0Ih8MIBAJIpVJIJpPw+/344Q9/iMOHD/fsfjU0NIYfTQCooBsB0C8r0HK5LPjXcxyHUCiE/fv3d/TQHlYB0C7org+KzWYzwuFw3/oXtiIDUCqVhNeSJElEIhEcPXoUFy9e7Honk4iMg4veql1QHzwZTUC5DMJgbGi6JRTuEneTBQAADEnJFVAVAZk71X4B3o9fuG4shMzNWEvwn76+1PG8hK7za6hkArESjEZjQx29ktKZYRMA7dZDEAQcDgccDofgcBaPx3H16lU4nU4EAgG43e6+lGR2UwKkJAPwkY98BK+88gpWVlYwOjqKL3zhC6hUqlm0P/iDP8Bjjz2G733ve5idnYXFYsHf/u3fAqh+73/1q1/FQw89BI7jcOjQIfzu7/6u4vVqaGjcvWgCQCVqavl7LQDK5bLgasMwDEKhEPbu3aso9TyM8wnEBACf1VheXhZKG44cOdL3bMqgMgA0TSORSCAajQq2pGICTqkYYcdnQd66Kn5lc5BXv9PPMOCS3U2spa6cBoLyfPkZk4J5A02BcbvyHx7znt0o/sdP5d9HHZ75KRhGqvX95SXx0h6xYN0+EULmpvrnsJvyH7lYLBZMTk5icnIS2WxWKJ2R2i0fNgEgd6edIAg4nU44nU5wHIf19XUkEgksLCzA5XIhGAz2dJo5TdNduQDJ5bnnnmt7PUEQ+MY3viF63cMPP4xz584pWp+GhsY7B00AqEDtj0Qvgm3e6nF5eRnlchnBYBC7du1S7fIxLDXu9fBZiXK5LDxWlmURDodx4MCBjlmNXkKSZN+eHzEHn507d8IqMXxL6WvFjs/W/nC6AQkrTQCNwX8vKeSAchm69zwM5icvdz6+SWwxE3PQ3Vzoz9pk0jwTwT4RFq39V0tz/T/HcvDMT4mW//D3y/6P/xPkH/xfPVsDgIbSmebd8mAwKFiO3o0CoB6CIOB2u+F2u8GyrGCNyVuoBgKBrucpdJMB6GUJkIaGhoYUmgAYIGrLbWiaRjKZRCwWQ7FYRDAYxI4dOyQDRSUMm6UpTdPIZrNIJpMgCEJwKjK3mVbbTwiC6GkGgHfw4b24fT4fpqamZAUcnQRAff1/Q/DffJ7mMiCgmg3g35s6HQidTugJEKvPb3b9KUWXGy7jy4AqoSkwlAlUMQ3m0V+D7qV/bDkXY260ZCUqRcm1AwA3OgPizrWeef67d8+BoHRInqjuhgYfvBe5ywswT42DSSuffdAN3t3Vpsx2AqP89T8GZTai8t8/A/2nvtrzNTTvlqdSKSwvL+PKlSugaRq5XA56vX4ovju6nRdCkiQ8Hg88Hg9YlsXa2lpP5imozQCk02nZFsUaGhoa3aAJABUMIgPAMAySySSWl5eRz+cV+9ffTTAMI3j15/N56PV6hEIhwaFiK+lFCRDHcUKz8srKitCINz8/r+i91LdsTblcFQB1AQtXLoMwmhoagElPdZgVu7Yq67SEp1ozTxVr/QCGw8cajim5wqBKIqUuHh+wVm3AbR4CRqYS6EdOxn/PXkEEABCCfyadASlRVqek+bcbCGpr7CwJgmgIkE+cOIGlpSUsLCwIMwZ6sRGhll5mJEiShM/ng8/na5inkM/n4fP5EAgEZA/MUitMaJoeyKRlDQ0NDU0AqKBfAoAvCVleXkYmk2nxd38nwe+2xWIxbGxswO/3C4/19u3bQ1Nm0E3Q3dysHIlEMDc3p/qx1a8lf+k1WHY0BtOViR3Q37yk6tz18EF/ffDPlcsdm4FJd+dptwXfBMwrNwEAef8U9KXGHfZOu//03D4AgK6QBSHV2yCBnBkAwQfvBZ1KtT2GYzkYd1TPVbpUfb55C1Wu0tnGVC3NWQGxLIH+pf8p/L/y6O/19P5JkgRFUdi9e7cg2q9du4ZSqQS/349gMDjwTF0/JoYDjfMU+Me6uLiIYrEoe7jaO+07W0ND452FJgAGCEVRKJVKDZc1B8Jerxfj4+M9bUiTwyAGlPHNd7FYDGtra/B4PBgdHcXu3bsb7luJDWi/UfqclMtlxGIxwcEnHA73rFmZFwD5S9JTftnIZNf3owR2xwGQl04rug0vAliSQsnshrnS2ChLVDaFh8fXq2UKcMXq508f8AO34nDvnut4G33AL3mdIehHOS49VbhdIzDpCwC4BNJiBZuXtvts3v2nzMaGf7cC3j40GAyiUqkgmUzi8uXLYBhGmDEwiF6dfgmAeuofa/NwNb/fj0AgsGUlihoaGhpq0QTAAOEHgfGTapeXl5FKpSQD4UHBB5b9uO/m8heHw4FwOIwdO3ZI7oSTJClY2d0N8A4+sVgMlUoFoVAI+/bt62riqhg9KwFaXwURGQfW5ZXy1BZAtm8kFkG/cge0K9ByecHXOu2UrJRaLmugbsdb6e4/AIDjQBgN4EplcMWSaPDP7/6bp8YBQHUPgH2i2g9BORyAxGTj1R/8WPh/swgwREKgr1xTdd+DRq/XIxKJIBKJCM5kFy5cAEmSwoyBfrl1DWKuSj3Nw9XqhQ8vBtR+7gchZjQ0NDR4NAGgAjWBMsdxKBQKSCaTSCaTwiRXpXXg/YAvTepl7Wkul0MsFkMikYDFYkE4HMbs7KysH7itHr4lBzEHn141ZkvRlQDgWKB+Iq7S4L8Dhve+D0hu+t8bjEC5QzCvEuryGdDb97c/xusBvboGAOCKeWESMGl3gM10MZ+gCS7V+TnUjY7Bnk4LWQA5/v93MwaDAaOjoxgdHUWhUEAikcDZs2dhMBgQCATg9/t7GuRuZdDcLHySySTefvttsCyLcrmMUqmkKAuidAaAhoaGRjdoAqCP8MNn+N1vi8UCg8GAw4cPD02NO9C7gLtQKAhe/QaDAeFwGEePHlVshzeMw8mA7hx8lLB64zK8k9tbLm8WALkrpwAA1m1HAAC6uvp5jiBAyBELlL7m/qMSbtve1gsN8gOfgqO6W25dvSl9kELHnxYRYOtfYGUI+kGvb1Tvqw/vW8vcDPILd0c2oB6z2YyJiQlMTEwgl8shHo/jjTfegMViEQaOdfs9OCy2pAaDQZi0nMvlcO7cObz11lsgCEIQPp02WJTOANDQ0NDoBk0A9Bi+5GV5eRnJZBI2mw3hcBjbtm1DqVTC22+/PRQ/WPXwpUlq4CcQx2IxkCSJUCiEQ4cOdZVNGLYMAMMwuHz5clcOPt2wfv0CXNO7u8sAUHqArlT/rb9MBaTHC3ZjXd06FKLLpHrj+FPItTxevhyIhy31aRaCAiinA/RGGnmJ8h/L1Djyi7dE6/91//x1wO0FNu1bdS98Bcyv/knLccS3/hu43/hcbxfeAavViunpaUxNTSGTySCRSOD69etwOBzCwDE1n6dhLJvR6XSwWq3Yu3cvisUiEokEzp8/D51OJ4gBsbIlbQaAhobGINEEgArEfqiy2SxisRiSySQsFgtCoRBmZmYafpyGdWeboihF66pUKsKALn5qbS9r3odBANQ7+JRKJbhcLtklTP2g9NaPAUPVclLHlEHrG5sOy2e/j+aVCVkAp7v1hDKDf8Je3ZHkREpn2B0HZJ1DDEYnTyByyTgIfxBcMq76vtrBiwDSYhUVAFINwHLKf1ruS2b5j2H3PlRerU4ttu7c1vF43QtfATazbGwyDtJRDSI5lgX5/J+BPf5fa2v41n9TumyBXvSfEAQBh8MBh8OBmZkZbGxsIB6PY2FhAW63G8FgUFFGbRgFQP0MAJPJhPHxcYyPj6NQKCAejzeURPl8PiFDqmUANDQ0BokmALogl8theXkZiUQCRqMRoVAIU1NTkiUvvZgE3A/kCBN+LkEsFkOhUEAwGMT8/LzqCcSd1rMVAqA5m8E7+Jw6dQrBYHCga+HLgNavX4C5UC1lGS/HgGRM0Xk4gmjw98f6quzgP7//vQAA67U3Fd2nGNR6AtR6AvmR9lac65E9cN98o+Gy5uCfunym7TkIhwtcer1lgm87pKw/2WJRcgZAL/AeO9hyWW4z+JfCMjWO8nL1OdG98JWW69n0hqq1kOdfBrvnYcnre11uQxAEXC6XMGE4lUopHsI1jAJAagqw2WzG5OQkJicnkcvlkEgkcPr0aVAUhfPnz8PlcmkCQENDY2BoAkAlt2/fRjKZRDgcxuHDh2U5UZAk2Z9BTl0iZbvJN7rGYjFks1n4/X7Mzs72fRjZIG1AB+Xg0y+oSkHyurK16stvzmwGt6QOnDcEYqNu97rU6Ltf7/3Pk5s5CMuZV6p/qHACkstKaDf0TK15WM6uP2GzAXXBOx/08yJADvV1+6TVAjaXl74/vb6hbKgfWPdXeypyZ851OLIJic2F5iwAoK4MqJ/19iRJwuv1wuv1gmEYrK6u4saNGx1994elB6AeOaLEarViamoKk5OTSCQS+Na3voXvf//7cLlcOHr0KB599NGB2KhqaGj8/DJc35x3CQRBYHx8HIcOHcLIyMhAbej6QX0JEMdxWF1dxYULF/Dqq69idXUVExMTeNe73oVt27YNZBJxv0uAWJYV3ElOnjyJfD6PHTt24NixY5icnBQN/pUKt3Ze/b2AqhSE4L/01o8bGoDr0dGNlxObVpusOwBYpKeaMnN7Gm9nb9qZbBJoOt4BCEA+0tjATK0nGm8qUf7jv/265HrEIHyt9qIN1ztc1X9FgjGdS7rWmrRKZLX0nQMyKc9/Htf8rPgVmz7ypFfZ7IPCUvv76yUsyw5kt52vld+zZw8OHDgAk8mEhYUFvP7667h16xaKxdp7ehgzADRNyzY+IAgCwWAQX/va1/CpT30K73//+/Hqq6/i6NGj+L3f+7223zsvvfQStm/fjtnZWTzzzDMt19+8eRMPPfQQ9u7di/e+9724c+eOcN2jjz4Kl8uFD37wg8ofoIaGxjsCLQOgAZIkkclkkEqlsLq6KliU7tq1a0ssSvshAPghZNFoFKlUCl6vF5OTk7Lqjfs5J6Eevtm3V5hSUQAA7QqASsuvWW8O/pspjc8D44BOJPvAplZhSv0ICEaULXaT5vIfwu4Al0m3/CuGfmYOnAJ7UzYvnT3hIZ2uliwJaawJGMJqB5drPytA73HD43Fj7fTbstfWjNjUX57KWkq4n3p0DgeYtLTtKfXGd4Hb1WZj+olPdVwDwzAD322nKArhcBjhcFiw2rx48SIACEPIhk0AqBUl2WwWe/bswUc+8hE888wzuHLliuR3DsMweOqpp/Dyyy9jdHQUR44cweOPP475+XnhmM985jN48skn8Zu/+Zv44Q9/iM9+9rP41re+BQD4kz/5E+TzefzlX/6lugepoaFx16MJgC1gEMGkHPgBXdFoFEajETMzM9i+ffuWp9R72SydyWQQjUaFIWSRSESxgw8vSNQ8L+mr1Xp1x2yjd33iVs3lJTA+g5t34ujG/4PRm2pZAJZtqPvXFbItx5Ol1jIXsfIfpRQPPAgAsEQvi69TZvNvCxxbLfdpuLPWx0C4vAC7+d4xWwGRMiDCKG8NpNMl6zjCaofe7UVl8Xrb4zwHdoKj/KCAVAAAIABJREFUt8YEgKZp6J/7s4bLmItnobNXn1Pq//vvADDQHgCl1FttlkolxONxFAoFnDt3TnDXUWo53A+UZADqqXcBIggC27e3WgHznDx5ErOzs5iengYAHD9+HC+++GKDALh48SK+/vWvAwAefPBBPPHEE8J1Dz30EF555RXFa9TQ0HjnoJUAqURtAL/VTkD5fB7Xrl3Dq6++ioWFBdjtdszOziIUCiEQCGx58A90nwEoFAq4fv06Xn31VVy9ehVOpxPHjh3Dnj174PV6Fb92PZvAK8HNO/IcbkqmanCQcEqUkdRB1L3HiOYJu3WlP2yoOvGWnpAONqTgCOn3CuNUVsqilBYxAIC11QXsZmUD2SifD5TPB9JsAWkWKQHqVP5jUN4z0twA3Kn8x7I5nRgADKFqU3q9HWhlLSVkA9imSdqnT59WvL5mtloA1GM0GjE+Pg6r1Yq5uTkUi0WcPn0a58+fRyKR2NLvWLUZACUuQEtLSxgbGxP+Hh0dxdLSUsMx+/btw3e+8x0AwL/8y78gk8lgdbW3AwA1NDTuXrZ+u+QuRW1QyDsBDXKnqlgsCgO6+FH2R44cEXoX4vF4Q13tVqNGAEg5+PSiP0OtIMlfeg0UAJoyIX31TEsWQC4Fs0dwApJCx+/oNwVorNEMXbMAaIIXAZ1Kf3iMty+hNLYDLGUULQPqBVwyLvQdEHZHQ+Nx898AQO05WJsX0Bz8i7gesQWF6yaJjsPI9Nvngcs3Gu9nOdrwN0HJDwzlWIC2g06lhP6HI0eOAJf+HVQoDHpZmZMUzzAJgHosFovQUMsPHLtx4wZsNpswY2CQ61b7/a5EAIj99jRvbHz1q1/FJz/5SXzzm9/E/fffj5GRkaHIkGhoaAwH2reBStTWhQ/KCrRcLiMejyMWi4HjOIRCIRw4cEDUWWKrsxLNyH1OB+XgM0j3ptUblxv+LxUulg02GMpZELS8sh3G5oIuq354V35qHyyLZ0Wvq28AbrnfzUyAbmOl432IWYB2C+ForeEHAMzMA+el76uh/Md4dzhC9QK+FEisJ2BYBQAPQRCw2Wyw2WyYnp5GOp1GPB7H1atX4XK5EAgE4HK5+l5+qTYDoGQQ2OjoKG7fvi38fefOHUQijX03kUgE//zP/wyg2l/wne98Rxs0pqGhIaAJgAHTz2C7PiAul8sIhULYs2cPzGZz29sNmwBoR701aS6XQyAQwI4dO2C1Kiv5UAJBELIyALHbNxEem5B9XppUVw+vQ/W1qg/+GXPVnUlXyrUcz9ika9nJUh6sUd4sh9LYDhhvX+p4nDFXLTNQWwbEmu0g/QCK/ckuAACZuIN++Uz5PvRLKL15Sv4NOnw+RWlTftVc/gMA+pf/b3S77TBsAqCdKCcIAk6nE06nExzHIZVKYXl5GVeuXBFmDNjt9r6IgUFkAI4cOYKFhQUsLi5iZGQEzz//PL797W83HLOysgKPxwOSJPHlL38ZH//4xxWvSUND452LJgAGTK8zACzLCgO68vk8/H4/tm/fDptIjbQUwy4AunHw6QW9ciVqVwakR/ud/ILZA5JT9hqxxsbAstssQDewZhvIQhZFvQ2mSmtTcuvxdpBiAqDDDALOYgfBtAbAPKTdAXbTRYjy+UCvdM5MVBdUDTapNvahwwDf1NtwmbdxmjFfBiR2LM8bb7yBQCCAQCAgZA2HTQDIXQ9BEPB4PPB4PGBZFmtra7h9+zZyuZwwY6CXGwjdZADkCgCKovDss8/ikUceAcMw+PjHP45du3bh85//PA4fPozHH38cr7zyCj772c+CIAjcf//9+MY3viHc/j3veQ8uXbqEbDaL0dFR/M3f/A0eeeQRxWvW0NC4e9EEgErUBp69EAD8j1gsFkM6nYbP58P09LTqHa1hFQC9cPDpBUr7PTrNAFByLobUQ8dWFAf/3VA2u2AoyBMKjN4s60uEM5iAQhYUIy10pMp/WF8YZHOZkcUO5Nvbb6KYB5daBWGRDu5ERYBBOjNTb/tJWJXNxKD33wfqTNOUXzW7/z2EyWRFhcCePXuQSCRw4cIF6HQ6BIPBoRMAagJtkiTh8/ng8/nAMAxWVlZw7do1lEol+P1+BIPBjhnTTtA0rUoAKJ2z8Nhjj+Gxxx5ruOyLX/yi8P8Pf/jD+PCHPyx625/85CeK16ehofHOQhMAKhm0AOB3wWOxGNbW1uDxeDA6OtqTmtZhEgCFQkGYPHz16lWEw2HMzs5uqde3kgyAWPBP0UXQVLWO/NKlS/BYxBuT8wYnjHSrtWVz8G8vJAEAJYsHxvwadJUiGGM1yGWMVugKHQJjALTD23Y2QNkszwKzHbrkEhj/iKxjY/pJ2HEerFk8qGb9IzURYGk8Rj8zh2ZJxZE6EBLOPGTzULN62gT/sinmYdx3EKWzb4K0WMHmW8uy2pHZdgz2K70ZJKer21GmQmHRY8REgMFgwHjqbYweehiFQgHxeBzRaBQURcFsNsPr9W65GOh2CBgvbPh5AslkEpcuXQLLsi3ZD6Xr0pptNTQ0hh3tW2rAKAm2OY4TvPr5XfBwOIwdO3b09Md3qwWAmIOP1WrF/v37h2Jegpwm4Njtm7LO5fF4gKJ0gK5jK2BIaeeiVeeUIAB4+OBfDIKulsNwIk44YhSttbp9oiWsVg5RLlZ3/+soU2YY6Fp5T0w/qeicgnjhhUA+A26zz4HIbVQtOclqYEh4fNK9BDodwDCgfMp7FTru/tMVGEdHAQBkuPqvIX6zb30H/cRsNmNychI6nQ6VSgUbGxtYXFyE3W4XXHa24nPayynAer0ekUgEkUgEpVIJyWQSFy5cAEmSCAaD8Pv9ihzFhuF7S0NDQ6MdmgAYMBRFodDBgjCXyyEWiyGRSMBisfR9F3wrBEAnB5/bt28PzcA0uU3AcjCloyga7JINwDFqHAG21aaxojPBlosja/QIl1k2qhaTvACoGG3Ql7Iouqo7vcasvPr25l4BpTD+kbZOQMJxEk3Bk5mzuGHfJ+u+6NCkrON44QODqWMzcWVqHvrFi4DBgMpIdcaCfulq64H1/QftbEHTG4DTLX5dO6wOgJGfHTSEq69zORaDIRwGk+2c+eERBoApsAXlOA4WiwWhUAgcx2FjYwPLy8tYWFiAx+NBKBSCzWYb2GdWacmMXIxGI0ZHRzE6OopCoYBEIoGzZ8/CYDAgGAzC5/P1/H7L5XJPLIs1NDQ05KIJAJUQBKHqh06qBKhQKAhe/QaDAeFwGEePHh1IKnlQNpcsy2J1dRXRaBTZbLatg49OpxuamuNePD/1ZUDNSDUA89mAiq6zDWXF2FrHLQTBTdAOb8fzKaFidkGHzgKgHTnaJFn+IxeClph34KgG4xV3CPqbbwMA2MDm7nxMXuam4X74ngKSAOdwg1gXmdFQyImWE5HhUeDcZbj3bgccTqBS99pLBP+5yyJiRC78TISm75z6DAYVCgt9DezMPMhrF0VPVb/jThAEXC4XXC6X0JN08+ZNwYggGAzCYpHnLqUWhmH6/v1gNpsxMTGBiYkJYcbAzZs3YbFYEAwGe1YKpcQBSENDQ6MXaAKgC7qdA1Bf+kIQBMLhMA4dOgRDL+qQh4Tm3gW5Dj4kSQ5NLa1UBqDencjjqAbgG9YwnDn5g5bKqNYYG9B+WJdcxIRAPe0sQevLfypUNStgEOlJaLmdyYnitnfBUK65+yz6j2E2+Z2Ot+XZVTghejnrq9WtszL7CfpBZWwO+luXOx8oA/feuqnLekOjCGg40A+kkrAeuxcAwMU3RVYbC9B+IiXI6xtraZrGysoKFhYWQNN0V7X0nehlCZAcrFYrpqenMTU1hUwmg3g8juvXr8PhcAilUGpRMgNAQ0NDoxdsfXR1l9JNmjudTuP1118HTdN9G1611fC9C8lkUuhd2Llzp+znrVfWm72geS3ZbBbRaFR4bJFIBOV8Z2vLosEOU1m8TKMMY89EgBT0pjjgp/d2agQGgDJlkSUCpCA3VsE6vQ19APX1/wBAMBVwuuEuf+hV8A8AHM00TgTWG6qlSvUZANPm7rm7ZuFJBEdqIqAN1PQ20NevAAAyM4dhv/a64jWS519uuYwXAOSFH1T/3v1Q631vThoPhUIol8stTkJ+v79non7QAoCHIAg4HA44HA5hEyCRSGBhYUGYO6B0c0jLAGhoaAwaTQAMAIZhBK/+XC4HjuOwd+/evqfIBw3v4BOPx2EymRAOhzEzM6PqR5rPAAwDBEGgXC7jxo0biMViMBgMiEQiDY8t1kEAJGzTsLDta7T5bECC3Nz1JiHaDyAGrTOBYkQm3gKoWKV3JtuVA63r/XBVGhuO5Q4DA4Cr8/8Fsxe/g9P+D+JA8rstzcD1tPPuF6Ps22ysXbkDACAzImU4MqhMzdf+v1n/rxhbLXDz/eJ7gDY9PqzTC3RoGLffOqfo7vleANFzqQj+pWBZFt6lN2UfbzAYGmrp4/E4Tp8+DZPJhFAo1HX5zFYJgHoIgoDb7Ybb7QbLsojH41hdXcWpU6fg9XoRCARk9UVoAkBDQ2PQaAKgT/D17rFYDJlMBn6/H7OzszAajTh79uxQBv9qSprEHHwOHz7cdUMb3wOwlfCNyktLSyAIAhMTE6IlWnIdgIBaFiBr6k0dfrMDUDshwFOyeoVpvb3AnF9FwSL+eN6Y/U1V56zYPIDNA31ho3oBywKbwSJZKYE1bK1/viiZdaDcfqCbKLww0lGKmoDF0Nna91Fs7Hw3nG//p6pz76zcQd4ZEZrPlcA7CU1MTCCbzSIejwtOQqFQSJWd8bD0CPGQJAmHwwGXy4UdO3ZgdXUVN27cQLFYFAaOSX3vayVAGhoag0YTAF1SHzRzHIe1tTUsLy8LE2vHx8fhdDqFY1iW7ekk4F7Bl7nI2VHr5ODTy/UMmvpG5Vwuh0AggFAoBIvFgtFNW0c5bFiru7Ilohao5kl7xyyAk9zABlsLBPKUHRa6epusNShcXtZb0UlCFl1hmNbFMwhKRADB9i4TQ7GNAXJMP4kpiLsV6XMpIegHUBUBXW74ViZ2AkDLrIT6TEAzhal9MF9t2vlmGECnA5FaqdqJFgrVpt9e9u+wjGBnykMER8AlxF/TTsF/Owr7HoD57H9U73ZG+rnoFoIgYLfbYbfbhfKZeDyOhYUFuN1uRU5CDMMMnXMO37ek0+mE/geappFMJrGwsIBKpSJcXv99qWQKsIaGhkYv0ASASngXII7jkE6nEY1Gsbq6CpfLhXA4LDmxdlCOO0rhrUClBICYg8/27dths7VvOlXLIAVA82vY3Kh869YtRa8ZH/z3gixrA0dVg2ArvYEsZ4cd1bKcrHsC5rw8q892KJn8y2O8fQmlsR3C3/UNwErI0SKikSCqwb8IZKWxT6LsGwWVTTWWAJWrGZByaAqGNfkN2fVUIjPQR68BAMyJRVXn2Erq+wCaie99FMFzL6k+t9osQDPN5TO8k1ChUOi4Yw4MRwlQM2JTgCmKQjgcRjgcRrlcRjKZxMWLVaclt9sNk8mkZQA0NDQGjiYAumBtbQ2XLl2CzWZDOBzG9u3bhyolrQSxWQBqHXz6tZ5ek8/nEY1GkUgkYLVaEYlERF9DsX6Es4tF7Jtqn/Go3/3vFeHi9ZbLNhyjMNBF0DLsQoHWxt6SwQ6WrH4VVCgz9HQB63q/1M1FWTTtwlTxLcnrzwXfjz2pH7VcLuX+owSyqGzS7rDALkdBjk/35FxEICyZGRCjXgTkZw+quk/ywg9EG4FVnauDk1AwGGwpvRtGAdBpTQaDASMjIxgZGUGxWMQbb7yBP/7jPwZFUXj3u9+tZQI0NDQGhiYAusDhcOCee+4ZCqvKbqkPuMUcfHo9fbgT/coANPcsRCKRjvMW+EwPz9nF9jX2steCztaIATaGHOmEld5ouJwlSJQpE2CpWneWKRMMdOu62jUAS8FbgEpRv/MvxqL/mOL7bEvd+4C2ukDlOmcrKsHJlsv0qWVU3CHFDccCDjeQToENjYMxmKvDwroIQMlQRNXtuKntIBYvC//yEIH2maeNne9WdX88eae69SpFzEno/PnzLU5C/RoE1g00Tcv+PTCZTLjvvvvw2muv4TOf+QwymQwefPBBTE1N4bd+67fwgQ98QPK2L730Ej71qU+BYRj8zu/8Dv70T/+04fpPf/rT+NGPqoI7n88jkUhgfX0dP/rRj/DpT39aOO7SpUt4/vnn8cQTT6h4tBoaGnczd3/kuoXo9XrV5TzDMuWWh+M43Lp1CxsbG107+PSCXgoA3oUpGo2iVCohHA4r6lloXgtJsGC57sSQnOCfpzn470S7RuCsLSh6uRTrej9cAIwS9qUAcNUmbwf5vPtBzLDVgLWgtyNNuDFePN/+RireA2LBvyqsrTuxbGi869MSuzefr/Wk6PXF0CxMy9LDv8hcGuzUdsnreajpbUBR2sI1vvdR2PMJ4e/CvgdgzIqvSQ7UG98FfeiDqm8vhpSTkNlsRllNw3WfUZuV4DgOn/jEJ3Ds2DGcP38eV66Il2/x9/HUU0/h5ZdfxujoKI4cOYLHH38c8/O13o2vf/3rwv//4i/+AqdPnwYAPPjggzhz5gyAagZ7dnYW73vf+xSvV0ND4+5HEwBd0LwzLBd+t32rMwf8bvjy8jLy+XzPHHx6Qbc2oHxDdjQaRTqdht/vx7Zt21T1LIj1bZBE//sTTGQJqLsbtssBUPXBf5mywIjunYBIqem7MhhPiwT/MkQxbW0aZlZpXQO3eZ6yJyzaB8AaTNAV1PUtCHc7MlvNAvANwAphXX6Q+ZqwKgSmYI4tdLWmfpGJLkLsW+GtJRa7RgaTGWx2Erpw4QIuXrwIp9Op2kmo19A0rcrhjbcBJQgCe/fuxd69eyWPPXnyJGZnZzE9XS0fO378OF588cUGAVDPc889hy984Qstl//TP/0T3v/+9w+lI52Ghkb/0QRAF/ACQOmPzlYKAN6RIhqNolwuIxQKYe/evbh16xbcbvdQBP+AehvQTCaDaDSKlZUVuFwujI6Odh0YSE0CHiZo0gAD5JcmsXXuMnpa2re+l0R0URR0dpgr7Z2QACA2cgjhpTdkn7sSnIQ+fqPj7j9HEBievNvg6Lb8RwyyVMC+5e+BHuntrn8neCchs9mMnTt3Ip/PC05CHo8HwWBQtpNQr1GbAVAyB2BpaQljY2PC36OjozhxQryX5ubNm1hcXMQv/MIvtFz3/PPP44/+6I8Ur1VDQ+OdgSYAtgCKokDTNIxG+WUg3dA8k0DMwWcQTbdKIEkSlYq8Wu1isYhoNCoMIItEIpibm+tZz4JUOdL5G3nsmezv7lma8sBByxtylTc4YCmnGy5j9CboKlVhkNM5YWVq5UQZZzWI6CQA1vV+BNuUAPF0agTmKegb7SozjlHY03c63k6K4sgO6Oii4tIfxiyRDeI46DdqpTCsyQoyXXMlYvowg6AQmFJ0/Jp/OzzJy8jvfy+sUXlTip0Lr2FjrtqfYapkUdTLy4aRF34AJ2o9AKbcCljKALJUfd+8tcRi3/L3FK2/F7AsC4qiGpyEVldXFTkJ9RolPQD1KHEBEss6S4md559/Hh/+8IdbREksFsP58+fxyCOPKF6rhobGOwNNAGwBvADoJ2IOPs0zCeoZNgGg0+lQKkmXl1QqFcTjcUSjVTvCfpYvqS31kkLNELAM5YaV2RAGifGUKROKOisoriqWxEQAAMSd29QvGEDcOo3Ixtui121b/xmuuO5te3srVQSansLbjt0YS18Q/l4J74Fv+QKUQLAMQMp7zWmHFyStrG6csTpAcFxL7b9+6ariycGsU/x1T0zeA3s+AULFe8y8Ie78UwxMwVRnX+pceE3xubuhdPMCjBO7+3ofDMM0iHySJOH3++H3+4VM55UrV8AwjKSTUD/WpCYDkMvlZJcnjo6O4vbt28Lfd+7cQSQi3qD9/PPP4xvf+EbL5S+88AI+9KEPDU3GV0NDY/BoAqAL1KaY+ykA6h187HY7IpGILAefYRMAYj0ALMtiZWUF0WgU+XweoVAIe/bsgdnc36mwahqSY5UQPIbW5l2a7D4AKRqUDXxi9OLNziypA1k35KuT+083hEo3UDC0ljiUTLVdT6XBfz+gDVZQpWpvACPSBLzVrEweEf7PZwF6RclghxGNTcC5pauwA2Al3kMAhMZu4Tw3B/c6Sn0HN3vvSzkJ9RqxOQByUDLV+MiRI1hYWMDi4iJGRkbw/PPP49vf/nbLcZcvX0YqlcK997aK8+eeew5f/vKXFa9TQ0PjnYMmALpArQDodbBdKBQQi8UQj8dhNBoRiUQUO/h02nEfNHzQzWcyotEoUqkUfD4fpqenYbfbB1bj28vhbVLuPyulql2nzyg+AKsemjQIE3WLOmv1MkIvZAGAapbBVlTW5JvTO2GtKHMcasdGWXpHs0RaML5WdSapOEax5p6BP3ZO9rl7OZ24GZIuCTv/nMh7TL/U5NDjDQAZaWtS1uYCmRW/nrWon96rlgycMKG7BmgqV3uf2BLXaldc+glgVm4920/aOQkFg0F4vd6elQsOoreLoig8++yzeOSRR8AwDD7+8Y9j165d+PznP4/Dhw/j8ccfB1AN8o8fP97yPXnjxg3cvn0bDzzwQF/XqaGhMdxoAmAL6EUGoN7BB+i+BGbYMgClUglra2t49dVXhUyG1HTlftOuCfj8jTx8EjprrexEkdYjYpGe1pum7XBQjfX1edoEC1Wt29eBQY6q7pLzNfxFwgIbOpeyiIkAhqCg4+S99zYqNjj10oHiVdtBbFv/WcfzOA1ZoE5blkgLrPRG17X/cqEdykuu5MJ6QiCTS12fx5hdQdnaus6yNwLDqvjUXePmMLfM6C4AgP2OdP/FxtwxOBdeQ4mywMF1FpnvVJqdhOLxOBYXF+FwOBAMBrs2DFCbAVDKY489hscee6zhsi9+8YsNfz/99NOit52cnMTSUvfvWQ0NjbsbTQB0waBLgBiGQSKRaHHwketn345hEAClUgmxWAzLy8sgCAI6nQ5Hjx7d8mE/9RmA8zekfdWVkqald39NpHg2JqtziV4O1LIARb21drzJC2em9mNfJKvXWZkN5HRO2FnxBuONinK71Ibbb+7+cyKeO/VzDTKO0Zbrr4XvhwX9n+5btrihLzb2SxhyjYKJr8snV2LgHG5whu4/a1IYcqsohqT7CvhAXy18A7BwPjhhR0Li6Brtyn/uVngnIbvdLmQZ4/E4rly5Aq/X25WTkNJsQrFY7Ml3uIaGhoYSNAGwBVAUhUJBnvWiHAefXrBVAoCmaUHUMAyDUCiEgwcPolQqYXFxccuDf0C5Deit0gj0ZO25jOZ9bbMAauHLfzoRd24DBwJEcxcugIzeAw89+N1APV3o2HMg1wpUJzIBuRNli/IyFSKdAudrnLab9U3DIZIBYANVYUMm+p/h4Lk6/X7MXv+3lsubA38xSgr7SraaXjblEwSxpU5CSixANTQ0NHqFJgC2gE7BtlIHn0GsqZc0i5pgMIidO3fCaq0FtJVKZWi89/l+hF7u/rcjXTLCY6iW/6ihTDbuJprYHApko2DM6eRZDnYDv/sfLN0EAJjL6YZGYDERcC18v/B/V34ZBfcIzKn+CpSKyYF1SxjB+NnuzzW+HfpbtaZYNjAKiASrLGUASZexEhAf3iSXnLtq5bph9MMEZfMc/JX2z+ti4F3w0eIuQ53Ql3NgF06AnLtH1e07odZtpxNb4SSUTqc1AaChoTFwNAHQBb0uAcpms4hGo4odfHpBvwUAx3FIp9OIRqNYXV2Fx+PpaEs6LAKgnQ1onjYATXFI/e5/M+eSI9jrbwy8YgUfIuYkkiUPAGC7/RZKaAziSbBgofx9UOTMogPCiqwJJlL5zrkSxgybu99tessLm170FCs984EXAQX3CPTlzqVBRbMbxqJ0U64Yrry8QJdcaTzOnNssobE5gGy1nIiJTIGg6x5Pj0R7iVK2C10MTMF0S7wvQF+pidnm3X/WYAFZrl6/QoVhRVz2fVK5DZQG0ATMsmzfs4PNTkLxeLytk5DarISSGQAaGhoavUITAFtAvQAoFApCM69aB59e0C8BkM/nBYcii8WCSCSC7du3dxQ1Yjagg+bfL+jwvt1MRxvQBB1EgGofJIk1+wJArlJzBfIb10Rr5nlIsGA21YaUnShN1JrAi1x1d71597/I1sRF/WAwnptQNpSqnqhlTvTyvNEFS2kdZpEZBXIouEdkH2sqpAQRoMT3v2QPwJhprYlvCPo5kfeBTdnuLUu1vnZil20lac+0rONoq7PBEaiZc+fOIRQKwev19vQ7rXkGQL8xGAwYGxvD2NiYMHm42UlIrSjRSoA0NDS2Ak0A9ACO4xRlA/jd8JMnTwLo7xArufRSAPAORbFYDCRJIhKJ4OjRo4rs8dR47/eaEk3if50h8f7ddE/FSKzgQ9jc2hOQZayw6ao73CR699grnB4sR8JIllr6AIzl9naQ9U5AS455jKQvNlx/xXWvqBPQJLUIGvKDWoJjG8p/xJCz+y+HfgbbumIOHKXsc+xZu9r5oAFAcgxYQ2OWYWL9DADgNf2D2I9qiVTR6oMttwHaKr5rXf867TYXsJjJ4MaNG7Db7QiFQpJOO5eiDHZE5AXQ/SoBkoPFYsHU1BQmJycFJ6Hr16/DZrMJ1sVKfg80AaChobEVaAKgC5R8yfMOPrFYDMViEQzD4NChQ0Pj/tCtAGAYBslkEtFoFKVSCaFQCPv27VP9+IZBAPDcuHEDmUwG1pD0MXwWoLkBmCdbNsJBZeC3Veth3l6LYMKVgtNIY6PU2MxLozWALLDV3XwDuen/T1QDNQrSpTNyWDe3eVB1uCpJnC3Oo9M+fIk1wEjWdt0tJWWlOHJIOcbhTt8C0P08gIpJfeBVsAbguF3rG6iMb4eumGss/2mGYVR969YPAOtEMaA+g1MPH/zLpezww5BOtlw+MzOD6enpFqfr5Dd8AAAgAElEQVSdUCik2sxgKwUAT72T0MzMDJaXl3Hjxg2cOnUKHo9HtpPQxsaGVgKkoaExcDQB0CXtvtzFHHy2bdsGs9mMU6dODU3wD6gbdsVxHFKpFKLRKDY2NuD3+zE3Nwe7vXtHkV4O31IKTdOIx+MAJgEARqMRobn7JI8vMpsfIwWfpgmXuBe7mFNPLyAJFhWuVVhcTI1jzJmCnWwtT+JxVVqDunquuFonjSqlojNCTdKDI3WqRQCtM4FiGvsgWMoIku7TQLwmgc0Y1E9e5mcAdEKOAxBQ3f3vJ/VOOwzDYHV1FYuLiygWiwgGgwgGg1DyARoGAVAPQRCwWq1wu93Ytm1bg5OQ3+9HMBiUnFieyWTgdg/X8DQNDY13PpoA6DEcx2FjYwPRaBRra2uiza4cxw3N7rYaMpkMotEoVlZW4HQ6EYlEsGvXri0Z0tUrOI7D6uoqotEostksVi21KZljY2NYl3AASpeMcBjlBYznkiNCBqATUkKgzFbLVwyk/Np2Kep7AXiWdaPoZD6UcowjDxt0HAOGax+E5Y3ScwuksJD9nwFA60ww56tlWBVj4y40p9ODYLrLrDTDuAMgKo2vmZhwoUoZ0EY7aHcQVKraV7I0+4DsgF8NUsE/RxDCHIR6Gib/yuDPvmPEf/0vtfe9TqdDIBBAIBBApVJBIpHAW2+9BUtoH2KxWEtzLc9/Xq2+X989WwTLsgPtAZADPwRMzEno8uXLYBgGwWAQgUCgwUkok8lgaqo3WRsNDQ0NuWgCoEt4h5hmB59wOCzp4HM3BsrFYlEY0mUymRAOhzE7OztUu3BqyGQyWFpawm1mF4AgjoxTcDqd+K6EI+StDSfGneJNj7dK8htV64mYazvsGcYGu659XX47+MbfXhGmloE6w6qUYxwAYEEWZVR3LUOouhotdywQ6h38OsqECaHU2w3XFetcaAq2AMzZxsbevDUAsi74NuRTLXMBOJ0eBF0GUZZ2SnJceRUw18q39GsxsJZqSRGZuCPMAgA2+w4oQ9clS524M/Ne+DI3Ja830nlJNyFLrvY+tOYam9qPVX6Eot7T8f7LDn/D32/q78NDh6Qfs16vx8jICEZGRnApyqBUKgnNtaFQCB6PR/Q7dNgyAEB1Tc3CpZ2TkM1mg9fr1XoANDQ0tgRNAHRJNpvF+fPnYTQaEQ6Ht8TBp1/QNC0087Isi0gkgkOHDvXcB3vQlEolRKNRQcxEIhHc3uzJPbXkxcMu8YDl1ka1Tncx5cSUW9r5BABWCxZY9e13kftV7qOWDGtHiamVCYWpZZAcgzJlhoHu7DHPC4EKjB2OFMfLxLGqC6q6rRglUy37ULAHYc7EwVIGGCo56EviIosQc/nhaXfdJmQ+LYiAZnTFHNimScJiDcCrjnEEk+L2nd2SNbqhZ/pU4tQDJicnMTExgWw2i+XlZVy7dg0ulwuhUAios8YdRgHAZwCkaHYS+sEPfoCnn34aNpsNkUgEDzzwwF3/3aqhoXH3MFw51LsQi8WCgwcP4uDBgwiHw4p+lLaqxr0dfLPymTNncPLkSZRKJezatQv33HMPxsbG7tofKIZhEI1G8frrr+PMmTPQ6XQ4dOgQDhw4sFl/XOPlt+S9hss58V6H9aK5Y/DfTJap7SRnGPmNkfUNw73e/edLQ9rVh1t0BaR1jTvDenbwAWZRhvd8wd5eXBjy1b4MXb5qVdpu978TK3seVn3bepLuOXgyt3tyrqzRDUehfT8HAFnlT7St9nzTVmfLzn838M21c3NzOHLkCHw+H5aWarMz8vn8UAoAsQyAFBaLBb/0S7+EU6dOIRKJ4OLFizh8+DB+//d/H6dOnZK83UsvvYTt27djdnYWzzzzjOgxL7zwAubn57Fr1y78+q//OgDgzJkzuPfee7Fr1y7s3bsX//iP/6j8AWpoaLyj0DIAXaLE2rIe3uVmGH7E+L6FYrGIn/3sZ/B6vZiamoLD4bgry5V4OI7D2toaotEo0uk0AoFAy8ThXtNuCJgabuWDGLe0zhgoswZVfQAs173m13E0GKL6vm92/VFC2iwvaFwyzgIAJsvidVl88C8lAuoD/6R7Du5ctOUYPvjnEURAWrxZWym64qa9a7nYkgVoht/9p929y4YAkBX8cwQBbohmElxZ5gC4MD/vxX9uJksWFhaQzWbh8Xjg9XqHZlOCpmnJRl8p+HknX/rSlxAMBvHjH/8YiUTrLAqgKjCeeuopvPzyyxgdHcWRI0fw+OOPY36+Nk16YWEBX/7yl/HTn/4UbrdbOJfFYsHf//3fY25uDtFoFIcOHcIjjzwCl0t5j46GhsY7A00AbBH8MLCtFAC5XA7RaBSJRAJ2ux1Go3GorEkBqGr2y2azWFpaQjKZhMvlwujoqKT3uBz+1xk9Jut+J3WbyynTJAxUY1nIelH5Lnz97n89t/LyAkAa+q7tQKUoklaY2BzKVOPjKhIWtKtgkhpUxpM2+1HhDNATysQDX/sPVOv/AWDdFIKruCzr9tbyRkP5T1fNvnX1/+sTB+G6+ab6c0mgZvd/3RaBK9sqcqQwFjdQMjnBSXw+WJICybZOLt8q9u3bh4WFBTAMg/Pnz4OiKGEy71Z+nyrJANTD9wDodDo8+OCDksedPHkSs7OzmJ6uDmk7fvw4XnzxxQYB8Fd/9Vd46qmnBFehQCAAANi2bZtwTCQSQSAQEL4fNTQ0fj7RBMAWwQsAo1FdvbRaSqWSUNev1+sRDodxzz33gKIovPnmm0NVlqTT6WQLgFKphFgshlgsBoPBgJGREdlNyvUlP2sZCh57LdhhWWDIzEYGDi8C5NIp+AeACtfbXdt1U3WeQbMQuGOYwWjluuzz8Dv/3fCq83HswELX5+kGuSKA3BRAxqJ4T0u9KDAU0yhvzk14hfhFvBffBzgWV5zHMJvtvfjpRDAYhNvtRj6fx/LyMt544w3YbDaEQiG43e6BZy/VliXl83lZWcmlpSWMjY0Jf4+OjuLEiRMNx1y5cgUAcN9994FhGDz99NN49NFHG445efIkyuUyZmZmFK9VQ0PjnYMmALpE7Y9MLyfvdoJhGMTjccRiMVQqFYTDYRw4cKBFfAxyTXLoNAyM71eIRqPC4+pFk3K+RHYM+vmGYKC9FWihQsGsrwoKuRagQGMfAD8ErFecW/Zjb6i1HGS1YIfNUK17D1PLLTv8U/Z4w2UmsoQiW30PpXUeOJi1nq6znsmMhC3TENGvLAAAWAuryJm9qm8vt/5fDt+3/0rtD6K36vjv/9OOo9PrLROBeQtQnvpg22KxYHp6GlNTU0in01heXsbCwgI8Ho8wbGwQYoCmaVUZALmTg8U2Z5pvR9M0FhYW8Morr+DOnTt4z3vegwsXLgg7/bFYDL/xG7+Bv/u7vxs6G1UNDY3BogmALlH7w8JnAPpFva89P4Rsx44dbXea7gYBIDZ8bPv27aonir5wwgi3rfF1KJZJWEyN91sf8NdTKJMwG1jkygZYDdVyFhMl/bqSRPW8nRyAymznj+ZywQujrgK3oftdawBC8C+FkZN2AupFb4FS0owdDp30ALOMyadquJhS8pHtio5PeWbgXlPmpd+ODWN3DbgkWwFLtg6JAwCOqAbZLNn6fvyH1UdxZEpa9L2plx6e18ylKNPyd7MIqEdst50gCDidTjidTrAsi7W1NdnDuHpBvxuTR0dHcft2rSTszp07iEQiLcccO3YMer0eU1NT2L59OxYWFnDkyBGk02l84AMfwJe+9CUcOyZvQJyGhsY7F00AbBH9EAAcxzUM6fJ4PBgbG5Nd/z5sAqB+PfX9Cg6HQ3T42P8+Ww1iPrBPWV13Klv9GDQLgW4pVNp/vBZWvZjzropet5yxwW0pgWblBdb1bkDtOLcsHizeyFUDiRHrCjYq6sRUMzE6VM0kSMBwOugH3GOuY/vTK6EEOZaqg4ZU8LzcT7yCH3Pv7d9iZNCpNJAkSfh8Pvh8PmEY16VLl8BxnDCMS6+X95mRi5oMgJKSyyNHjmBhYQGLi4sYGRnB888/j29/+9sNxzzxxBN47rnn8LGPfQwrKyu4cuUKpqenUS6X8aEPfQhPPvkkfuVXfkXiHjQ0NH6e0HKAXTIMGYBCoYBr167hZz/7Ga5fvw6Px4N3vetdmJ+fV1QLO2wCAKjWvZ44cQKXLl2CzWbDsWPHsGfPHni9XtHgv/n/SljL1H68TXp5W8c6GZ8gXgjwu/92g7IAMFms1l3f2FBfAqIEPmgvkq3ZoiIhPkSKZ530iV5eIKrnUjMZuBmSY0ByDAJEq7jgewF6QbMD0MZ2eTva7479P8L/L4fEmzpzjsad2zXPrOx1KZkKbCxJZ0hIlQ3QtKH1PXDVdlDVudrRnBWoR8luOz+M68CBA9i1axcYhsHZs2dx/vx5JBKJnk1lV5MBKBQKsFjaf6Z4KIrCs88+i0ceeQQ7d+7Er/7qr2LXrl34/Oc/j3/9138FADzyyCPwer2Yn5/Hgw8+iK985Svwer144YUX8OMf/xjf/OY3sX//fuzfvx9nzpxR/Bg1NDTeOWgZgC1Cp9N1JQAqlYrQzEsQBMLhMI4cOdLVrtYwCACWZZFMJrG0tIT19XUYjUbs379/4M3S6bwONEOgTAMuW/sAQUdUd/EYlpQs/9GR4udYWPVixiPfavLGhheTzlrWoEjrkYKjoQxoo2yD06B+mrAURcICK1u7nwJrgp6oPV6x4H+VrV1mFgn+m52ALKT8ZmMAMLG5BqFSbtNczIGEOb+i6PxS5GcOwhK93Lb8Zy24U/I6WldbJx/8r3lmRQeDKYUm9KA46eDekY2B1XX+niDpMhh9Y8kM7wb0LuNJ/L94V3cL7QK15TZGoxHj4+MYHx8Xho0tLi7C6XQiGAx25RamxrEsnU7D6RQvLxTjsccew2OPPdZw2Re/+EXh/wRB4Gtf+xq+9rWvNRzz0Y9+FB/96EcVrU1DQ+OdjSYAtgiKolAqKRuYxAfH0WgUxWIRoVAIe/fu7ZltZ7eiRC0cx2F9fR3RaBSpVAp+vx9zc3O4c+cOQqHQwIN/AKCZahBgoDadgERijdtxAmNBDsUKKStjUKSrQZfdUMDCqrzdfGpTOKTK7ctyaK79R/lMTNxSdNK5ikShGoDMWW60tfasx8qmQRP9//rgZwCkHONwp28hCwcc6I03fyfo0ASA1t16HrHgv1MjcMY33fB3ytN/Jxa5DcBlowOGUm/6SfqNmmC7GZvNhtnZWczMzGB9fV1oHvZ6vQiFQn2dF8LDW4BqaGhoDBpNAHRJNyVAuVzn3c7mplefz4fZ2VnY7eJTaLuBt90cFPl8HtFoFPF4HHa7HZFIBPPz88Jzyg/JUcP/PqtX3Aughk7NvGJkyu0bEeU0ANdTFRYW2PW10pCNTcGgJhNQJkwwcK0NwRVOXnYpCwdskA4kV00jYLiqouJ3/4usGWa0X2v9DIBm2u38Z4xe2EvivRaduOk9DF+lZqe5sf0+6Luo4a/f+R8EFwxHEEbrIDme5tp/JSLgV0ZeBcrADcMOTJYv4artoGAHWjFYZYnJP/zzIv78D034j7cceGCXMvHRK2cfgiDgdrvhdrvBMAxWV1dx7do1lMtlBAIBBINBWZsQataTTqc1AaChobElaAJgi+jUA5DNZhGNRpFMJuF0OhEOh1uaXnuNTqdTnJVQCl+6FI1GQZIkIpGIMIegmU42oN3ywonBZxaAWiZAKU5zCRsF8TWXmKoIUIvDUMQq64NdVw3C+SFbPEVSXkDXK9YqHrhQq/Hf4NxwY0V099/E5pDm7DCRtfduxtRYjpQxemErpZB0z8GfGoxHf5Jo349Qv/tvqtQ2A1a82+HMxaAv9m83ntEZVPcANDNZvqTqdn/4560i8+j0erfL6QqdTodAIIBAIIBKpYJEIoG33noLJEkKw8bEvqvUzk9RWgKkoaGh0Ss0AdAj5Ho584jV2xeLRcRiMSwvL8NoNCISiWBmZmZg0y3lZiWUwrIsVlZWsLS0hEKhgHA4jH379nUsXZIjAP7hp9WAuFOdPn/c/3Ffe4FDMwAl4+m+HSfgdlRfb5olqmVAMj9NAcsGNgq1EqDlrAMhWy3Yqwbz0qyV5Gd/zsflTROWyzrhhYtr3E0nwYIFiQxdFSB2Ko8YLb8Zt7g55yCl88ME8d31HG2CW+bzWzCo21G94Lwf+/LflX18WW+FoZLDhiUEc6W12TbM3EZMN9ZyuZ4uoUIZYaxUMzZFfX9LTerLfzZMAfjKi9CXq5/zvDUASy6BsnFrdqHf/W4/AOlG5a1Er9djZGQEIyMjKBQKWF5exptvvgmr1YpgMAiPxyOUIaktSdJKgDQ0NLYKTQB0SbcuQDRNIx6PIxqNgmXZng2zUkMvm4A5jkM6ncbS0hLW1tbg9XoxMzMDu90OgiDwvbMUaJbA4wekdyG7XQ9fBsQH//X8w0+N0Hfx7nc7CJiN0qKj2QK0ftc/kW/c8cuVdYII4Mt/3BZpoVKkdTBRDDgOKDE6GHXV50jMi3/cncOtVG8DzA3CAydX838vsI1CjhcCQLWcyUA2Zrr48h+gFvyLwdf/y6XIGhuyAJ24ZD+Gnes/AVAN/hvO5RntyQ58mKn5tl8OPYgAF+v6nO0ocmaYybysbE3R7O7tfdPS31l37txBMBhUZVKwku9/LX4nzGYzpqamMDk5iUwmg+XlZVy7dg1ut1soEVIzBEwTABoaGluFJgB6gFIRwLIsNjY2kEqlcPLkSQSDQezatUu2HVy/6EXJTaFQEOr6rVYrIpEIduzY0bI7RrOdn7NerKc5+P+XN2pBSrEMmERiFrlZgHaUaBJGisV6vvoR89k7l1vcXHdhwrXeNgNgoFoFUYnRwW4oixzdPzYIj6rbxegQ7FR7G8tbuTBshsZAfpS6I+v8RdYIs86AHGeDlciq6tGop77+Xw4FvR3dG50OFrHdf5Le7M0wu2EqVMuuxOw/ea5Ru6CHtFhnWRZnz56FwWBAOBzGf/sHO977gB+0Cn0/yD6lZgiCgMPhgMPhAMuySKVSuHPnDjKZDAiCQD6fV/Q9vrGxgVCod9a1GhoaGnLRBMCA4DgOGxsbiEajWFtbg8fjgV6vx7333juQMfVyUDubgKZpoa4fACKRCI4ePapqR6yebpqAeVw2FuvZ1p1xRiSGkApGKF3tumZhwAuZekGzXmhVFfly9YYWQ+1OzIbqIgplErmyDhTJ4ea6Cx5r511shiNA9qgo32EogiKlX/ci17jDz4HoOrBuxrDZDHwrF5Z9mxgzgrBuqe0xOqL2QmeNtR3vpHsOEHnIZ8MfBFAtY5oovi57LQW9vaEMiKprrr0cehDbl38kedv6+v+8vfb4K6budoZLRruqvo2y0QHU6WZeBFDlPGiDRfhXCc3Wm4Bd+ExJ1c+v5K3wWVpLElmWHVhZZDtIkoTX64XX60UqlcLi4iKuXLkChmGEYWOdMrnZbFbLAGhoaGwJmgDoAQRBSP6I5XI5xGIxJBIJ2Gw2hMNh7NixAwRB4Gc/+9nQBP+AspIblmWxurqKpaUl5PN5BINB7NmzB2Zze4ebZv71tF6yDEin06FSab3uf/y7AX/wPnk73nzwH3ZXEEvVdtad1mpgWKoor9u9ESPgU7nNmy/rBBGwlqHgsQ/edlUO8ZIPQaM8z3y5guB8egYR20bL5Ya6OQBhS7W/IEM32p6eWNuBezzymk1znK3lXyux2dyMxozQ26738A+ihWYHIDHKKuv39bS8UqWc2QtrQbmD0amVavmU1cAIwfw13Q74sNbmVtIUzW5QTPcGAbz1JlBrAD516hQA8YFpYqidAdBv7HY75ubmUCqVkEgkcO7cOej1eoRCIfh8PtE1a03AGhoaW4UmAPpAuVxGLBZDLBYDRVE92xHvN51sQDmOQyaTwdLSElZXV+HxeDA1NQWHw9GVkPnn1w345cOtAX27DECzCFjPkh0bgethuc7rjSVZhP1kQ2ZgLUNiNMCi15Y4ZgOLCt0fMSimTc8t+7E31NkbXvKcKrMARcYAI1l73eqD/06kieoOvoOrOQHVZwEYVgeGI2AgO5dbNQuBXlDQSzdnp4I7elL/n9G5kdG54fv/2Xvz6EYO+87zW7hBAATB++yDTYrdTfYlNbullj3xczKW3Un6ZXY8tuKsjzizlidKZp/y8uTd9Ub2Uw634yQbO4qjf+LYHseSI00sKeNIkS3bkWSpm+y72eyDN4mLJO77qqr9A11AVaFOoEBSUn3e4yOAKlQVgALqd3x/v19eOvPBMNa2BpG6atXUE/mXwuMkEU0ZcavwywCUf3d3ogNQKpUqx2S1WjE0NIShoSGk02msr69jZWUFTqcTvb29nOnsehtQHR2d7WJnW6TvEAiCQKlUqgzpKhaL6O3txbFjx7ZliFW9iA0Cy+Vy8Pv9CAaDaGlpQX9/P8bGxhoexMMgNkSrnhoAxhH46ZtJ3Hu0HFnr85QNwhabOoO1v9sgaDxrSSRp4hQTs+U/axE7htqFrbciaagU//JJFix48wrwkRP11QWwZwDw5T9i0BB3XlLF+gfVpUvV52ZIO1qMwu8HSWlrEMaLTrjN2kxTbsTwTxo9cEB9BuDe7mXO/QXjfnSapaP/JZMNplJta042poJ0/QabTnsCoay4cWsxUgCMyGbVfce1GAKmNSRJCgZ4HA4HhoeHsXfvXsTjcayvr2Nubg7pdBotLS16EbCOjs62oTsAGlAoFDA1NYXu7m6MjY3B6ZSe2rpTYUuA+N2J+vv7MTk5WVcXDznECoLZDsDf/VtVSyuWbJiZozDYVzYMevpqpRluewnxrPJTviigzmltqRorNM09llDaik4F+n2lJNMEZtMtODjENbpyxbuRRgEHIFmo1Rzvbk/jyooTbqe0N8PMAKjsR6Hx3wxcphQKlPpOWEZC+jVakIfPMyG7HQNBIUfV77zzawLqxU5r35Y31LoXzmJtv305418pRYu4LOoP/qYAiqQwuLv+UumdmAGQOyaCINDW1oa2tjZQFIXXX38dZ8+exa1bt/Dd734XjzzyCIaGalvG6ujo6DSLnRVGeYditVpx//33Y3R0VJXx3+xBV/VQKpVw7do1nD9/HtlsFuPj4zh58iSGhoY0M/6//1atYfXcVK2xp7QmgTHC8/nadYWM+J2GkPxnabMFbS5hYzaSrDU0GMNfLCsgxny0PDBrI+PCQrIP6/lOmWeox2IU/hAypHS9iIUlFRppD8N8V9oTQjdC6K4sC1L9qo+JX2NQL1KyHyFeCxxC3iwuowlb+5AluAZ02t5Rs17IOqBqv1qQs3sQdI8h6B4TXeeS+UFF2zop74MBEG4B6vf76x681SxKpZJiiafBYMAHPvABvPDCC+jv78fg4CA+85nP4IMf/CB+9jPxYvFXXnkFY2NjGBkZwdmzZ0XXe/7550EQBC5cKBexF4tFfPrTn8ahQ4dw4MABfOUrX1H34nR0dN6V6BkAjahHA89Ibraj5z+bZDJZmTpcLBYxNDSEtrY2zQqUmdab/+m+skHntCtzeup1kOJpAwAKZlNV/qMWqfaEBCGsqweEOwABgHfTgMEu6u62Cd4yE3o82sxfUMtgazVK3WIugmK9rmjeCY9VWgYTybuQvut8eOzi8pBEvtbpK5Cmml8gI0iQ4Do4dlM5q0KDQIGuOqFZ0gy7sQiaV8+RLdlgNzUezS4KyIrMJeWC+oS5A63FsnwniAEYiMac/aRRm779cdINJ2LwGXZjgFpByVTO9IhlAGKWbrQVNhRt22IkOXJ+o4ECSUnHmTo8BoSjyt8bg8GAcDiMS5cuobe3F93d3dteX1VvVqJUKuGRRx7B5z//eaytrYlOYidJEo8++ih+/OMfY3BwEJOTkzhz5gwOHjzIWS+ZTOIb3/gGTp48WXnsueeeQz6fx/Xr15HJZHDw4EH85m/+Jvbs2aP6eHV0dN496BkADahXj1pv200tyOfzWFpawttvv425uTm0tbXh1KlTsNvtnCK1RmGM/3oKXA0GA9703ye6nN3jf2o6ipG9Vo4kx2zSPkoYTRqwxKu/5N8HgGS2agxEZOZJCRn/VnP12KOZ6utk5D9C8OU/KxFu9DSeEv4MIrlyJD5TrM3wRPNO0ftiE4nNhurrSeZtggPKGDbytdFtBrYER6rGYKtgG/9+u/IhZWmju8b4pwkCtILvWIjsQpDWNto/QK3AZ9hd8zjjCOx03G43BgcHcfDgQRQKBVy+fBkzMzMIh8PbllFVkwHgw/zWDg0N3e2QVMvU1BRGRkYwPDwMi8WChx9+GC+++GLNen/0R3+Exx9/nDNlnSAIpNNplEolZLNZWCwWve5AR0dHdwC2k612AEiShN/vx4ULF3D58mUYjUbcd999uPfee9HT09O0wjrG+P/nCxb884WqkZrNc/f33Tes+O4bVaOPiaix9f9iHDjY/PFL0aQBwQ1uRoFv/K+uN+8rxXYEgPIAMCHevNLYflbyg5xMQM1x8JwCh8gQsmTehnZ7GlZjEX3OWi9IKMLOh4n+80kUtndoHgCETOIzC4rgnrMOQzk7Mt7HLcS9Ujys2fHkaPkWvAW6fA4NUCuS68Vbemoei1m6BdaUhp1B+oO/kS5Kn/Er+w4zRcA2mw179uzB8ePHsWvXLoTDYUxPT2Nubg7JZHJLZULNrkvw+XycGoHBwUH4fNwfn8uXL2NtbQ2/9mu/xnn8ox/9KBwOB/r6+rBr1y784R/+Idrb6xvkp6Oj8+5BlwBtI1vhANA0jUgkAr/fj0Qige7ubhw4cAAOh3ihHk3TmmQA/vmChRORNxnpGvlLNm+odMGxW2lk89XlYg4Ju/hW6DAZiZHbXvveqi0EVsqSD7BaygfDjv5vRAB+YJACAbuFqnGAhDCbGc0/1/iPJI3ob98e2RBQ6wQwsKP/fPqcicqU482csLEXyHWi2xYVXCYEIwPaSQRGfqlyO2HmZvGGo7EAACAASURBVDiUyoDWWg5gKHNT0+NSw0a+E26sy663YBpXve3//Kvyhr6Q9p+B33GHP503EolgZWUF2WwW3d3d6O3tbXo3NnYbUKWo+Z0VcmbYz6UoCo899hi+/e1v16w3NTUFo9EIv9+PaDSK97///fiVX/kVDA8PqzpeHR2ddxd6BkAD6jWW1QzeUksqlcKdO3fw1ltvIRAIYGBgAKdOncLo6Kik8a/VMX33Datotx4+2byBI3lhCoKVZiQSqdqL4/694s9lHINoor7PLZtV9v7c8XIt/5yCoWORtHaGyuy8tGF8LdilyX4c5jy6HUmO8e/QQINfLxZjfe1P5ViylvXWauQ/DGlq+zMWTPRfS5Rkcdj8949xnXKPU91vzeqmWTLabjAY0NnZiYmJCRw9ehQmkwk3btzA5cuXEQgEmhZwEWsDKkUqlVLcNGJwcBBra2uV+16vF/391eL3ZDKJmZkZfOADH8CePXtw7tw5nDlzBhcuXMD3v/99fPjDH4bZbEZ3dzcefPDBSoGwjo7OexfdAdhGtM4AFAoFrKys4Ny5c7h9+zZcLhfuv/9+TExMoL29XZGj0kynJFcQP92yeQPCrM6Ez01ZVEfUEika5N0Aq9O6fdFxKRKZ2te0Ht1ZLQ2VIKfJL1BmtNu1a2FZz9AxIZjpwM2EL/9RQtjahxWjtGPBtATdyNVfDJyl6tf5m4j6Mi2ddpkiGJUolduYzWYMDAzg3nvvxf79+5HL5XDp0iXMzs4iEoloKhGqJwOgZgbA5OQk5ubmsLS0hEKhgGeffRZnzpypLHe73QiFQlheXsby8jLuv/9+vPTSSxV51E9/+lPQNI10Oo1z585h//79qo5VR0fn3YfuAGwjWjgAJEkiGAzi4sWLuHjxImiaxrFjx3Dfffehr69P9UVJKwfA1aL+4upoKRuVTDZgampKcv1kunYfbsfWFAHeXCCx7Fe+fudd1YPFLP+++GPyWm6l/OSStlFfGgRoEHCaxSP8cpN45dp/7hSkipcBwEHUNyiM3cKUIVUSz8qxUWv8t2Y3sWkdQqspDQuhfkaFkVSfTflZ6qT8SncpDwMrdwJSSj2DwOx2O/bu3YvJyUkMDAxgc3MTU1NTmJ+fRyrV+MC3eo5JzRRgk8mEp556Cg899BAOHDiAj33sYxgfH8cTTzyBl156SfK5jz76KFKpFCYmJjA5OYnf/u3fxuHD2tWe6OjovDPRawA0oF4JkMlkEm37JgVN04jFYvD5fIjFYujq6tJsAFkjDsDTr1rw+Q8V8MzbVtgsjUfX3lp/H+wqApbtbgK4Gym+cIvA8f3ix5DKEnA7aZAiQ8j4RJPqLu6RuHbRRbZdwZ8BEEqa0emqGtyvXdy6bILDrP7cjWTt6HEIG1yBnPYzCLaSeiL/YpgFnKgQqV6ylbB3VdpyMhIgoQ5ADHPGcYySNwCwCoFpIGVV53j8LHUS+22blfvff9uF48cBKKgrkKORgluCIOB2u+F2u0FRFMLhMBYXF1EoFNDd3Y2enp666gUIglB9HUgkEnC73YrXP336NE6fPs157MknnxRc9+c//3nlttPpxHPPPafq2HR0dN796BmAbURtBiCdTmNubg5vvfUWvF4v+vr68OCDD2o6fbgRB8BiJvCtn1kh1YkvleVeJDfCwivHBIaornlr+8wryeLHsyZE0+W/yn5D5ddoNIhvgHFixIx/sVqAaKJ2myv++h2CrIycPpSUHtAmlgVw2iiYDNzzL1M0C7YD1QqhbkBq4cuBsmRjx0vSwobbPtyp3PYa9za0j3q4abm3cnvBVhuxnaUVTtNSiVAHILXs79wUXTbUGhFdBgBrceGoOFNTo1XHHYPBgK6uLhw+fBhHjhyBwWDAzMwMrl69imAwqOp3sB45kRoJkI6Ojo7W6A6Ahqi9CCgxtguFAlZXV3H+/HncvHkTDocD999/Pw4dOoSOjg7N+vWrOSY2T78qHfVk9//P3VUT8DsBCWGzCq+z5s0IOgJAbdT9wi0CF24RmFshYTBwI+l7BuRP/VsLRSyvaVdQyjgBqZwRqRzXgMkVDYqKhPOF8p8cuwe5n0sozI0oO23SUqlrK9WiVbGOP83GZsjDZqhmGdiGv9uiXX1Bs2lGAXAjdQCy2+ZNg2Zr/4O5clejkH0IaujxkDg1LB/9/+7L8t+BZrTcNJvNGBwcxH333YfR0VFkMhlcvHgRN2/eRDQalfxtr7eWQG0GQEdHR0dLdAmQBtST/gXEMwAURWFzcxN+vx+5XA69vb04cuQIZ7hLs2hmETBDiSRgMnIvmoW7bwNBABtRA6ws+1Vq8q5a2FkABqNBuRRICYxtUipV6xrSmfILWPHT2N1PoFAU3p8SJ6BR5Ix/JaSKNsk6ACkK5LvvZ0dM/iNl/DNZBX7xr9J6ADk2Cx3osoQ12RYDu+XqLYO2OvI+TwGZZB6AtMPZ7J77LS0tGB4ext69exGPxxEIBDA3N4eOjg709vbWdFGjKKqu49EzADo6OtvJu+9K/A6C7QDQNI14PA6fz4doNIrOzk6MjIzA5RKettos6nEAnn7VAotZnQEdSRhgl5HadnUYQdNAJits/UeiJbgcyuQfi2tlo1dJ5F8rMlkKjhZtDJVI0oh2l7LPhR/9H91T/ppH78qqGOerRBLwp9wwGSiYjRSKZO17w+72Izb5VwiLoYgCJfzZsFtHirWRXIyXZSjDbvGo8WbKhi5n/e1GSZqAkdi6YVFKYBv/UjMVAODygh0HZTqSbhY60Gra2dmSHncRhrtSvA//R/HJ0Az1FNzWA0EQaGtrQ1tbG0iSRCgUwvz8PIrFInp6etDT0wOLxVK3Q5JIJDA4ONiEI9fR0dGRR3cAthGj0YhCoYD5+Xmsr6/D5XKhv78fBw8e1Fzao+aY1DgALkf1OPMFwG6D4gLgrMIaUpOJQKlEY3ivA4tLZWNmdS0Dp1N90eWyj9LcCUgkS2h1lb9KsXgJHe3l210d4kZBKEajs037z7irw4xMlkaLvbrtdE54PyYjjTZbHqmCvBPFlwFlCua6MwB8NvId6LaKR6pthjxyVNlb1KodKEMo44DHLiwpaxbJgr2p4svNgrARLdcC1GfYDSj46seLrciSZphEhpr1uLLyGxHBZSshU5C+LG2VA8DGaDRWjP5CoYD19XVcu3YNFosFHo+n7gyALgHS0dHZLvQaAI1QY7AXi0Wsra3hypUrSCQSsNvtOHnyJA4fPozOzs5tM/4BdQ7At37GDeFbLZAsAGbIyWjYrQJ2vclUfk+G9zYuj1j2UZW/dxLsDkBKswFqMRtFirKzzZefMbD17UwmoFlstfHPcJtS34c9Xqhtnxq3yncG4rf/XI25Of+bQTTvxK1Q9dj2eUKKnidk/KfytcY1QRA4+5wFZ5/TrvOSUiwWC4aGhnD8+HHs27cPqVQK0WgUt27dQiwWU1wToEuAdHR0thPdAdgiKIrCxsYGrly5gunpaRSLRRw5cgQtLS0YGBhQPUWyWTRaA9BiU3bx4xflbhcLa+XXuuyrbx6D1VJ2TBLJ8vOZ6L8U/rsNUkIxddFsdvHvjQXg4i31Ucd4qta5ZIx+vvHPLgRmyMhkC1Zi5YEHcxF5wzSWLRtvYl14GJrtBAghNwOAjxkF/M+LA6r302FX1oNeyPhXw51o4+9hWCSzwGY9pbzgeSNhxkaieR2ntgKHw4He3l709vaip6cHfr8f09PTWFxcRCYj7VzqDoCOjs52sjOszncBBEGAoihO9J6maSQSCfj9foTDYXR0dGB4eBgul2tbo/xSKHEA0uk0fjDVDgAIRSl0qhjiw8fdakA0QVeKZfnslLfJHyyiv1fYWOntNiO4Ud+UVABwO+UdAUpmlVDSjFsLRXRJ2GgW1uEb7kpphHT/bEfAFy0bngOe+mUdW40/2cZpNXljoxsnelK4ndgFh6WAblt0G4+uSrpolZ2lEC/Y4bZo995LRf/FirPDhQ50KCwmJikglbfAadWuexafn66+Dz9dBYgdEEAAykXJJpMJHo8HHo8HJElic3MTd+7cAUmSFemQ2cz9/dC7AOno6GwnugOgEWyDPpvNwu/3Y319vRLhHxsb23Ldaj2IOQDFYhHBYBB+v/+u3vVBJNPSEhp2C1AtsdlMMJnE38t/fyuJnj6uVIikaBgNwsfDZAECGyT6upVH1dvcTF9yxU9piI1QEd2dXCNixUdh94ABV2aysNlrv85iTgsDhfo/o420C90OgYENO4gbG7VTdwH5rMO7iZnILgy545pv12wga4q4e1xZpPLayXKE5D87jVKpxKkBMBqNlaxAPp/H+vo6rl69CqvVit7eXnR0dMBgMOgZAB0dnW1FdwA0gqIo+Hw+BINB0DSN/v5+TE5O1kR9djpsB4CmaYRCIfh8PmQyGfT29uLna6fQ1bG9p83yXAgjB4QNu0agZLS7/mCxIltijP9GWfGXMLKLa+SkcwQcAlIql0vdPvndgABgoF15pqK3gxZtVyqFEvkPn2ZPAb6d2NXU7W81lxeUSYKCSWUzHJbofZLLGflPomBDqyXHaQeqlOnVHkzuWsf0ag9626SfH4yZZdfZKTAZACGsVit27dqFXbt2IZVKIRgM4m/+5m/g9XqRzWa3vMubjo6ODsPOD0m/gygUChgfH8eJEycwODioyPhnpEM7BaPRiFwuh9u3b+MXv/gFNjc3sXfvXjzwwAN46caY6POS6fq7s7A71jAwffMBoFTibvvgkd7K7RXvzjEShF5HPQgN+lIy/EstJUr869/Tzn3PI2mZnq0KSBbs5Q44LNYi6rXtNAjMR7hap0anAesoo9VS2/kplNF+0Nk7CaVtQJ1OJ0ZGRvDEE0/gzJkz8Hq9OHXqFP74j/8YS0tLks995ZVXMDY2hpGREZw9e7Zm+dNPP41Dhw7h6NGjeN/73ofZ2VkAwNTUFI4ePYqjR4/iyJEj+OEPf1jfi9TR0XnXoTsAGmEymbB37160tKi7GIoNA9tqCoUCVlZWcP36dSQSCbS1teHUqVM4ePAg3G53ReJktYqfMo04AUpZ82rTelKMwEatnscXkLe+HS1G5AsKtPw8MT8zJyDAa5KSzhFI5whshrnHsxEqOzztbbUGRy7bvPMoWzDIFgDXg7uFxEpYXWenuVB75fZmSrw7ESP/cdu1e1/8dpnG+w1gNcifZxtJbbsxvZ04LBv9FyKY4mYWGCdATv4zvaqsGPmdEv0HyhIgNU0cTCYTfuM3fgMejwevv/46du/ejUceeQT/+q//Krg+SZJ49NFH8fLLL2N2dhbPPPNMxcBn+MQnPoHr16/jypUrePzxx/EHf/AHAICJiQlcuHABV65cwSuvvIJHHnlkR1xvdHR0th/dAdhmttMBoCgK6+vruHTpEi5evAiapnHo0CG4XC709PRwahb++l/Ejb9QtPEMxkaofiH9+ob24fHltQKMxrLT43BUL+42m1G1/CcSq//9KRQoUDQUORdyqJH/AGhoyJYa3C3qP/tGkmZLia3vKsTAz4BoyXq2TfBxJfp/i1H9b1CiYEM8J50ZYrcCFYMZAgaUi4gBgCBoEAJD2la9tUXTCwsLSKerw86eenlrMxKNTCZ2Op341Kc+hVdffRWnT58WXGdqagojIyMYHh6GxWLBww8/jBdffJGzDruWIJ1OVwI2LS0tFeckl8vt2OYTOjo6W4/uAGwzjbbdVAszcXh2dhZvvfUWotEoRkdH8cADD2DPnj2w2+2KjkeuABgAApvqrLTAhrgRwpYBJZNFWCzNO3UvX0tUbrONfy2IxZQ7K2plP0OD4sZlYH1rnMwlv/YGBrsQ9E5IvhWlHFK9/9PFskGbLZkxUxrHTGkcXuPehveplEShNsLfaAtQJQh1ANrIN68uQ2n7T6OCr/n/eGsU8/PzuHjxIrxeL4CtdQLUZgCAstOgtCmEz+fD0NBQ5f7g4CB8Pl/Nen/7t3+Lffv24fHHH8c3vvGNyuPnz5/H+Pg4Dh06hKeffnrHtJzW0dHZXnQHQCPqjaxsVQYgl8thaWkJb7/9NpaWltDV1YVTp05h//79nEI0JQ5JIiVv2K8G6otYs52AK1elI5fNcgL6+x2w28Ujerk8jVye+/pa7NxjWVqWHzC1vl4bYY8kyucRX/ojxYqPqjH+M1kaSk6rdL4xWc9GuraIUcgJaGbk+/qKXXFRKsmqe1gM1XZgcZjzFScAAOLZ+jvapCl5I5S9LwDobwkLOgF8BrqbL7cTgn1schN71UJR1fMmUzAp3n5npw1v+B/AxMQEp54qFAptSX1VPRmAVCqluABYaLCY0PXm0UcfxcLCAr761a/iT/7kTyqPnzx5Ejdu3MD09DS+8pWvIJfbmsyejo7OzkZ3ALaZZjoAJEnC7/fjwoULuHr1KkwmEyYnJ3H06FF0dXUJRqD4F5avPm/EV5+Xvrgxw7AY6jX+GdgFwEphZECXbxTR6tZOJ53LkchkqsY4+zbfCZAjEpHu+Q4ABqL8B1RlP2z5j8tlQlblfpUSTm39VFWlqI38ryXa5VcC4I27aoxwPsxwMymevahdlqC/RVnP/WawnpKvx1DioLA5t9yDc8tc2ZVQduv1i+JO7x2vvDPAdNxhiEajmJ6extzcHJLJ5rWrrScDoGYGwODgINbW1ir3vV4v+vv7Rdd/+OGH8cILL9Q8fuDAATgcDszMzKg6Vh0dnXcnugOwzWjtANA0jWg0ipmZGZw7dw7pdBoHDhzAyZMnMTQ0pKot6ef+tDwwSarwl48S439uPou5efHhRokUifkF5RdsprCWXQuQjBeRjDenkJDtBPBZWs7IRv/5kX+mAJhfCMw4VnZrbbRPzgmwmMvPYZyHoX5xA0XI8DcaarsErcfL545WxcCZfHX7say8UanECbgRkDaqcqWdLX8oUvLvrdJC4MvLbgTCVef9qlfeKQoluM6+kum/QqwnxTM+WzUOZXR0FJOTk/B4PFheXsb09DRWV1dRKGhbM1RPBkDNDIDJyUnMzc1haWkJhUIBzz77LM6cOcNZZ25urnL7Rz/6EUZHRwEAS0tLlevLysoKbt++jT179qg6Vh0dnXcnO/tq+A6iXgmQVjUAmUymMnzM5XJhYGAA4+PjDRV97Rsra4DzeUqVE8AmsEmhr0vZc40GAqTAyFtHC4F0hq5pB9oMKJqGQWA4Vj5f/oys1vKFPpMh0dKi/KK/sZGVHF7Gp63VWDP9N5kscWYBtLcZEYlJnztGA6Go+JfvBLjt4kZSNG2Cx9GY0xpMiBuxK2EHOpzaOG+pLAG3RuqjC8vteHA4qM3GeCjpACTFSqZPoyMBbvhbMd6fqHlcLvKfY9UROCwk0gXh74dcbQtJKdP+M6QzZKWbFh+DwYDOzk50dnaiWCxifX0d169fh8lkQl9fHzo6Ouou4K0crwo9P0MymVScATCZTHjqqafw0EMPgSRJfPazn8X4+DieeOIJHD9+HGfOnMFTTz2Fn/zkJzCbzfB4PPjOd74DAHjzzTdx9uxZmM1mGAwGfPOb30RnZ3Nnbujo6Lwz0B2AbcZkMqFYrM/YKZVKlem8BEGgv78fJ0+efE8UeSWTRbhc1WgpRdEwiEz6Zbh9YxNj49yuJGJOh9bwjX8h/b8QzORfu5VAPSoGrU+FaFq7DfJtpljWhnhGm8mviZz0cfL1/+miFQ6zuERrJdaGrraqntxBpBo7QB5XA+WWpUf6NjTdrhy9rnL3HCXSHyFKlAEmg3Y6+4vXy97BiSNchzSdoeBoKZ8w9N3vKyHwfZcq/jWbzRgcHMTg4CDS6TSCwSCWlpbQ1taG3t5etLa21h0wUfu8eDyuagrw6dOna7oEPfnkk5XbX//61wWf98lPfhKf/OQnVR2bjo7OewNdAqQxQgVbUqiVANE0jXA4jGvXrmFqagr5fB6HDh3C5OQkBgYGNDP+aZqGb602CggAaq+R88vqHZyRfeUCuStX45xi4L7eql47mSzCtybf4pDN7RubostSySJSSeXHarsrzbk5G1N1DGIkk6VKn38ACIXVv28WMwGLmYDJJG38Sw0B4xxTpvphW0zVc1vIEZDSyjMR/2DChkTOghZL84ozGamSFBaz8PdUrh5gq3Ca0qq09udnzTh3Q700K5h0YH5TmSGavOtU8YtzxZytEkmgRBJw2svvNdngRx4IVh00IeNfDQ6HA/v27cOJEyfQ1dUFr9eL6elpLC8vb0mRrBoJkI6Ojk4z0B0AjSAIoq7okVIHIJVK4c6dO3jrrbcQDAYxNDSEBx54APv27YPdrm2HFb4sybeWwNKCMkP71kxI8PH55SK8/iJH+z83n5XsdjM6Kn+B7Ox2Ip0uoFSiKrUA7GFbYnUAl8+tym47mxU/tny+XKfANv617ubKRP/VIucDKjX+VzZtMCvwJ4U6AQnBl/0IOQH1zARgWIxIny++iFkyg7FTjH+xOgChSPv/urGHc//FC+UM1+XlWnmJx1n/e+tLOAUfzxTK55JcxkUIpW1uGTmQfzUquk5aoi5HCoIg0N7ejvHxcdx7772wWCyYnZ3F5cuXEQgEmtaiWU0RsI6Ojk4z0B2AbUbKASgWi1hdXcX58+dx+/ZtuFwu3H///RgfH4fH42naUBej0Yjf/WrzumYI0dUhLv0QcgTWFsudUrKZsoGfz5dQKikPMQ4f6EU8XrVAUsliRabDGP5ud60hls+TlXoAIdLJ6jZv3UmLrsewsZHndP4BwMkCKOXmbAwkVVscbNJAUcPOAqhBSfccPmokQHMrdyPLrI+j0SizUtK0sEEsBbsVqsOcx0bKjo2Ucud9YV373vap3N1J1GHu+34zIOxMSbXmXI1Ujy+eFV6vkfNx/0S35PJGe/+bTCb09/fj3nvvxf79+5HL5XDx4kXMzs4iGo0KZncpilKt/wfU1QDo6OjoNIN3v1h8h8OPtlMUhVAoBJ/Ph1wuh97eXhw9ehRW69ZFJxstilNDLFpALAq0ecqaX6OBqGh9pejqdyOdLsDhsCCd1q6rB9sJsNuNyGbJSuEvH4fDgjQKKOZJTiYgnSzA4Sq/HrkpwIxREU+oL6oVKwBOpsvbdDnUGe7huAGABR3O8vvptFNIZaU/i8WgGcO9zem2JEdnuxG+japRlsqqe72MDCiascDT0tg5ZLcZkM0p8z6kag2EcFuylWFgyYwBrpba/bBt02Cy1jmZWmxHb7u6aPZm2oFep7a1DnzUDrsDgD27yr+FK175J/8/f09jfS2Cv/+yum5Gdrsde/fuxZ49exCPxxEIBDA3N4fOzk709vaipaXsbNTTAhTQJUA6Ojrbj+4AaAhBEHXXACQSCfh8PoTDYXR0dGDfvn1wuVzbMrqd7QCw6wCWFuLYu48btWLPALh6SVxfLwdToMsvyFUiA2JTKqoPAeeypKoOPc3EZhN3voolGpFoEYG1OMYP19fJgy//yRXLn1/Z+C+zGrJhV2etDjqZIeBqoSu3PfXVjSoinDKr6gQkJP9R6xDw4Q8Bu6dduED3xRv7GtrPSkz6HL/m8yCTo9Gi3XgLQYIRY418TEz6A1TlPwztMp2hGDlZsUTDbKp+NsGYBb1t0sZ8V4dR1XA8oGz8A9Wi4XogCAJtbW1oa2sDSZLY3NzEnTt3QFEUenp60NraWlfARJcA6ejobDc7w+p5l6DWWM/n8/D5fIhEIlhYWEBHRwdOnTqFAwcONNSRQi2/++cJ/O6flw39//Tf7khe0MRqAYSM/3SaRDotfdH2r5V1vSMHygZtq7P52Qe29EcKqRoAAKKZAbUItTctFOnKf76sZ/xwJ1aWy5HZxbmIqn3JdWxhOwJscoXyuZhIV8/JxaC58v/SqrLBW0Ct4Sh6LClt5g28E7Bby5/LD95w4wdvuDHtHeAsb7Ep+y345aPKI/aM/Acoz4uQG2ynpDVnImcSlP9I1ZJshCkEotJD6Dpat0jbJYHRaKxkZMfHx1EqlXDjxg0kk0mEw2FVwR89A6Cjo7Pd6A6AxshdBCiKQjAYxMWLF3H58mWYTCbY7XYcO3YM3d3ddelJ64Ux+ktFrqH7/z4lbXgtzscrf0KIFQLLoaYdJ3lX7M2X/1AqMzBLCzFsBMtGUy5XgtG49RkXORjDa6C3aiTt3sONzN6zX73efiNuRCJTe745bMLGlpQRV+AF65f85fdRrA5g3qfuPN+McddfDah6Ogd+C1At+Z8XBySXe6ziBjrjBIzt29pi5HBM/jsTTck7u0oKgRPp+qPxUnVCW43VasXu3bsxNjaG1tZWhEIhTE1NYX5+HqmUvBOmOwA6OjrbjS4B0hCxiD1N04jH4/D5fIhGo+jq6sLY2BiczrIR5/P5tvIwOYg5AUpR0wIzmxGWCLgEim215Na1dew/3FPzeCZTtVpzOe2mMW81ayuJGgfA01o9F9fjZvS4xeU0fCcgHDegg+VfsA3/dJZG613pTy5PV1qhirEaJLAa9KBXw9lDq4GyjEQMu1V4WYvI43yuLFhxdJ86nb5WHBs3I6OyC+Xlzd24Zw9wZ1mbeRZOO41MjqhkajK58mesdEAXQXBrEhohnDBwov+M/l8Nvbvrm2asBJIkYbfbsW/fvkr91uLiIgqFAnp6etDT0wOLpTa7oUuAdHR0thvdAWgiuVwOfr8fwWAQDocDAwMDOHjw4Lbo+oWo1+jfasKRsvHa0a7MUYjHyq1G3W3lwsmB3e0I+spRud7B8kWXMf5tLWbkMvUXsSqpOZCaAtzSIvwVzOVpWMzV8ySbp2G/a2zn8+V9Mskis0ZSJDb8ycBiME5AqUQjnDAAUG75zfsM6PIQDbX+VIIvYuZMQ/Y4SjVTaqOZ8utdCTT/u3nN1y5YyCvEzTVrXdr/zXAJXR3SP+8uO4lklvs+MMa/FL6QEQOdyj6zSKK8LYe2nYoFSWdIxGM59Pc3sTiFB0mSlSJgg8GA7u5udHd3o1AoYH19HdeuXmLQSwAAIABJREFUXYPFYqlMHWYyvIlEAi6Xsva5Ojo6Os1AdwA0hCAIkCSJYDAIn88HmqbR39+PEydOvCem86olGsnA7uBG9BKpqmEhVg+QThfg8dgQjQqHSl1uG5LxHOKxLDq7HSgWlOuH0+kiHA6uoyHU9lML/b/VakQ+T8HRIrwtpg6ATbvHXHEA6uX6PHBoRP3zogl1Yd3VoLwxvRYg4Zapod2IErLD55IZ+XXYOCxkjRPQTOM/WbBj6rYdHSoqp3920YDhXeqO6bUrTow3VpMsSjRlrKtrTyNshklshhuX//xvn7+Df376Ho2OqkqpVBKsmbJYLBgaGsLQ0BBSqRQCgQAWFxexuLiIoaEhlEolmM3vnRoXHR2dnYdeA6Axly5dQjqdxvj4OE6cOIHBwUFZ458gCFDU1hW5sYt+m4nDIX7RXlusr2NQOFKEx6MuJBqNZABUo/9ypNPqMgJSEfiNjazoMqBs0AOAwUhIdgAyGmp7/APAyFht8S1b/sPn8p3Gv/L8uQD8IuZcjkIuR0GqpCOnsGXm+ZmqtG6JpZQz8CbBShn/8SQNX0SdscXvANQoU7eVh8BLdSZE6ikfSmWkl8tlA9TQyPDeNif3fJlflP5e+f3yMzi0gJ0BEMPpdGJ0dBSTk5Noa2vDX/7lX8Lv9+NrX/saAoEGill0dHR0GkB3ADSEIAhMTk5idHS00idaCfxZANvF5/40iq7BLk22dWsmhFszIVx4/bbgcmebfCT0+rVwXfvOpgvo7Hags7u8j+5eJ5ytVmQyRY7uf6cj1JUlmdSmVuH6fPV2sY63JJbQ3mGVGgKWzlAgiHK9QpvbgA6PQXHE3+1qXmRfSQvQ164r75CkloFu7jmSEamzYXP5ZuO/NaGE+oh8VkVZxa0lGreWyq9tdPfOkEwKIZYBEMJgMOAjH/kIfvjDH6KnpwdutxsPP/wwTp8+jfn5edHnvfLKKxgbG8PIyAjOnj1bs/z111/HvffeC5PJhOeff56z7Atf+AImJiYwMTGBH/zgB+penI6Ozrsa3QHYAUhNA9aaX/+vszWPhQPqWkluJYz+v5mkE+LRRIuleV8R891tZ/MULOat/Sr2dGlfN5DJSkuE/JvVPzYzC8omALe3CXQsaqk1DoUmFystAN5uEmnl5wHf+Ge4uaTV0VRhujxZWYmRFou4E9jMMiep6P+GN4orU97m7ZyHkgwAn2KxCJvNhs997nP493//d3z9619Hd7fwlGOSJPHoo4/i5ZdfxuzsLJ555hnMznJ/w3ft2oVvf/vb+MQnPsF5/Ec/+hEuXbqEK1eu4Pz58/ja176GRKL5mV8dHZ13BroDoCEEQdRV4LuVDkAzKORqjfT1NeGhSVKsLgm3FdUKh6NsvSjJAqiVAanBZjPB6VKv/2U6sBR4NQ1Kih4jaRP+/YK6c0xoCjBN0UimyvvnS39SaXVZAaUyILUQBOC7e/qt+bT5HPltSAFl0f8+cq1yu8Oj7ud2eJe2MiQh8gVljpHFXHYC2DUAYlmAcLz2NzCVkd7P7XmVrY92CGoyAAz8FqCjo6OiLUGnpqYwMjKC4eFhWCwWPPzww3jxxRc56+zZsweHDx+uaSE9OzuLX/qlX4LJZILD4cCRI0fwyiuvqDpWHR2ddy+6A6AxaicBA1vnAKTTXF2syWyEyVx78dJKBqQGMePf79UmYrXdk35tNlPlz2RW5iTKtdgU49oN5cOgxOAPBFNbAAwA2ezdWQ0ZeUN/LSAuS1Hy/HSGRpplZDYq+2HmGIjxk9natrLpTPk1/OMv+vCPv+jDC2/X3+ax3joAPieGhbN7BFGe/KsGC89nVVsQLNW6lU1ra61z7NvYmTKgejIAamYA+Hw+DA0NVe4PDg4qbht95MgRvPzyy8hkMgiFQvjZz36GtbU1+Sfq6Oi8J9Bb0+wAjEZj0xyAUqlU6UpUjlRxaxPWV+srxmUTWY+Bpml09Hq421aYBRAy/lcWlc8XUAJJlo0Ph8NSMzys2Tid0pFcLeU/wYB8JFWp/Cdf4Eo+hCiVaJgkhqcpMd6VYDIRSKSqBmSrsz6D0OMof88u3CJwYLj6uBYdgBopcgWE5T8mo7wzcPVWCUf2V3/Ki6zuUVOL5fqD23Mp9J60a6L/ZxNNEPC0bp28Sq74d6shSVJ1BkDNDAChgJLSLPOHPvQhTE9P49SpU+jq6sIDDzygd6PT0dGpoGcANKTe/v4mk0nTImCaphEOh3Ht2jWcP38e+XweR44cwfHjx1Vva/WWvJ62nqyHFgh1A4pHlRsIjCRIjmbWATRKOCxfWWk20bh+s36JhVz0n22gNkvW0whiMqCbi8qOVUj+w0BS5b9GSKQNqrT/9dDRqX6YANMBiD/lmYEgqlr/FguFaKL6+yckA2IwmxrzlPp6hV/L3Ix0R53l5WXk89oOeNNCAiTF4OAgJ2rv9XrR39+veF9f/OIXceXKFfz4xz8GTdMYHR1Vdaw6OjrvXnauZfMeQisJUCaTwfz8PN566y0EAgEMDQ3h1KlT2LdvH2w2aQMguBJseP/hYBQAsOENSa6ntgVoIzKgeEyZ4Zu4u97qnPraBYZMiruvldvrCG9Iy3Gydfb0LxQo+LzKpD6NGP9s6Lt9PYUMXkb/L9SFZt+u+n9mpFrJasVWDP9iE9jgOvtrfm3qFBgJEp/bc+XzZG6N+zqFukwJwUh/CkXhjJCY4R9JSL+vctklIcSMfzn2T47iyW85cePGDVy9ehXr6+uaBF3qzQAodQAmJycxNzeHpaUlFAoFPPvsszhz5oziYwuHy53Url27hmvXruFDH/qQqmPV0dF596I7AE1AbUS8EQegVCrB5/NhamoKN27cQEtLC06ePImJiQl4PB7VWYnYZq0ch50F8M5zo2zBFa4xL2f8N4rUNGB29D8WUZYJ4GcBYhtlzfRmIIHl+TCW58VbkW6sV2sqCAMBR6sdprtynnyuhN1jtTpxpTD6f59PnfFe1Eo8rhKbzaCoBaUU88tlQziwyfUwTDIRY6FOQIB2BcBaQ0oNSOChRP6zVTCOAEHUZgX8YfWOmpIagktvrVRup3mFxLOXVlXv86+e7cXX/rEL//3Psrhw4QJu376NRCLRUBZT7W+sGgmQyWTCU089hYceeggHDhzAxz72MYyPj+OJJ57ASy+9BACYnp7G4OAgnnvuOTzyyCMYHx8HUO429P73vx8HDx7E5z73OXzve9/TJUA6OjoV9F8DDam3C5DRaEQup9zQo2ka0WgUPp8PiUQCPT09OHToEOz28rChX/3Mdfzo24cAAA/91iX82z/eW7ON+ctzGDnGTQcHV4KwtcgPLPLOBzA40ie5jtliQbGwxWNDAVgsVUNELPovJv1RG/3v7nFgYz2NeDgNR2v1fTPxNP0rixHsHhbuBS+k/w+H8hgYqEY6fb4c536zicRItLcJG3T1yl34rT/5MNON55eLTYv6L6yROL6/KZtW/L54g+oN9hVvEbsHzYgmaMkhbwDgdJo4NQA7DWcLofk04bmZAIqFIsx3PZRcWtlv6YkTJxCJRLC6uopsNouenh709PTAarXKP7kB1EiAAOD06dM4ffo057Enn3yycntychJeb61U02az1bQM1dHR0WHQHYAdgNIMQDabhd/vRzAYRGtrKwYGBjAxMVF37QGDmKHOj+4zGE3yBlo2nYHJrN3p5fcm4PcChw53NLSdaJg7+tTTIT+wbXk+jP5dHtHljPGfTmTR6uFuz9lqQyrBNUhKRVpxJyCl5PMUrFYDjEYDSBFrtLXVgkSigJ4u5VNpIzEKRKPVrSrgG/9y0X8+6QwtmhGoB0b/H08qs/B3Yg2EkP6/3AFI2FFIpmm4HMreQyZwLhdAJwii7ii7mg5AUsY/k91raXVWjqmjowMdHR0oFovY2NjAzMwMTCYT+vr60NnZWdNaUwsSiQT6+qQDKDo6OjrNRpcA7QCkHACSJOH3+zE9PY3r16/DarXi5MmTOHToENrb22uM/1/9zHXOf7XkMtvbZaMZswCYDkCN4F+N1jzGlgAJkc8Jf6bzN+urM4jFpZ1Ev7/82SUECqFbW7lZj7kF7jqztxKYX0xjeVX486cpWjLKzZ8JoAV8KRAbdkcgQFwGxOfCLQIXbqlzEBjj/1+vloc1/dtMWdol9H4IyXsiUWEp0l5PDHs95W5XUnUAFouhpm6Az1ZKgRiU2PPXr3G/N92t0rIsoQ5dP39lTnDd0YkBAEDxrh4p5F0X3e6taeFtMJjNZgwMDOC+++7DyMgIEokEpqencefOHSSTScHnUBRVl4OgNgOgo6Oj0wx0B0BjtBgExkh8ZmZmcO7cOaTTaYyPj+PEiRMYHBxUrOP81c9cB03ReOi3LnEeX7sj3dlneWZR8HG2/j+wLG3Emi0WEIR2p1dkPYbIeqyuycCpZB6ppPLuH5sBbtFxIixsAPDhR//ZrCxGsLIYqRj/t66JGytipFO1Rl4uK/9+KBl8JgUto1lnjH8tuyUx9QByrHprDUa+VnxsWPz7onUBcJb3Xn/r6mE4nSbBIWD/elHeCJQz/NWQL6jPThSK4l2AAOUTf03GsvwHADRMDDYFh8OBkZERTE5Oor29HcvLy7hw4QLW1tZQYGVL6ykABnQHQEdHZ2egOwAa04gDkMvlsLi4iLfffhtra2vo6+vDqVOnMDo6ipYWeakKP+pfFBHbWmxljWujMwACyxuSaf1cKoNCNo8Lr99uaD9aIecEZJLiEX2lToAa+E7A9BvLldv84l+1xcAA8Oabm3jj9Q20tJiRSBSQSFTPh0ymiKvXEyiKGHeRWNXw3Aypa50oVwysRCajpA5AyPiXoqRSG78ZM9RIf5gsQIG1rXhcvYP19kVupqVRGZ9WBENQLP+5McvN1on5iUwWgP9z5AtsfY1QS6sTmUS5K9LH/89lyXUNBgM6Oztx6NAhHDlyBARBVLrpbG5uolAo1FVUq6YIWEdHR6dZ6A6AxqjVupIkiVAohFgshqtXr8JsNmNychKHDx9GR0dHUw2DZLh22FYuWdXIF7JVw4/f/UcIUqMONM2QAQmRz1UNt3QiC3dnGwAgsOiHyWSESUGtw1bicMobG7FIRnYdvvyHD99Z6OpsblHkVnB7Ub1EJpUR/x4bBYafud3cDlXForyjw84CeAPqjzGwJv5d2QyXsLQsIunaIXXCbAer3gF9jBSomZjNZgwODuL48eMYHh5GLBbDtWvXkE6nkUqpm7ytZwB0dHR2AroDsA3QNI14PI7Z2Vm8/fbbSKfTsFgsOHnyJIaGhmA2i7e6rBe+DIghGY5xjP560Mrwr4e1O8LzC4QKV4uF6nGyjX8AsNjUvedXzy1x7gsZhI3Cjvr7ViI1y4XkP0Jync2AdPZi9pbwnAV2n3a1WQA15OuYhbC6Kl1/oZRMVvm5G4kUVH3OTgUOWyNcvF42mKWcAJe7vt+Sejr1MLMg+LfZvHZOfMNDQ0709TlU7ZP9nVYCE/1vBKfTidHRUezfvx92ux2Li4u4cOECvF4vimIpNRa6A6Cjo7MT0B2ALSSfz2NpaQlvv/02lpaW0NXVhQcffBBjY2N1aUlF98Mr5GUbhYz8px5SUXFDg3ECcqkMcqn6HQqaprdtsjAADN0zJLvOrcvq+49LwZb+NEI0pN4oFjP+2TTT+GcTDjXmiG43aqRABZWGqxYUVdYAqHUC3ncwI2r48+EPAWtvr2MqmEKECoDF5JFqoCgKTqcThw8fxuHDh0HTNK5cuYKZmRmEQiFQlPB7kUwm4XK5Gt6/jo6OTiPoDoDG8CU7FEUhGAzi4sWLuHz5MoxGIyYnJ3H06FF0dXU1TeJTzwUuy4uOde/qrlmH7wRoHf3fDi10q0dZ1PHcv13m3F9fa+7QMzE2N+QN5TffVF7fYbEYOfMTxIhE8ohE1DkDdnvjPzE+n/zrVVsPoASvt+xQRSLlbXd11TrPbKOfJGk4HOWIu8tZ+34qKehVIwPaDGpflyKHWEHw1avVbj+ME6Clgu7nr8xVugG9/pMFzrK5GZ/o8+S6/zRCqVSqBG4sFguGhoZw/Phx7N69G5FIBNPT05ifn0c6zXXMKYrSNOCjo6OjUw87vB/DOw/GgE0kEvD5fAiHw+jq6sLY2BicTmdT953PZGEVGeRFUzR+/b/OwmKzCmr/+SzPLFb6ZdcDPwtw4fXbyCTSIItFDIzuqnu7MxfL0feB4W4ElrQxwAs8OZCZH54EENsQnwjMZnGmfHzDE/W9xrBAtF1I/sM4AUq6APFptCuQELkc17hdnItgeLQdfn8WC3MF5LJFjB/urHv7Pl8GAwPlQni+/Idt/IcjxZpp0Wz9v9IZDF5vGg4n9zzY3MwLOgGAfPQ/sEGqmgDMQJI0jEYCfm8S/YMuoFXacAyH8zUtX5VCEOXagGSahkXjORV8RILjsvz8lTkYTFWnUsr4bzYkSdYUARMEAZfLBZfLBYqiEAqFMD8/j1KphEQigUOHDu2Ygm8dHZ33NnoGoAlcunQJCwsL6OjowKlTpxQb//VIX375o+c49/nyH86ytPgyJvpPkbVRymwqLSn/2U6UDhsT0wqzB4MxdQC9u3tEtyMk/1mcWa55LLhSdRicreJTfLWQ//i8jeuahYjE1FtpUq1Ab1xT57Cxh4CJzVQQYnVNOmOgphtQJFJAsUhWsgBKUPs9NgjUFXgDpZpsgN+bxJ0F9d2g1KDWNv3U+9XLttZD5e/iniELUgKtbRkuvbVSua12qODq7QBWb0s3LmhUBsTOAAhhMBjQ3d2NI0eOYGJiArOzs/jwhz+MSCSCV199FaTAb62Ojo7OVqE7AE3g6NGjOHbsGLq7uxUPilE6DZjNr3xsSnK5kDEvF/1XMvU1vlkbkVaKsQkFzmKFwErp7q8W5FF3jbdERJlRvb4WgsPNde6yMjUQ+Uwe+Qw30q/EEbhzQ3j2gposgNrosBL9//KScB3B2op4fYFvjStf0UL/z8yIcDjMWFzOg6ZpRZ14GFZXqp+5w2lBifXcYlGZsZZTWFTc3SPe1pdt+EejOYRCXMedKf4Vgv35RiLKnAWC4Br+yyvytSQzN2oDAv/jX6rnYVurdpcWvvFPlaQ/01Q0jmhwU5OCXymEMgBiWK1WPProo3jjjTfgcrnwwgsv4NixY/jiF7+ISET89/SVV17B2NgYRkZGcPbs2Zrl+XweH//4xzEyMoKTJ09ieXmZs3x1dRVOpxN/8Rd/oeq16ejovPvRHQCNIQiirumQ9TgADL/ysakaZ0DI+BeDpoUvqFIX0Nh6OcIt51CwW4lqiVbyHwDY8Ncaqq3t6uRPizOrFfnPdrAwVzupWC1rC/XPhQgGqkYjOwuQSoh//gt3IjUdgFZXqo6BEu2/HCaTsra8qyupivPLaP/rwazhMDQxNoNJUSegcNdpkTP+8wXx9+RPP1v7GkiqOvmYMf6/8F8SoGmao/+vByYLoGaa9O0ra4LyHyZTKSTjE0JuFoAUchkAIdLpNPr6+vDNb34TU1NTOHz4MCwW4WMlSRKPPvooXn75ZczOzuKZZ57B7OwsZ52///u/h8fjwfz8PB577DF84Qtf4Cx/7LHH8JGPfETdC9PR0XlPoDsAOwSj0agqJSwX/WegSBJksVjzx17Oh+0QpOPiUVyhTEAhl0Mhl6tsx2QxIZ/OIRWR7zbTCBte5UYIuwVoPlesRP0BIB3PIBmrGoC+uTVNji8V1aZtpZICYKBc8yE3wVcthQKJQoFEMFg+Bua/GhgZkJJME1/2s7ggfA6l07UZELZ8qFCgkM+TCG2kEdpIw2QmEA7lEAnnEA7lMD9XjWazswCNUFA5dCyVrH0N0ai4Ea9VAXAkKp09+tT7MxCzcdfX13Hu3DnhhSyWZldqHlsPkbhxKy342QFAIKDN90UJv/7bM5U/NajJADAkEolKC1CbzYaPf/zjovLQqakpjIyMYHh4GBaLBQ8//DBefPFFzjovvvgiPv3pTwMAPvrRj+K1116rOLwvvPAChoeHMT4+ruoYdXR03hvoDoDG1Fvg1UgGgJaoqBNbxncEap5HUyAMBCiBLj98wzIRilT+GBgnAFCvtV29pY3R3QhsCVAhJxzFlpMisOsAAK4TENuoX0alln1jHYrXXbq9Lr8SlBn/xUL5fK6nUBkArDaTqNHPIGZAAmXDnyGfL8HlrtZiMF9Tfl9/vmOSu+uEsNt2Li+Xje/Nzep5QZK1Bn84rL1en12zIkdHp3jtiRyf/g8ZfPo/lPf1W6cylb+ZG3H8+efN+PPPm3Hw4EGcOHECvkVlErw78+JGfSpVQl932dPQyviXqodiKOYLnGyBGieAJEnVGQA1MwB8Ph+GhqptiQcHB+Hz+UTXMZlMcLvdCIfDSKfT+OpXv4ovfelLqo5PR0fnvYPuAOwQ1DoAP/mnE/jJP52o3KcpSvBPDqF1GKOfyQ6IyYm0jjCrgRKQLYWD3CyAf7ksaVE7LAioGoKens4aQ2L/MfkOP06R1qL5TL5i/K8vc4sUlTgF0c3m6prZKO3/b7OXo6BBhYbbwp3GnB+hgmAxR4DvBABA0Fd9DymKhtVmEsxI5HIllIpkxfhnT6plnAAlKHUE2FkAqei/VshF/4X4889za3iMRiN+8P/txl/9vhV//Nnq53L5chSXL5e/jz/8u3skt1koUNi7y4JnvnUZP/9f1wDUJ/Fzd7erfg5QDVD8yz9M4F/+YULx80qlUl0ZALfbrWhdIekaP8Akts6XvvQlPPbYY03vPKejo/PORXcAmoTaTiD1ZgBee/5+vPb8/aqfpwaqRKKUL6DEiuRLZQ9qni/gQCxdXxJYU+Vx0RSMZmHpFGP8KyUdzyAdF46uenqq7SuVtgNl4GcB5Fi8Kd25hE1soz5ZVSKSqfxpjVInIMsypvkFwOvr0pFbq03e6Crwhl5l7zoIFF3+48POBBjuOgMlgcJfJYZ/Pk8iHi+/vmZkAeQQ0v+rHQJWDw6HA79/OodHP1z+PD/1/psAAK/Xq2o7ty8uIBFpftexf3667JioNfwZmp0BGBwcxNpaNRvq9XrR398vuk6pVEI8Hkd7ezvOnz+Pxx9/HHv27MFf//Vf48/+7M/w1FNPqTpWHR2ddzf6HACNqVcCpLYGgM9rz98PiqLwHxXWBjAYRKb1CEl/AHCcADVQJFnZV3A5IHnh1GJKpxSb3k10DXY1dR9s1DoBzeTaBS9sLfVPg+YjJgWy2Mw18xWEUNL9Ryjin8+VAIUdjWKRqkPBZCukIEka8djWGe5JmRkCBdbrN5kMHMckGUvD1VabbWpE/tMoBoMBf/X7VgBHceBAHoFACaOmn+LYcScGBgbw3V/0AgD++DPl38ofnC8f61//31YEAgF4PB4MDQ3hv/2JsixLfCMiGv0Xm43y3b8oG9KME1AP9TgA7BoAOSYnJzE3N4elpSUMDAzg2Wefxfe//33OOmfOnMF3vvMdPPDAA3j++efxwQ9+EARB4I033qis8+UvfxlOpxO/93u/p+pYdXR03t3oDkATqMcJaKQGgD107O/+rBsDAwNwOBw1MwKEEDP0lcBkAYScCK3lQcxFPn5XJsNc8DPJTOW/2VqWJwRWQ5xj2PCWDfDuQeVaeCmmXyvLFFztbQCAXEpea/xeIhhII7Ieh9PdAqORQL1jx9Z9cbR1cI1b/hAwtVitpooUSCk0RUsWLbP1/yWBuhAjrytYaCOLzm47kvEiXO762+La7GYk5Wf61dDu4e5zeSWNPbuVTcNWi9VqxZ49e7B7927EYjH4/X6c7LyNnp4e5PP9yOfzmHDeQCqVgtHYj8nJyYqs5rtf4R7TJ/5QXSaBgZHwWVvseP6bIwiHw1hcXESxWERvby96enpUS3mAcpZX7W+9GgmQyWTCU089hYceeggkSeKzn/0sxsfH8cQTT+D48eM4c+YMfud3fgef/OQnMTIygvb2djz77LOqX4eOjs57E90B2CGYTCbk88pbZhYKBQQCAfj9fthsNgwODmJsbIzTgvTH/3QCJEniw795sRmHXIFxIhhHgG38M21A+bUGew/tbXi/ZosZxXzVvOzbVf+kWTFa251IRFLoHOhByKesQBYAVmaXsfvgHtnuP7GNCNp40cvLb8zi2PsPCq5fnh+gXtfb2mpBItHczIoQuWwRkfUY2nvaBJdn0wXYHdVIvpS8x2Q2ctbb3MigxVE2Zh2OqlEb3sxwCn6r+yrCalX+k1cqUjCZxVWSyXgObe3V6LKQ8c+wuVk2QilesTA/+p9KFuF0STsFJrNRUJ5U2WYsK5kBYGv/k0nuOVGeAaC9MpQgCHg8Hng8HhQKBczNzeEXv/gFjEYjhoaGsH//ftlo+vf/YhCAckfAbLVUsokv/cNEpUVzV1cXurq6kM/nsb6+jsuXL6OlpQX9/f1oa2tr6qTeZDKJXbuUTwk/ffo0Tp8+zXnsySefrNy22Wx47rnnJLfx5S9/WdUx6ujovDfQHYAmQBDKeo+zUZIBoGka4XAYXq8X2WwWfX19uO+++zh9pCmKAk3TIEkSJEmCIAi8/P17QRDEljkChII5CEwNAN8RWLmpbS9932J1eJZ33q/quWLyimZQb1cg/0K5ZqBvb6/ketcueLHnnm7R5XIzAG5MV2s2Ro/UGjA2uwm5rHxkXcoZYCNXBwAoqwXgIxb9J0kapSJZMa6dLiti+WylVqHFVb9sKpsrwmSq/U4wx8J3SoRagvKx2WudBIvZwNH/L89HYbZUt93duzXnshi5XA5erxebm5vo6urCqVOnUCqV4PP5MDU1hc7OTvT398PhkD5OxhGo0o///f/ifrf/5VtlTT9FUZXfQqAq2zEYDLBardi1axeGhoaQSCQQCAQwNzeHrq4u9PX1wWbTXkaVTCYVZwB0dHR0monuAOwQpGoAMpkMfD4fNjY20NbWhr1796K1tZUTqaIoCqVSCdTdSDtBEDCZTJxvWDPPAAAgAElEQVSMwKs/mKzc/tDHp5v0SqrRfv4QsGI2B7O9fFFlXuv8lfmagmIl04LjGxF0DvZIHEPVAQv71tExwF1XaR2A1ETgZCRWd+cRNs1uCXrtQn3SCQahgUts1uY2MDRa61yk4hk43eITb6VY9zWnCDSbLsLu4HeyIUCSdCXDYDIbOXUDjVDgdaAyGImaLIAYkY0U2rudiIZS8HRWsz5CzgSf5flyBx6mFSvbEWDgR/+bAU3TiEajWFtbQ6FQwODgIIaHhzm/S2NjY6AoCpubm7h9+zYoikJ/fz96enoUa+y/d7bqBDDGP1CuRzAYDDAajZzACEmSMBgMlayA2+2G2+0GSZLY2NjA7OwsDAYD+vr60NnZWXMcFEXVlSlQUwOgo6Oj00x0B6AJaJEBIEkS6+vr8Hq9IAgCAwMDuP/++zkXIoqiKhEu5oLELJebRsw4A1o7Akqi/wxSnYTuXLgFALjn+H4AwoXBjP4fAFLRBAKrZkEZkKujTbSXv2+ubBzzZTjbRTzU+ERfNSQiGXT2uir3S8US5mZ8GJ0YkH2umOGvhHgoCXenS35FFutrEfQMVT8nfg9/oCz/kSMaSsPTqSwabraYKka0WkwmQ8UBKJUomEwG5LIlWCxGhDbEHYzwZkpS4iPGwu0QPF1ceRhTu1Dva6iXUqmEYDAIn88Hh8OBPXv2SEa+DQYDenp60NPTg1wuB7/fj6mpKbjdbgwMDNQEPIT43tl+AP2Cy5jfQ6PRyPnNZG4zToLRaERfXx/6+vqQyWQQCASwsrKCtrY29PX1weUqn7P1DAED1HUB0tHR0WkmugPQBBgHQE2EyGQyoVgsIhaLwefzIRaLobu7GxMTE2hp4UZRmWg/I/ERivYrRWtHgIn+sx0Btv6/mC1LFMS6DwG1jsHS9UUAQGgtWNH1MsN7UlFuK8zAagjRYFiRQc8Y/zsNJQOM+ASWgrIyoOU7VTlUa3uLZBtQKSdg7uqqoAyITyFXAO5mACLrwtWq0c047A5uJsa7GKoUdCslnS5y6gDUwCniLZIoFZXMz1Bf5M5IpAoFEhaLuu4xQmwGqu9pYK2xjElZ+9846XQaXq8XkUgEvb29OHbsGEeiqASbzYbh4WHs3bsXkUgEq6uryGQy6O3tRV9fn+rt8WGyAgA3c8oEUZjlLS0t2LdvH4aHhxGJRLCysoJcLoeenh643W7VHYAAdUXAOjo6Os1EdwB2AIVCAX6/H5FIBCaTCQMDAzh48GCNxIdJXdM0DYPBULfRL8SrP5isFBb/H49vyD9BBrkhZFSp2haUfZvP0vVFwei/XKtQdnFtMhyDq0Nedy4EYSBqjD3ybq1De7/6VqJWDVtwKmVjNYjuXcLOATv6z0dK+jN3dRW2FnmNtJDhH1mPSRpPZqv5brEzEAM4nYD4WQA2SqL/uWz5vImG0nC22mC2GEEQAJOwK9cAcM9ds8UEmqKRTRfQ4rJWzgd2ZyChtqEOpzJDNZ8viRYnF3gtUPlyoMr+Q0k4JORWhIGA35tA/6B09PlPP6v+94SmaYRCIaytrYGmaQwNDeGee+5puJiWIAh0dHSgo6MDxWIRgUAAV65cgc1mQ39/Pzo6Ohreh8FggMViqTgAzO9sqVSC0WisOATMcRQKBayvr+PmzZsgSRLhcBjt7e2Kj0PPAOjo6OwUdAdgm6AoCqFQCD6fD7lcDn19fXA4HDhy5AhnHX5BL1u3qgVMYbHP50M+n0dfXx9+9L2jMJvNTa0TALgtSMWcAL6hL2X4R4PcfvtCHXbEEFo3EigXxrIHgQHKZU4rs8vo6NN+3kBwJYze3Y23NG1tr0+fz2ZtbqPyv2tQnYwqHuL2ed/0x2oi//lMHuhwIBxUF+FOxnNwuW1IxsX7+ZtVROHZg8sYctki3G3aFIqu++Po6edGhtldj/iGfzRUrU3JJMsZI778B6i2MGX+bwTTyKbzaO9ufEJssViEz+er9O6/5557mjZ51mw2Y9euXdi1a1el7fHc3By6u7vR398Pu722178a+FkBscJhi8WCoaEhuFwurK2tIRQKYWFhAZ2dnejt7a3J1vJJp9OyRc46Ojo6W4HuADQBqWhQKpWCz+fD5uYmOjo6sG/fvkpEyO8vF7EpKehtlGw2WzkOj8eD4eHhir6VoVl1AmJIZQKEyMSVDQpSipzDYDSx6y/oisNhdyozpBOROCBT76tUWrKThosJUcipKzDd9Is3tFdr/DMk4zmOvEdLctn6phuYzAaUipRsMbDJbEQmmUUmma04nGyjX4xkrFY+xj6nsulyZoUpMK6HRCKBtbU1JJNJDAwMcHr3bwWtra1obW3lFOwCwMDAALq6uuqS5rBRUjhMkiTsdjtGRkZAkiRCoRDu3LkDmqbR29uL7u5uweNgsrc6Ojo6243uADQBvgPALogzGo0YHBzE6Ogo50LApJ9zuVylfoBJQWvVl5q5YDKORn9/P/bu3St7wdyq7kFA7UwBKfLpbKWrkBhiHXb4HYqYxzZWAuje3Se7b4OBEJwwup0oqQOoh0JW224xqWgC7k5P5b6U8S8FSdKChcDNRMhBUzo12GQ2IJOsnneZZK5m0Fl12c4aLkdRVKUpARMF58sUtxp+wa7f78fS0hI8Hg8GBgZqAhpqkSoczufznOVMAXM2m0UwGMTFixfR2tqKvr4+RQXMOjo6OluN7gA0Cab9ndfrRSKRQG9vL44cOVLTW5odXWpvb8fFixfR39+PgYEBzSJFiUQCfr8f0WgUXV1dOHjwYN0p863KCsg5AsxydmtRrdhYCcAk0DbR09OJ4OJa5T7fAcim5DXoUuQzWVhb7IqzAJlkfYWbuUxeE/nPTiKXqXfecB37qjP6L0QsnEaLS/j8ZToBma3c3wF2JoCmaJCgYDAZBKP/fMxWE4oisxCE9P+5XK7SgrizsxMTExMNy22aQUtLC0ZGRrBv3z6EQv8/e2ceHkWVtv37ZCeE7HsvLIYEQXGBQBhkXhWURScuH6MgCApRCCLgOIp+zjB4jX6DM77ORHYRAWHYZzSAkBFZdBxHghBkUwhrL+ns+9JJuru+P7qqqe50J91Jdbo7eX7XlSvVVafqnNpOnfuc5zxPOa5du2YxaUxMTESgE66F20MYFaivr4dKpUJNTQ1SUlLQyjssEEyE+vTpg4EDB2LAgAGW+l9w4zxixAgpTpUgCEISSAC4AcYYzp49a5nQe8cddzg1oTctLQ0tLS2WwDgxMTFQKBSd+uC2traiuLgYOp0OwcHBkMlkSEtLk6wnqruFAGAWA+LfAoJnofaEgKsTgQ0tBrsiQDDH4Ewm1JbZH10Ql6P4mhqJgxRO5yvw30M/YMzkkS7v1xGlqmJwJhPi5VEdJ3YDtl6b7CFMAO6Im5d0GDTU7Pbxxs8aDBhiHSCqI/OfqvIGi3/8xjo94pIjUFZUg36RbcWRPT/6riLu/Q8I9ENovxA01rUdPXBH7//KV/ti0V+tBWplqVlI2JoCcRyH6upqqNVq6PV6yOVyjBo1qsumNd0BY8wS6VdwrnDq1CmEhYUhOTkZUVFRLteBwiRnlUoFxhiUSiVuv/12cBxnZSJkO3E4Ojoa0dHRaG1tRUFBAZ566inU1NRg3759mDJlSreaTREEQdjCXPVX7yJuPbi3Ipjy2Prsd2VCrxAYR61WIzAwEAqFosOPlzDqoNVqJXWb5yzdNVfAGQL7hLRxJyoIgNoys6/9EJHtvj2TIAAWEWC7vSMvR/YIjQxHY3Utn3db0w9H7j+VQwfx263LYG8EQDABEo8iXL9gjuIbr0xEyY1bEVPvvG+YxR2lEAnY0Nq2d9iRCZCtFyDxJODim/YjCwsCQDABMnEmBAbd6p0VewASEE+GBWDxAiQWAD+fvu6UANDbnItYAIT2C4G/f/ujbgaRAE1IMpuYODIBEnsJMudhfV5BIYGWfBvr9JZJwOprwr0QRgDs914bWg2Wyb0A2o1a/ecFfli67tb1EI8CRMeH4d05fjAajdDpdBbf/QqFoke4rOQ4DjU1NdBqtairq0NCQoJTkX5bW1tRVFQEnU6HyMhIKBQKhxN4bScOA7dGBQRKS0sxa9YsjB49Gl999RUmTZqEZcuW2Z04zXEcFi9ejIMHDyI0NBSbN2/Gvffe2ybdW2+9hU8//RRVVVWor781MtTc3IxZs2bh1KlTiImJwa5duzBgwICOLpUrkE0TQfg41AXhJoSGemcn9IoD4wiT7goLCyGXy5GYmGglLoTAOSUlJYiIiIBSqfSI3Wl3Txp2BPPzg4H3FiRuhAkNfwF9faOVCGiPoD5m953NDZ3vnRUa/+a8bzXeO/IqpLpojoPAmUwdjibc4Bv7/W8f0MlS+i72RgHE2Db+7VFX3Yh+kaGW/6Va82TreFnHXpeqyuqtPPGIXYc6oiPzH6m4ePEinvsfhs1fD7Fav/aNcDQ2NuLSJXWXfPd7M4wxREZGIjIyEgaDASUlJTh37pxlhDY2NtaqPm5oaIBarUZ1dTWSkpIwYsSIDk2IHE0cNhgMlm01NTWQy+X44IMP0NLSgkOHDjkc3T106BAKCwtRWFiIEydOIDs7GydOnGiT7le/+hUWLlyIwYMHW63fuHEjoqKicOXKFezcuRNLly7Frl27OnH1CILoqZAAcAOMMVy9ehVKpdIqQm9nbfrDw8MxbNgwtLS0QKPRWMyD+vTpg/LychiNRiQnJ3vNML0nhYBtY7oje3p9fSP82rlmXWnwO4u94GmOEOYgdCQEbv50A4DZW5FAqarYKs25by9gxIPDLb3/nsCPmc+5oaYBfSOcc49Yoq6E3kkzoY5obTG0Me+pq3Y8lyMgwN9qFECgqqze6r89IdAR1y8VY2BaolXjX9+ot4wAcCaTS5G2xYwYMQINDQ3A1+bfgcEBeG1qBU6fvmLx3W/rmKAnIjT6ZTIZ6uvrUVRUhKtXryImJgahoaEoLTW7tVUoFJ0ymXQ0cVhwwCBsDwoKwmOPPebwOLm5uZg1axYYY8jIyEB1dTV0Oh2SkqwdFGRkZDjcf/ny5QCAqVOnYuHChS4HpyQIomdDAsANtLa24s0330RNTQ2ys7PxyCOPSPJhDQoKQnx8PFpaWlBcbG7MhYWF4bbbbkNkZKTXVe7d6T3IFWzNd4z8b39RL5+t+VB3IC5XWyFjXWbxZGQxwX1dmy/iqcZ/TXmVlScgwCwCIuM7nqchbvxfu1iEFn1bMeCK+097dvj2KNVWWI0ElOjMbmib6tvmL7guDe3Xx2Hjv0XfiqCQQL685vtbUXbLjEPf2LZcdVV16Bdl37tNha4KMUlRKC+qRGxyW3e2ffv2xf8uNPvuV6vVuHHDXKcolUrExMT0+Ma/LWFhYRg0aBCCg4OhUqkAwGlzS2cQev6Li4vx8ccfY8+ePZgwYYJT+2q1WigUt0S+XC6HVqttIwCc2T8gIAARERGoqKhAbGxsB3sSBNFbIAHgBoKCgrBv3z4UFhbiww8/xJ/+9CdMnz4ds2fPRmSk6xFpBTeiOp3O0oOVmppqGVZWqVQW86CEhASvGAWwpbtGBdprRIvX27Ph90Sj3xGdmWMA3BqxcNYzUtHlm0hO7d+pvNyBsxOAuwPB/Ee8XFPRfsRdADAZnTPfaWpoRlBIIGoq2vr3Fzf+nXkWOnLVWldXB7VajdraWiQnJ2PMmDEICAiwxAPJz8+XzH2mL9DY2Ai12mz2lJSUhIyMDAQGBkKv10On0+GHH35Av379IJPJEBER0SkxUFBQgDVr1uDKlSuYP38+zp071+G8AwF7c/NcKUNX9ycIoudDk4C7gerqanzyySfYsmULMjIyMH/+fKSmpnY4odeViWvNzc3QaDQoLS1FXFwc5HK50x8bT9FdowKOGvy9BUEM2LsGhuYWKwHQlUnAwK2JwB1NAjYftxmJg2QAzL3/ANAnzHoEw3YCsD3TH9sRgAFD5E5NABZjzzVmkx3zr+AQs228IAJC+gRazH4EbAVAaD/7ozJCUC4BP38/+Pv7obGuyUoAhNmYRgnCVjwJWLhHwggAAKtRgLnjC6FQKBAdHW233jGZTJaI4K2trUhOTm4z18jX4TgOlZWVUKlUMJlMUCgUbez/xWmrqqpQVFSE+vp6JCYmIjk5ucO5EQaDAfv378f69esRExODV155BePGjXOq8b169Wps2LABAJCeno4HH3wQ06dPBwCkpaXh+PHjDkcAwsLCrCYBT5w4EcuXL8eYMWNgMBiQmJiIsrIyKUUAqQmC8HFIAHQjwsdh5cqVCAoKQnZ2NsaPH2/1AWpuboZOp0NxcXGnXNeZTCYUFxdDo9GgT58+UCqVXu/Jw5vMg3oyAcHWjRdhorSnBYDQ+AekEQAAYDKY0P/2WyYUHU0Arq00m/P0EZlQORIAhlYjAgL90TcitI0AcNT7b08E2BMAAjXlt6If2wqA+poG9Ivq55IAWPmqc/MrgLZOBeRyuSVauS8ieDfSaDTo168flEqlS6Mcti6Vk5OTERsba1UnV1VVYcuWLdi5cycmTJiAl19+GQMHDux0mb/44gusWrUKBw8exIkTJ7Bo0SLk5+c7TG8rAFavXo1z585h3bp12LlzJ/75z39i9+7dnS6PHUgAEISPQwLAA3Achx9//BE5OTn48ccfMXPmTISGhmLHjh14+eWXceedd3Y5eI0wgqBSqdDc3GwxD/JmO18SAp7BkwLANjZDVwWAyXBrlMMZAVBbWYfw6H5WAqCpocny35bOCgCgrQior2mEvyjQnSMBIBAW0RdN9eaRgdDwUIsAaG2+ZbpmTwC40vgXI/SYazQaSYNqdRdNTU1Qq9WoqKhAYmIiZDJZl70b1dXVQavVYsOGDTAYDJg8eTIOHTqEH374AXPnzsVzzz1n162nq3Ach4ULFyIvLw+hoaHYtGkTRo40xwW5++67cebMGQDA66+/ju3bt6OoqAjJycnIysrC8uXLodfr8eyzz6KgoADR0dHYuXMnBg0a1OVyiSABQBA+DgkAD3LlyhWsXLkSu3btQkJCAjIyMvCb3/zGavKXFOj1emg0GpSVlSE+Ph5yuRzBwY5dE3oKYa5DUVERfve/9H3xBPEDZW3WORIA9VU1iJUlWK0rVRUhXmn2zW+00xhuTwDUV5kbvXEKcyyDkhtayAYrrfbvjAC4evYaZIPtuwcVGv7OIggAAIiI7eeSABA8MoVFhKK+xuxpyN8m0rUgApwRAFXFlQjmRZjg4UVcn+/4X+nqESGoVnFxcZdt492JYLqjVqvR2toKhUKBuLg4STs+TCYTvvjiC2zcuBEXL15EbGwsXn31VUydOtUroyS7Ce+68QRBuIz3dgf3cP7whz9g4cKFGDt2LG7evImTJ09i3LhxeP755zFz5kx89913ltgBXSUkJAQpKSkYNWoUQkJCcObMGZw/fx61tR1HZXU3wkjFxYsXcfLkSbS2tuKuu+7Cl7vSrbwIEd1D6XUtSq9rAZgb/h1NLnWEvca/Peoqqjt1fGe4+ZMaV89es1qnLdRYll1p/OvrzaMBYhedIX2k7wk3Ge3N02iFobkV9SJTqboqc9mbRXMFtr/vOAZCVwkKCsKAAQMwevRoJCUlQa1WIz8/HyqVCq1eMHneaDRaJjMXFRVh4MCBGDlypKSjnvX19fjoo4/wy1/+EkePHsXf/vY3aDQafP7557hy5QoyMjJw+fJlSfIiCIJwNzQC4CEMBoPdUPAcx+G7775DTk4OVCoV5s6di6lTp0raY89xHKqrq6FSqdDS0gKFQoH4+PhuNQ9qbW2FTqeDTqdDnz59IJPJHE5QBMg8yJNEJsa1WSf01gOwGgUoVRVZpYsRbRP3/gO3oiv3i4m0Op6jEQBHvv8djQAAgIFvnAojANpCDWSD5S73/AsCoG/ELfMOISKxqyMAttgbBRCPABh4Ex+j0Qh/f39Lr7/Q+A8ODQHHcdjxvwpMf/WWe1gpRwDsIX6HQ0NDIZPJJHGf6Qp6vR5qtRrl5eVISEiATCaTfHTz5s2bWL9+PY4ePYoZM2bghRdeQHR0Wzer4gjvvQAaASAIH4cEgBejVquxevVqHDhwAI8//jjmzp2LhISEjnd0gaamJmg0GpSXl0tmJ+sIQXhotVrU19cjKSkJSUlJLudHYsDzCBOKhYnEiYMUltgEfgH+MImCZcX1T0bZTbMw6BNubkALjX8BccRmdwgAMf1iXJ8ULwgAwCwCqkvNdvaR8dEICrn1/LYnAOqq6u0GO/MP8EdVcSWiEqNFaW8JlI4EgBClursFgICtxzJnPeZ0NT93zm/iOA7/+c9/sGbNGlRVVeGll17Ck08+abfTppdCAoAgfBwSAD5AY2Mjtm7dig0bNmDIkCFYsGAB7rrrLkl72gRPGVqtFmFhYS57ymiPlpYWS09hWFgYZDKZJIHLSAj4NuLAa71FAJj3tz8p1z/AH7UVtQiPCUddVZ2VeY+/v7/XCgAxBoPB8q4HBwdDJpMhJiZGkrqqOzycNTc3Y8+ePdi4cSNSU1OxZMkSjBgxQtI8eggkAAjCxyEB4EOYTCYcOXIEOTk5qKurw7x58/Doo49K2isleP5Qq9UwGo2WSXSufsCF42i1WjQ1NVn8irvDgwgJAd/G1j1pnCIRJTfM8xDEAkBz6SYAIJYXCAKuCgB9fSPi+rf1p15XccvsRhAJ4oa/gCMB4Ezj37y/cwKAM3GWcxMEgLAcHBrilQJATG1tLbRaLaqrq5GQkIDk5OROxSbpjhgnQrTe/fv344knnkB2drbTUXd7KSQACMLHIQHgg3AcZ4ky/M0333QpynB72EbLlMlkHTbgm5ubLd5CIiIiIJPJEB4e3m12wSQGfJuA4CCHDWmOt6EXCwDbGADdIQBam29NjI5X3ipLVwRAbXkN/AMDYGw1wD/wlqC3F+PAVgD8Y02KZZs3CQABo9Fo8e4lRDJ3FIBLjGDm09TU5LYo57bReqdPn+71ARS9BBIABOHjkADwcToTZdhVhGF9rVaL8PBwKJVKK1/XHMehvLwcWq0WLS0tlt5+T9rLkhDwbfxsJsZyogm0ggBwFABMjDsFgLjxDwDFvPcke9vaEwC1/IRfQQAIy0DHAkDc+AduCQBvafzbUl9fD41Gg6qqKsTFxUEmk1m5zjSZTCgpKYFGo0FwcLDFzEfq+qyz0XoJC3SxCMLHIQHQQxA+aqtWrUJgYKDdKMNdheM4VFRUQK1Ww2QyITExEU1NTSgrK0NUVBRkMplk8wakgoSAb8PsPL+OBIBt4x/ovAAAzCJACgEgbvwDXRMAtj3gtgLAVzAajSgtLYVWq4Wfnx8SEhKg1+tRWlqK2NhYKBQKyXvixdF6H3roIbz88ssYMGCApHn0IkgAEISPQwKgh2EbZXj27Nl45pln0Ldv56KB2mIymVBeXg6VSoXGxkYwxqBQKJwyD/I0JAZ8G0EMcHx8jIiEGKvt3S0AAOuGvisCoPiaFomDzEHXau0E/XJkBtRTBIBAbW0trl27hpqaGjDGkJCQALlcLll9BQA///wz1q5dK3m03l4OCQCC8HFIAPRgSktLsX79euzevRsPPfQQ5s2b1+kow42NjdBqtSgvL0d0dDRkMhnCwsIsvsCLiooQGRkJhUIh6cfbHZAQ6Hn0i4myu767BIC48S9eD7hPAPhq499kMqGsrAxqtRqBgYFQKBSIiooCx3EoKyuDVquFyWSCTCZDfHx8p+z+TSYTDh8+jHXr1gEAFi9ejEmTJvUWH/3dAQkAgvBxSAD0AlpaWrB7926sWbMGiYmJyM7OxpgxYzr8GJpMJsswPQDLB9nefsI8ALXabIOsVColc//nTkgM9CwEIWCv4S9gTwDYNv7Nx2orAMSNf8CxADC0GPhjtJ2Y31kB8Pn6NPyfBVcA+Gbjv6WlBVqtFsXFxYiJiYFCobCy/xfT1NQErVaLsrIyqw6Hjqivr8f27dvx6aefYtSoUVi8eDFuv/12qU+FIAFAED4PCYBeBMdx+O9//4ucnBzcuHEDWVlZdqMMNzQ0QKPRoLKyErGxsZDJZAgNDXU6n/r6eqjVatTU1CA5ORnJycleH0CHhEDPQgg4JkZf32j1WywC7AmAptp6hMdZR3x1RgAIjX8BeyKgrqLaoQBYNq8B735yy7+9rwuAuro6qNVq1NbWQiaTITk52elefZPJhIqKCmg0GhgMBouDAdv9nY3WS0gGCQCC8HFIAEjAnDlzcODAAcTHx+P8+fNttnMch8WLF+PgwYMIDQ3F5s2bce+993qgpLewjTI8ffp0HDp0CGq1Gk899ZTTrvrao7W1FVqtFjqdDtHR0VAoFC4JCU9AQqBnYzupWBABtgKgqfaW2Y5YBNgKAMAsAqQUAP9Yk4Lm5mY8+7oOgLUA8BUEcx61Wg0/Pz8olUpER0d3aURQr9ejqKgI165dw9///nfMmTMHjDFLtN6FCxfiiSee8GhnQ15eHhYvXgyj0YisrCy88cYbVtubm5sxa9YsnDp1CjExMdi1a5evTkQmAUAQPg4JAAn45ptvEBYWhlmzZtkVAAcPHsTKlStx8OBBnDhxAosXL8aJEyc8UNK2nDhxAm+88QbOnDmDe++9F9nZ2Zg8ebKkpjvuaAy4k4aGBmi1Wiz8fdteYaJnIh4xEDf+gY4FAGAdA8BZASAQKspb3LP/1OIbluX/+3w1WltbIZPJ3OIPXyq6Q/Q3NjZixYoVyMvLQ0lJCbKysvDmm28iPDxc0nxcxWg0IjU1FYcPH4ZcLkd6ejp27NiBoUOHWtKsWbMGZ8+exbp167Bz50589tln2LVrlwdL3Wm8s/ImCMJpaEaUBPzyl79sd7g5NzcXs2bNAmMMGRkZqK6uhk6n68YStqW0tBT33XcfVqxYgddeew2lpaV48803sXnzZjzyyCP4/PPPYTAYOj6QEzDGEB8fjxEjRiAlJQUlJSXIz8+HRqOxRDf1NCaTCcXFxTh16gqLSe4AACAASURBVBQuXbqEiIgI5O0YgS93pXu6aEQ30FRbb/mzpbas0vJnj/YCgHWF3TkDsDtnAADg7rvvxp133ommpibk5+fj0qVLqK9vW1ZPUV9fj59++gmnTp2Cn58f0tPTkZaWJmnjv7i4GO+88w4eeOABhIaG4vDhw7hw4QKio6Nx//3345133pEsr86Qn5+PlJQUDBo0CEFBQZg2bRpyc3Ot0uTm5mL27NkAgKlTp+LIkSNwcyccQRCEXbzbMLuHoNVqrbzvyOVyaLVaj4aaj4uLw969e5GYeMtbyYQJEzB+/HgUFhZi5cqVWLFiBaZPn45Zs2YhKsq+lxVX6devH4YOHWqZEJifn9/hhEB3IvZuFBsbi9tvv71No0UQAWQeRNgTCMF9XXtuxb3/ANBYW281CmCLIAJCQkJw2223YeDAgaioqEBhYSGMRmOXvOV0BXsT/4cMGSL5yJ44Wm92djZOnjxpFSPgtddew29/+1uUlJRImq+r2KvnbUd6xWkCAgIQERGBiooKxMbGdmtZCYIgSAB0A/Z6eDxt/sIYs2r8i9enpqZi5cqVlijDU6ZMwejRo5GdnS1ZlOGgoCAMHDgQ/fv3R1lZGS5cuGDlEtCd10dwQ6jVasFxHGQyGW677bYO5zuIRwNIDBACzQ1mT0H+DuJg1FVUW8yAbBv/ncHPzw9xcXGIi4uzeMsRhLRMJnO7G16DwYCioiKL69+0tDTJ8xQCG65btw5xcXFYsmRJu9F6HdVn3Ykz9bw3fgsIguidkADoBuRyuaWXDAA0Gg2Sk5M9WCLniIyMxG9+8xssXrwY+/fvx29/+1sEBARgwYIFkkUZFqKAJiQkoLa2FiqVCoWFhZDL5Xa9fXQFsWvBmJiYLjVcaFSAsMUocj0qiAFhXXVxmVVae2KhM559+vTpYzE7KSsrw6VLlyyi1pHL3s7S0NAAtVqNqqoqJCcnY8SIEZIH/6usrMSWLVuwa9cuPPTQQ9i6davPTJJ1pp4X0sjlchgMBtTU1JC3IoIgPAJNApaIGzdu4NFHH7U7CfiLL77AqlWrLJOAFy1ahPz8fA+UsmtwHIezZ88iJycHZ86ckTzKsEBLSws0Gg1KSkoQGxsLhUJhNeTvapnLy8st8w3cZS5BQoBwFf/AQBzadrfkx7U1a3PVja8YjuNQUVEBtVoNk8kEpVKJ2NhYyXutxdF6s7KyMHv2bJ+L1mswGJCamoojR45AJpMhPT0d27dvx7BhwyxpVq9ejXPnzlkmAf/zn//E7t27PVjqTkPDFgTh45AAkIDp06fj+PHjKC8vR0JCAt5++2208j1/8+fPB8dxWLhwIfLy8hAaGopNmzZh5MiRHi5115AyyrAjTCYTSkpKoNFoEBwcDKVSiYiICKcaH3q9HlqtFqWlpS4FEpICEgNER3TH5HJxID/GGGQyGeLi4pwaFTAYDNDpdNBqtQgPD4dSqZT8/RFH62WMYdGiRT4frffgwYNYsmQJjEYj5syZg7feegvLli3DyJEjkZmZCb1ej2effRYFBQWIjo7Gzp07MWjQIE8XuzOQACAIH4cEANElOhtl2FVqamqgUqnQ1NRkMQ+yzUPordRoNF7hMpGEAGGLp7xKCa5tKyoqEBcXB5lMZnfSfWNjI9RqNSorK5GUlASZTCa5mQ9F6+0RkAAgCB+HBAAhCc5GGe4qzc3N0Gg0KC0tRVxcnGXUoaioCMXFxYiMjIRcLke/fv0kzbcrkBDo3XiTK1mj0WgZFfD394dMJkNMTAyqq6uhVqthMBigUCicHilwBSFa77FjxyzReqXyLkZ0OyQACMLHIQFASI44yvBjjz2GrKwsJCQkSJqH0WjE1atXrcwbBg0a5LUBkgRIDPQevKnhb4/a2lpcvnwZNTU16Nu3L1JSUiR3R8lxHP7zn/94VbReQhJIABCEj0MCgHAbjY2N2Lp1KzZs2IAhQ4ZgwYIFuOuuu7o0gbClpcXS2x8eHg6ZTAaO46BSqdDc3Ay5XI6EhASvtyMmIdAz8fZGP2D2hqXRaFBeXo7ExEQkJiaiqqoKWq0WgYGBkMlkXZ7oq9frsWfPHnzyySdITU3FkiVLMGLECAnPgvAwJAAIwschAUC4HZPJhKNHjyInJwe1tbWYN28eHn30Uad7ATmOQ1VVFTQaDfR6PZKSkpCUlNRmf71eD7VabZmMLZfLERQU5I5TkgwSAj0Db2/4cxyH6upqqFQqtLS0QKFQ2HUTWldXB61Wi6qqKiQkJEAmk7lkxldcXIyPP/4Y+/fvxxNPPIEFCxZ43D8/4RZIABCEj0MCgOg2OI6zRBn++uuvO4wy3NLSAp1OB51Oh379+kEulyM8PLzDnkmj0Yji4mJoNBr07dsXSqUS4eHh7jglSSEx4Ht4e8Pf9l1QKBSIiIhwej+tVovg4GDI5XJER0c7fPdso/VOnz5d8vk/hFdBAoAgfBwSAD2EvLw8LF68GEajEVlZWXjjjTestqtUKsyePRvV1dUwGo1YsWIFpkyZ4qHSmr36fPLJJ9i8ebNVlGGO41BQUIDg4GA0NDRYevs744lEGDlQq9VobW112+RGqSEh4N14e6MfMI+GaTQalJWVIT4+HnK5vNMN8traWmg0GtTW1qJPnz6W2AKuRuvtDjqqBz/44AN8/PHHCAgIQFxcHD755BP079/fQ6X1aUgAEISPQwKgB2A0GpGamorDhw9DLpcjPT0dO3bswNChQy1pXnzxRdxzzz3Izs7GxYsXMWXKFNy4ccNzheYxGo3Yv38/PvjgA9TW1qK+vh7Dhw/HypUrERkZKVljoqmpCWq1GhUVFUhMTIRMJvN68yCAxIA34e0Nf47jLO5y9Xo9FAqFpPNhDAYDjh07hjfffBN9+/ZFRUUFnnjiCbz88steEa3XmXrw2LFjGD16NEJDQ7F27VocP34cu3bt8mCpfRYSAATh43h3VyjhFPn5+UhJScGgQYMQFBSEadOmITc31yoNYwy1tbUAzL3vtiHqPQHHcTh58iRyc3PR0NCAhx56CGPHjsWNGzewd+9eNDY2SpZXnz59kJqailGjRiEwMBCnT5/GhQsXUFdXJ1keUiHEM/jxxx/x7m+BzX+VebpIvZpVf4zA20sMuHr1KvR6vaeL0waTyYSioiKcPHkSarUa/fv3x6hRo5CUlCTpaNeVK1dw4MABBAcH48EHH8S4ceNw7Ngx/OMf/0B5eblk+XQWZ+rBBx54wBIVOSMjAxqNxhNFJQiC8Djki60HoNVqraLwyuVynDhxwirN8uXL8fDDD2PlypVoaGjAV1991d3FtMtnn32G559/3sp0QIgy/OCDD2LChAmYN28elEqlJPn5+/tDLpdDJpOhsrISV65cgclkspgHedJ8obW1FUVFRdDpdAgPD8fAgQMtcxe+3GUWbDQi0D3Y9vYbjUaUlJTg3LlzCAoK6tAmvjuwjYkxfPhwhISESJqHEK137dq18PPzw+LFi7F+/XqLsKipqcG2bdvw3XffITMzU9K8XcWZelDMxo0bMXny5O4oGkEQhNdBAqAHYM+My7ZhsmPHDjz33HN49dVX8d///hfPPvsszp8/71F7eMYY3nvvvTbr4+Pj8fvf/x5Lly7Fnj17MHfuXCQkJEgaZZgxhpiYGMTExFiin169etVt0U/bo6amBhqNBnV1dUhOTsaIESMc5i9umJIYkB5HZj7+/v5ITk5GcnKyxSa+sLAQSUlJSE5O7vbnRRwVe9SoUZLHv6ivr8ff//53bN26FaNHj0ZOTo7daL0RERF46aWXJM27szhTDwps27YNP/zwA77++mt3F4sgCMIrIQHQA5DL5VCr1ZbfGo2mjYnPxo0bkZeXBwAYM2YM9Ho9ysvLER8f361ldYWgoCDMmDEDzzzzjCXK8O9+9zvJowyHhoYiLS0NBoMBOp0Op06dQkREBBQKBcLCwiTJwxaxl5WQkBDI5XIMHTrUpR5lobFKQqDruGLfHx4ejqFDh6K1tdXyvPTr1w8KhcJt3qZMJhNKSkqg0WgQHBwMhUIh6RwZASFa79GjRzFz5kwcOXLEZ6L1OlMPAsBXX32Fd999F19//TV5KiIIotdCk4B7AAaDAampqThy5AhkMhnS09Oxfft2DBs2zJJm8uTJePrpp/Hcc8/hp59+wvjx4y1RdH0JjUaDVatWuTXKsGCDr1arwXEcFApFlwMjCTQ0NECj0aCysrJTftbbg4SAa0g1qdc2ToUQjE6KXvmWlhZoNBqUlJQgNjYWcrkcffr0kaDUt+gp0XqdqQcLCgowdepU5OXlYfDgwR4src/jWx8OgiDaQAKgh3Dw4EEsWbIERqMRc+bMwVtvvYVly5Zh5MiRyMzMxMWLF/HCCy+gvr4ejDH8+c9/xsMPP+zpYneaxsZGbNu2DRs2bEBaWpokUYbt0dDQALVajerqaiQnJ3fKJanJZEJ5ebllwqFcLkdsbKxbza9IDDjGnd58mpubodVqUVJSgujoaMjlcvTt29fl49TW1kKtVqO+vh4ymQxJSUmSm/n0xGi9HdWDEyZMwLlz55CUlAQAUCqV2Ldvn4dL7ZOQACAIH4cEAOHTdDXKsLOIJ+hGRkZCoVB02LDT6/XQarUoLS1FTEwMZDJZpxqDXYGEwC26042nWPSJR5HaE30mkwllZWVQq9UICAiAUqlEVFSU5KJWiNZ74MABPPHEE8jOzqZovYSrkAAgCB+HBADRI+A4DleuXMGHH36Ir7/+GtOmTcPs2bMlt1/mOA7l5eUWW2OlUomYmBhLI00cfKylpQUymUwyc5Cu0FuFgDf47hebfcXHx0Mmk1l562lpaYFWq0VxcTFiYmKgUCgkN/MBzOYvq1evxtWrVylaL9FVSAAQhI9DAoDocTiKMix1T2p9fT1UKhVqa2stPajFxcVunxDaVXqDGPCGhr8tgitRrVaLoKAgxMTEoKamBnV1dZDJZEhOTpZcKBoMBuzbtw8fffSRJVrvfffd53Nzfwivgx4ggvBxSAAQPRYhyvCqVavg7++PBQsWYPz48ZLa3tfW1uLmzZuoqqoCAMTFxaF///6WYEPeTE8UAt7Y8BfDcRzKyspw/fp1NDc3w8/PDwqFQnJXopWVldiyZQt2796Nhx56CAsXLvSKaL1Ej4EEAEH4OCQAiB4Px3E4e/YscnJycObMGcyePRvPPPNMp+3xbXtyFQoFoqKirMyD/P39oVAoPB4syhl8XQh4e6MfMM8h0Wq10Ol0iI6OhkKhQGhoqMWVaFFREcLDwyGXy7s0cvTzzz9j7dq1OHXqFLKysjBr1iy3ubIlejXeXakRBNEhJACIXkVZWRnWr1+PXbt2uRxluLGxERqNBhUVFXZtucXU1dVBpVK51YuLO/AlMeALDf/6+nqo1WrU1NRYAonZm6Delbkj4mi9/v7+WLRoESZOnOjRIH9Ej4cEAEH4OCQAiF5JS0sL9uzZgzVr1rQbZbirLjy7a4Kn1HirEPCFRn9HE8U7wtZ7lFwut2tSZhutd/HixRgyZIik50IQDiABQBA+DgkAQjLy8vKwePFiGI1GZGVl4Y033miTZvfu3Vi+fDkYY7jrrruwfft2D5T0FhzHWaIM37hxA3PnzsWvf/1rFBcXIzc3F+np6V3y5y4gdvEYGBhoMRsi8yDn8IWGv8FgQFFREYqKipx2FdsewjOj0Wig0+lQXV2Np556Cjqdzipa7wsvvOAV0Xqdef8BYO/evfj1r3+NkydPYuTIkd1cSkIivLviIgiiQ0gAEJJgNBqRmpqKw4cPQy6XIz09HTt27MDQoUMtaQoLC/HUU0/h6NGjiIqKQmlpKeLj4z1YamvUajWWLl2KI0eOIDw8HDNmzMCrr74quelObW0tVCoVGhoaIJfLkZiY6PXmQQaDAVNmFHR7vv/aOdLrRZI4WFxSUpLkE3oB87uzbNkynDx5EkFBQXjrrbfw/PPPe020Xmfef8BsGvfII4+gpaUFq1atIgHgu3j3S0kQRIeQkSghCfn5+UhJScGgQYMQFBSEadOmITc31yrNhg0b8NJLL1l6K72l8c9xHFatWoUnnngCISEh2Lt3L1577TUcOHAA8+fPR0FBAaQUyuHh4bjjjjtw9913o7m5Gfn5+bhy5Qr0er1keUhFQ0MDLl26hJMnT2L9ilgc2HqX2/P8186R2L1+MP73dyE4efIktFotjEaj2/N1BY7jUFFRgYKCAvz888+Ijo7G6NGj0b9/f0kb/3q9Hlu3bsWLL76IiIgI7NmzB8uWLcOmTZswY8YMnD59WrK8uoIz7z8A/P73v8frr7/ucO4MQRAE0T14R/cR4fNotVooFArLb7lcjhMnTliluXz5MgBg7NixMBqNWL58OSZNmtSt5bQHYwwxMTH48ssvER0dDQAYN24csrKycPToUaxYscItUYaDg4MxaNAgDBgwACUlJTh37hyCg4OhVCoRERHhsZ5vsQ27EMVWHEdBbI4jpYmQ+LiRkZGIjIxEc3MztFot8vPzvWIOhcFggE6ng1arRXh4OAYPHuwWLztCtN79+/fjySefRG5uriXWxNixYzFnzhycPHnSa4SRM+9/QUEB1Go1Hn30Ubz//vvdXUSCIAhCBAkAQhLs9ZDbNmANBgMKCwtx/PhxaDQajBs3DufPn0dkZGR3FdMh06dPb7POz88PEyZMwPjx43HlyhWsXLkSK1askDzKsJ+fH5KSkpCUlISamhqoVCpcvnzZYh7UXd5cxK4qo6KikJqa2mHjVmi0d0UItGffLxZJZWVluHDhAvz9/aFUKrvVxWpjYyPUajUqKyuRlJSEESNGSG7mA9yK1nvt2jVkZ2fjD3/4g8Novenp3jMvoqP332Qy4ZVXXsHmzZu7sVQEQRCEI0gAEJIgl8stXk8AQKPRIDk5uU2ajIwMBAYGYuDAgUhLS0NhYaFXNWTswRjD4MGD8eGHH1qiDD/yyCMYNWqU5FGGIyIicOedd0Kv10Oj0eDEiROIj4+HXC532BDsKrW1tVCr1ZaItOnp6S6PcrgqBFyd1Ovn54eEhAQkJCSgrq4OarUahYWFSE5ORlJSklsa44JrTpVKBYPBAIVCgcGDB0suyHpCtN6O3v+6ujqcP38e999/PwDzCEdmZib27dtH8wAIgiA8AE0CJiTBYDAgNTUVR44csTQit2/fjmHDhlnS5OXlYceOHdiyZQvKy8txzz334MyZM4iJifFgyTuH0WjEgQMHsHLlSrdFGRbyKSkpgUajQZ8+fSzmQV3FZDJZjisOZiZlo9OeGJDSm09rayuKioqg0+kQEREBhUIhiTmO0Wi0mPmEhYVBqVSiX79+EpTYGttovS+//DL69+8veT7dgTPvv5j7778f77//PjX+fRffUacEQdiFRgAISQgICMCqVaswceJEGI1GzJkzB8OGDcOyZcswcuRIZGZmYuLEifjyyy8xdOhQ+Pv74y9/+YtPNv4BwN/fH4899hgyMzNx7tw55OTk4O2338asWbPwzDPPSGYX7u/vb+nlrqmpwc2bN9Hc3Ay5XI6EhASXBYfYx3xsbCzuuOMOt9nUu9t1Z2BgIPr37w+lUomKigoUFhbCZDJBLpcjLi7O5WvT1NQEjUaD8vJyJCYm4p577kFQUJDk5f7pp5+wdu1anD59GllZWfjuu++65C7UG3Dm/ScIgiC8BxoBIAiJ6EqUYVfQ6/VQq9UoLy9HQkIC5HJ5uw1VjuNQXV0NtVoNvV5vEQ/e7nq0M4ijNbtybVQqFVpaWqBQKBAfHy/5SI7JZMKXX36JdevWUbReoidAIwAE4eOQACAIiWlpacHevXuxevVqxMfHY8GCBXajDHcVo9GI4uJiaDQa9O3bF0qlEuHh4VbbBVOWvn37QqFQSGI+5AvYXhuFQoHw8HCLiZOwXavVIjQ01G3XhqL1Ej0UEgAE4eOQACAIN8FxHL7//nvk5OTg+vXrlijDUk/mFSarqtVqtLa2Ij4+Ho2NjaiqqkJiYiJkMplbTFl8AY7jLJ6V9Ho9EhIS0NzcjIqKCrdOrr558ybWr1+PY8eOYebMmcjKyvKKaL0EIREkAAjCxyEBQBDdgEajwerVq3HgwAFkZmYiKysLCQkJkh1f8N1/48YNS0AxmUzWoQlMb0AQAdevX0ddXR0AIDExUfKYAhzH4dtvv8WaNWtQU1ODhQsX4vHHH/eaaL0EISEkAAjCxyEBQBDdSFNTE7Zu3YoNGzYgNTUVCxYswN13391p7zu2vvvlcjnCwsJgNBpRVFRkCVilUCjc4snGmxE8HanVavTp08di5sNxHEpLS6HRaBAQEACFQtGlmAJ6vR579uzBJ598grS0NCxZsgT33nuvxGdDEF4FCQCC8HFIABCEBzCZTDh27BhycnJQU1ODF198Eb/61a+c7i0WfOHX1tYiOTkZycnJdvflOA6VlZVQqVQwmUxQKBSIi4vzKR/zrtLc3AyNRoPS0lLExcVBLpcjJCTEblpnr6M9dDodPv74Y3zxxRd48sknkZ2dLemoDkF4MT23AiGIXgIJAILwIBzHWaIMHz9+vN0owyaTqUs917bRbGUymVsCaHmKmpoaqNVqNDY2uuzpyJWYAqdPn8aaNWss0XqnTZvmtiBtBOGlkAAgCB+HBADR48jLy8PixYthNBqRlZWFN954w266vXv34te//jVOnjzpFQGJhCjDW7ZsQXp6OrKzs5GWloabN2/izJkzSEhIQGxsLORyeZds1w0GA4qKilBUVCRpAC1PIIgitVqNoKAgKJVKREZGdnqEQ5hLoVarYTQacfXqVTz55JPw8/PDvn37sH79esTHx+OVV17B2LFjvWIkxZnnfffu3Vi+fDkYY7jrrruwfft2D5SU6EF4/sEnCKJLkAAgehRGoxGpqak4fPgw5HI50tPTsWPHDgwdOtQqXV1dHR555BG0tLRg1apVXiEABIxGI/bv34933nkHtbW18PPzw5IlSzBjxgxJffdzHIeKigqoVCoAgEKhQGxsrFc0ajuipaUFGo0GJSUlkogie5SXl2Pp0qX47rvvYDAY8Pjjj+P111/3qmi9zjzvhYWFeOqpp3D06FFERUWhtLQU8fHxHiw10QPw/kqCIIh28V++fLk7j+/WgxOELSdOnMDZs2exaNEi+Pv7o7q6GpcuXcK4ceOs0i1duhTPPvss8vPz8fDDDyM5OdlDJbamqakJn376KXJycjBkyBDMnTsXBoMBX3zxBUwmE9LS0iTz6sMYQ2hoKJKSkhAeHo7i4mJcvXoVJpMJffv29cogVXV1dbhy5QpUKhWioqIwZMgQxMbGSm7K9NNPP+GDDz7AuXPnMG/ePPzP//wPDh06hPPnz0OhUHjN8+LM8/7ee+9h8uTJGDt2LAD4fNRhwit429MFIAiia5B/OqJHodVqoVAoLL/lcjlOnDhhlaagoABqtRqPPvoo3n///e4uYrvU1dWhpKQE+/fvt/TSzpw50xJlePz48W6JMty3b18MGTLEYgt/8uRJREVFQaFQeLzBaDKZUFZWBrVajYCAACiVSkRFRUk+UmEvWu/69estQuill17Ct99+6zUmY4Bzz/vly5cBAGPHjoXRaMTy5csxadKkbi0nQRAE4V2QACB6FPZM2sQNRZPJhFdeeQWbN2/uxlI5T3x8PN5666026+Pi4vC73/0Or7/+Ovbu3Yu5c+ciPj4e2dnZ+MUvfiFZb31gYCD69+8PpVKJ8vJy/Pzzz/Dz84NCoUBMTEy3mgeJXZzGxMRg2LBhkpv5ALei9X766acYM2YMPvzwQ7vRehljGDduXJvRJE/S0fMOmOd8FBYW4vjx49BoNBg3bhzOnz+PyMjI7iomQRAE4WWQACB6FHK5HGq12vJbo9FYmWvU1dXh/PnzuP/++wEAxcXFyMzMxL59+7ymV7c9goKC8Mwzz2D69On4/vvv8eGHH+L3v/895s6di6lTpzp0d+kqjDHExcUhLi4O9fX1UKlUuHLlCmQyGZKSktwa3ErIr7a2FjKZDOnp6W7J78aNG1i/fj2OHz+OmTNnWmzkfYmOnnchTUZGBgIDAzFw4ECkpaWhsLAQ6enp3V1cgiAIwkugScBEj8JgMCA1NRVHjhyxNB63b9+OYcOG2U1///334/333/eJxr8jhCjD+/fvx2OPPYa5c+ciMTFR8nzEPfLR0dFQKBQIDQ2V5Ngcx1nMfPz8/KBUKrsUnKu9fMTRel9++WU89thjPhut15nnPS8vDzt27MCWLVtQXl6Oe+65B2fOnEFMTIwHS074ODQJmCB8HN/86hGEAwICArBq1SpMnDgRRqMRc+bMwbBhw7Bs2TKMHDkSmZmZni6i5MjlcvzpT3/CsmXLsG3bNjz99NOSRBm2JTAwEAMGDIBSqURZWRkuXrzY5Ui6wpyDoqIiy6Red8w5EEfrHTJkCJYvX4577rlH8ny6G2ee94kTJ+LLL7/E0KFD4e/vj7/85S/U+CcIgujl0AgAQfQwxFGGq6urMW/ePJeiDLtCXV0dVCoV6uvrLeZBzrgqbWhogEqlQk1NjcsReF2BovUShFugEQCC8HFIABBED8WVKMNdpaWlBVqtFsXFxQ798ouDbAHujTsgjta7YMECPP300xStlyCkgwQAQfg4JAAIohdQU1ODTZs2YfPmzVZRht3hStM2Mm9YWBh0Op3bIw8bDAZLtN6EhAQsWbLEa6L1EkQPg14qgvBxSAAQRC/CaDTiwIEDWLlyJfz9/ZGdnY0JEya4JehXSUkJrly5Ar1ej7i4OKSmpkrmpUhMZWUlNm/ejD179uDhhx/GwoULvSpaL0H0QEgAEISPQwKAIHohHMfh3LlzyMnJQUFBAWbNmoVnnnmmyz3zHMehsrISKpUKJpMJCoUC4eHh0Gq1KC0tRVxcHORyuSRC4KeffsK6detw+vRpzJ07F7Nnz/Z40DKC6CWQACAIH4cEAEH0csrKyvDRRx9hrRGujgAAH0lJREFU586dGD9+PObPn+9ylGGDwQCdTgetVovw8HCL6Y8Yk8mEkpISqNVqhISEQKlUIiIiwiUTHSFa79q1axEYGIhFixbh4YcfdssIBkEQDiEBQBA+DgkAgiAAmCfy7t27F2vWrEFcXJxTUYabmpqgVqtRUVGBpKQkJCcnIygoqMO8ampqoFKp0NTUBLlcjsTExHbzEaL1bt26FRkZGVi0aJHdaL0EQXQLJAAIwsehbjOCcCN5eXlIS0tDSkoKVqxY0Wb7Bx98gKFDh2L48OEYP348bt686YFSmhGiDP/73//G66+/jk2bNmH8+PHYtm0b9Hq9JZ3JZEJlZSXOnDmDCxcuICIiAqNHj8aAAQOcavwDQEREBO68804MHz4cjY2NOHHiBK5evWqVD2CO1vvmm2/ioYcegtFoxNGjR7FmzRqPN/47uq8qlQoPPPAA7rnnHgwfPhwHDx70QCkJgiAIwj40AkAQbsJoNCI1NRWHDx+GXC5Heno6duzYgaFDh1rSHDt2DKNHj0ZoaCjWrl2L48ePY9euXR4stTVarRarV6/Gvn37MGXKFAQFBWHv3r1YsWIFMjIyEB4eLkk+RqMRJSUl+OMf/4jS0lJMmjQJR48eRW1tLRYuXOhV0Xqdua8vvvgi7rnnHmRnZ+PixYuYMmUKbty44blCE4S00AgAQfg4NAJAEG4iPz8fKSkpGDRoEIKCgjBt2jTk5uZapXnggQcQGhoKAMjIyIBGo/FEUR0ik8mQnZ2NSZMmYfv27cjLy8O9996LuLg49OvXT7J8/P39ER0djTFjxqCxsRGrV6+GWq3GvHnzkJmZ6TWNf8C5+8oYQ21tLQBYgp0RBEEQhLdAAoAg3IRWq4VCobD8lsvl0Gq1DtNv3LgRkydP7o6iOc17772HGTNmYOTIkbh+/Try8/ORlZWF9957D5MnT8Znn30Gg8HQpTx0Oh3++Mc/4v7770dJSQl2796Ny5cv4/PPP8ePP/6IESNG4Pr16xKdUddx5r4uX74c27Ztg1wux5QpU7By5cruLiZBEARBOIQEAEG4CXvmdY483mzbtg0//PADXnvtNXcXyyUWLlyIb775BtOmTUNgYCD8/Pzw4IMPIjc3F5988gny8/Nx33334a9//SuqqqpcOvbp06eRlZWFmTNn4vbbb8fJkyfxhz/8AQkJCQAApVKJP/3pT/j++++9yq+/M/d1x44deO6556DRaHDw4EE8++yzMJlM3VVEgiAIgmgX7xlXJ4gehlwuh1qttvzWaDR2TUG++uorvPvuu/j6668RHBzcnUXsEEd+9RljSElJQU5OjiXK8COPPNJhlGGDwYDc3Fx89NFHTkfrFUykvAVn7uvGjRuRl5cHABgzZgz0ej3Ky8sRHx/frWUlCIIgCHvQCABBuIn09HQUFhbi+vXraGlpwc6dO5GZmWmVpqCgAPPmzcO+fft8tnEYERGBJUuW4NSpU3j00UexdOlSPPnkk/jyyy8tvd6VlZX44IMPcN999+Hs2bPYtm0b9u7di/vuu8+lOADegDP3ValU4siRIwDMAcuEaMgEQRAE4Q2QFyCCcCMHDx7EkiVLYDQaMWfOHLz11ltYtmwZRo4ciczMTEyYMAHnzp1DUlISAHPDcd++fR4uddfgOA7nz59HTk4O8vPzERMTg9raWmRlZWHWrFk9IlpvR/f14sWLeOGFF1BfXw/GGP785z/j4Ycf9nSxCUIqfEu1EwTRBhIABEG4DZ1Oh127dmHRokUUrZcgeg4kAAjCxyEBQBAEQRCEK5AAIAgfh7rkCIIgCIIgCKIXQQKAIAiCIAiCIHoRJAAIgiAIgiAIohdBAoAgCIIgCIIgehEkAAiCIAiCIAiiF0ECgCAIgiAIgiB6ESQACKIXkZeXh7S0NKSkpGDFihVttjc3N+Ppp59GSkoKRo8ejRs3bnR/Ie0wZ84cxMfH44477rC7neM4LFq0CCkpKRg+fDhOnz7dzSUkCIIgCN+BBABB9BKMRiNeeuklHDp0CBcvXsSOHTtw8eJFqzQbN25EVFQUrly5gldeeQVLly71UGmtee6555CXl+dw+6FDh1BYWIjCwkJ89NFHyM7O7sbSEQRBEIRvQQKAIHoJ+fn5SElJwaBBgxAUFIRp06YhNzfXKk1ubi5mz54NAJg6dSqOHDkCNwcLdIpf/vKXiI6Odrg9NzcXs2bNAmMMGRkZqK6uhk6n68YSEgRBEITvQAKAIHoJWq0WCoXC8lsul0Or1TpMExAQgIiICFRUVHRrOTuDM+dGEARBEIQZEgAE0Uuw15PPGHM5jTfiq+UmCIIgCE9AAoAgeglyuRxqtdryW6PRIDk52WEag8GAmpqadk1vvAVnzo0gCIIgCDMkAAiil5Ceno7CwkJcv34dLS0t2LlzJzIzM63SZGZmYsuWLQCAvXv34sEHH/SJnvTMzEx8+umn4DgO33//PSIiIpCUlOTpYhEEQRCEVxLg6QIQBNE9BAQEYNWqVZg4cSKMRiPmzJmDYcOGYdmyZRg5ciQyMzMxd+5cPPvss0hJSUF0dDR27tzp6WIDAKZPn47jx4+jvLwccrkcb7/9NlpbWwEA8+fPx5QpU3Dw4EGkpKQgNDQUmzZt8nCJCYIgCMJ7YW728OF59yEEQRAEQUiJ9w8LEgTRLmQCRBAEQRAEQRC9CBIABEEQBEEQBNGLIAFAEARBEARBEL0Id08CJjtBgiAIgiAIgvAiaASAIAiCIAiCIHoRJAAIgiAIgiAIohdBAoAgCIIgCIIgehEkAAiCIAiCIAiiF9GhAGCMGRljZ0R/AxhjIxljH3Y2U8bY3Yyx7/nj/cAYG2WzfaIov3rG2CV++dPO5tlBeTYxxtK6sL+GMRbZznY/xtgbnT2+lDDG3mGMLenEfk8wxl7jl59kjA0RbfuWMXa3nX1GM8b+6kIeVteJMZbCGDvjIO27jLEHXDuLzsEY82eM/buT+1pdKwdppjh43jcxxrIYY3/rXMnbzXMbY+xxqY/bTn4fMMYuMMZWMMZeYozN6K68+fwteYrP3dGz68Tx7mWMTerEfoMYY9Nc3c/BsX7DGAvhlwMYY9VdONYExhjHGJstWpfOr3O5vuhE/g8yxjI6ue+v+XKmdCF/2zptKGPsR8ZYAWPsto7ef/H1b6/e6g5cfb/48nKMsWzRunWMsZnuKWHXYIyFMMYMfB15kb9Hixhj7Tod4c/zKRfzimWMvehEuiGMsR9cObaT+d/JGDvLn6NC6uOL8pnEGNtrs24nY+xRfvlJ/nr/yNfjz4vSPc8YO8+v/5F/dvp1tJ9o//mMsVJm3c68zYWyBzDGXhf9dngv+HO6JsrnmLP52BynzfVyBzbPegy/7i+MMS1jrNwm7RuMMTVj7H1nj++MF6AmjuNsP5A3AHTlYf8zgLc5jjvEGJvC/75f2Mhx3L8A/AsAGGPHAfyW4zin82OMBXAcZxD9ZjBHPTbZS89xXJuHUmL8ALwBYIWb83EbHMd9Jvr5JAATgJ872OcEgBMuZOP0deI47i0Xjtshts+MTV5GAOM6eegOrxXHcQcBHOTL8S2AhRzHneF/Z3UyX6+Bf/+yAMRwHNcq8bH9+fvTLhzHrZYyXwD3ArgDQJ6dMjl8lgAMAjANwE4JyvAbAJ8A0EtwLAA4B3PZtvC/pwH40VFixpic4ziNRHk/CKAcwPed2Hc6gG9hLu87nczf9j19EsBejuP+yP/u7PvvEh19q5yhk896CYBXGGMb2nl2HdLBM+8OqoV2CWMsCeb3qS+AP7WzTwqApwDsdiGfWAAvAviok+XsKv8HwA6O46zOizHmB4DjOI5zdwEYY6EAVgG4l+O4Yr7TQcFvexzAPAAT+G3+MNf1sYwxo6P97PApx3G/7UTZ/AAEAngd5nakMyziOO6Aq3l1BWffDwfpLM86zz9hvq6nxIk4jlvBd0I43RHSKRMgxtj9jLED/HIcY+wwY+w0Y2w9Y+wmYyy2g0NwAML55QgARS7kHcDMvYn5vDLO4tdPYIx9xRjbCaCAV/vnGWPrAJwGkMQY+4iZRxwuMMaWiY75LTOPSgQwxqqZuZfyR8bYfxlj8XbKID7ntRC5O2WM7WeMneLzEBpvKwD0Y6JRDAfpbPOxjCwwxjIYY1/xyw/y5TvDl6Evv/4N0XURn98yZu5VPgxgsINreo1fjmWMmRhjv+B//5eZR32yGGN/Y4yNAzAFwF/5/Afwh5nG531JtO8Extjn/PI7jLGNjLGveQX+kp1TbnOdAATw+11gjB1it3o8xb24f2HmnqCzjLH37JzfO4yxLYyxY4yxQsbYHFH5LM8Mv+51/rk5zxh7WXR9qkXHc3Sdn+fX/cjMvfeOrpWryBlj/+LLbvkQMMYm8/fnNGNsl/Ac2Jz7fMbYSb5MexhjfUSbJzLG/s0Yu8wYm8yn78Nfq3P8cX/Jr/+BiUbJ+HfmLsZYGGNsM389Chhjv7JT/i9g/jifZIxNZaJRKP65PssY+46/jxbhw0QjH4yxPMbYfezWO/oOYywfwChm7qn+mpnfp0OMsQQ716HdkS8793gaY+xj0fJ5/hoe46/hMgAz+PsqnNN6Zn7HNjG+15i/JqcYY6P5Q68A8AC/3yLmuD6T8df4DJ/3L2zK+wqAeAD/Zny9wK9vU3cxxhIYY//k72E+c9zTfg1AODPXAQzAQ+A7Yvjj3MsYO8GXMxf8B5cx9gozv38/MvN76c/M34Fwfjtj5nc+ljH2GH+MAsbYl4yxeGbu7csC8Bp/vr/gj5PDPxfXGGNPOLhv4QBGA3gBZiEgrLfUPfxvS482s6kvWNv3dBGAhQDmM3P94NT73xGMsXDG2FFmfq/Oslu9q/a+VXbfbcbY28z8Pp/nz6lNrzezfr++5Z8Jq7rZDsUA/g3gWTvHE9/3fzDGIkTHfpcx9g2AJazjb0gGv1zAGPsPY2wwv/07xtgdovxOMMaGOXtdOY7TAcgGsIjffzAzv3un+Wc+nU+6AsAE/h6/1E46MSsADOX3eZcxFsXMdYBwDyfbuV5pfPq7GWOBzPzdzOffj+f5NJOYuQ3xGX9fNtk5zhP8eb3EzPX/EP4YH8H8nMQzxmazW9+rP/L7hTDGypm5Xilg5jpxDGPsG/5dmujsteWJhFkcV/HXW89xXCG/7S0Ar3AcV8xvM3Ict57juOsd7Nchjq61nevwAYBI/poL1zGQmb/BFxljXzDGgjrI6yPGjyIwcx31FTOzkzG2hn/WLzHGHrazbxxj7AAzfzP/wxi7nV+/gjG2lpnr5487eBb+xRjbBSc61jmO+y/Mgr3rcBzX7h8AI4Az/N9n/Lr7ARzgl1cBeJNfngRz4z62g2PeDkAFQA1AC6B/O2mPAxgp+r0AwBv8cjDMDTclgAkA6gEo+W0pMD986aJ9o/n/ATBXdkP5398CuJtfzwGYzK//QMjLpkxrAPxffvkxfp9ImzxCAVwEEMUft9rmGG3S2clHIzpuBoCv+OVDAEbzy2EA/GH+gK2BWYz4wdwz+QsAo2DuxesDs9i6DmCJnby+ApAG4HEAJwEs5fe5ym/PAvA3fnkbgMdF+34L4D1+ORNAHr88AcDn/PI7/DUPgrnhUgHA36YMVteJv4etAO7kf/8TwDRxGQAkALgAc68ZhOtlc9x3YK4oQvi8Nfx+ts+McK1CAfQD8BOA4eJytXOd74K591C4r9H2rpUT79u3AO4W/c4CUMiXpw/M70wyfx5fAwjl070F/pm0OV6MaHkFgGxRuQ7w55DGHzeYv+8b+DTDANzk79lrAH7Pr5cD+Ilf/rPonkQBuAwgpIP7+g74Z5C/xqP45fcBnLF93vjfeQDuw6139ElRHfAd+DoHwAwAHzl4BpaIzv1xO+++uIzTAHwsKmOC+PmyU753AOQL584/Q8LyEAAnbN+JDuqzpQCW8uv9AYR1UD84rLsA7AKQwS8PAHDezrEmAPgc5lGF+QD+B8AGm+t2gb8Hffhyvs+v1wEIsrk+qwE8yy+Pxa06IQq33tX5uFVvWPIR3aMdML9nwwH87OB9eQ7Aen45H8BwB9d5HYCZcFBfoG2dJj5vZ95/cZoU8M+xTVkDAfTjl+MBFIrSW75VaOfdxq16hfHXZ3IHz7rdutkmfQrM3/cUmL9FfsL14rdfBHAfv/z/RPf9WwArRcfp6BsSAb7Oh7mtsItfnis65lDw70o7dWQIgHKbdQxAI59HXwDB/Po7APxHlOde0T5209kcdwiAH0S/g8C/i/yzdFmcDuY6swD4/+2de7Bf0xXHP8szqUfEjI5HPCqqwlBUW9qgKpQyab3rEVoRRYdOUJ1S9SgtfQwVajxSbxEaTah3QgXxCCK5GDTSeEdKlITIRVb/+K6T37nnnvP7nXtzUzXZn5lMfr/z22fvffZZe+2191p7XzaL68ehCIas3lOBdaIubwNrov79JDk7J1feOcgjnJXxCbBVfF8fjeerR70ejHx7IV2wU6S7A7gVyejXgUdKyunQNnHtBmDPXP+YBVwHHECj/8yloO8LeZTeV0hzFDCbhp35VLRJs7bOt0MHeYjf22nYd7cA++aeaUaunCvi+spo/N4RjWHr5dKPQ/K1KbJbl8+3F9KTma7+bta+8e4epiFjzWThPaBfHVlvcf0ooi/V+dfdEKA8A4G9ANz9TjN7p0aeR6NZ4xhTTN5IpLDrsCswwBpxtH1orGo/7O4v59K+6O6Tc98PNLOhqCOsjV7os4X857v7HfH5CcpdvzugF427jzOzubnfhpvZ4PjcD+iPBK1IWbq6YU4PAeeb2fXAGHefFzPT3YmVbCTQGyMX5hh3nw/MN7NbK/J8IJ5rAHKjHo7Cd+qG8Nwc/z+BjIwy/u7u7cBsM5sDrIGUQzOmu3tbk7znoMHzMjO7DRm1ZYx19w+BD2PF6qsodCIvM9ujtvoAwLSCOJCOMlLVzn3RgDYHIPu/hxjv7nOjTs8hA3FNJL+TTIuA2QBQZAszOxOtxqxCx/a50RVq8LyZvYL60UDg9/EMz5jZ68gwuBENIr9Gijxzo+8K7G6NvRu9on4vtHook6dwBXd/LC5dTz090A5kIWkD0KA7PtphWWQY9yQPAVeb2U005LyMcSFjIGP+QjP7MvAx6t9lVOmzycAlJo/XWHevDMXJUaW7BgFfssZicV8z6x06ocho4Br0/kah0BxM8aefc/cHzexCNCHMvFHPANeavAJjc/mcFHn9IL6DZONGM1sTtVEzORnrGtWmmdk6FWkOpBEyeEN8n9Ykz7r6ooqq/v9Y5R0NDDjXzAZGHda1hrc8P1Z9g+q+vbNpL1YvpNufQAZeM+roZtx9uskDd8CiCuu993L3rPyr0DvNyIeytRpDVkP9qNgXbgCeCh1yOHBFi+epIhPwXsAIM9scGYobVKSvm65Yxh/M7Jtxz/rW2P+3NjAGGOzumVzvCmxkjf0Uq9II0ZjksXJuZlOj/FY2wAvunsnetsA92VhjZqNQ+/8DmOfu90W6NuBNd//YzNoqntMrynMAdz8kdNkgNCHdCRmbizCzbYDL4xmHu/u4OvcFnUKAzGw5qts63w5lPO/u2bhdlPtOIUBhQx0DTECLZHk7cnTooWfNbBaddflA4LTI5/bwPPSK38a6+4L43EwWHvKeC6esTU/8JeDu/LXfw4CfxuebkNB0pbxj3H1Ch4tmg4D3C2nfz/3+xSjza+7+HzO7FimAIu25z59Q3UadOkzUYQe02jbfFM/dqYy66ZDhkIVpLfrd3c8ys1uAPVBYxbdQu5zl7iMLZZ1YVtcSHkCraRugOPyToo4Ta9wLkAl5szZbkPvcLF3te9z9o1A8uyBD42jU0YoU2yD7npeZOrJc1c7Hl5TRU5S1gaHVvE4u+wJXo1XCp03hJfnwj7I2KW0Dd3/JtEF5U2Qg/DB+MrRy+mKtJ+lIs/bOyz507B/zQyFneUxz98WN0V5YqE++vGFo5WxPYKqZbVGRR16WTkBelUPQitG8intK9Rko1BL18evM7Lfufl2LZ6jSXYb0XnvnWzri7q+ZrM4dkXfi2/HTssBHpvCYCchzm72D70T67wG/NIVzPABcGQbkYODUSHsR8JsYKAchXVNFXu7LQl3WiHI3MTOP5/3IzE6mQn66oC+qqOr/dXTZoWiCt3UYY6/SkLOiHurUt61jLPZrZnYW5eNGkTq6OeNsNBHP9mK00on5ercaQ84G7nL3P5s2bN8J4O7vm/b6DUYx713elI8mHe+4+7tmdg5aGT8YtU/VYszPaqbL8yPk3dsq3uEsGu9gDtrHsh2Nia0BP3b3+/OZmA4Q6M54WHe8yue9MPd9YUU5b6NFrDyro+cBIBYhpkaoyhRkyD8HbIUW0h4HtjSFTvZucV8dmrV10dYr0p223Ry1w9qF61W2Q0az91B8X1Wy0Op5lgg9cQzog2hjDbEKXRSiMl5Hihs0wNSOC0MxqcdkCtcUb9e7xT2g2dZc4D3TpqGuxsHlmYiUBqaY51Xieh9gThj1m6FVZjw2deQGidJ0JcwEvhKf98kumll/d5/m2hg0Bbld7wKGWiNWtF+sLk0E9jbFBa6KjJgyHkbvpD0MhTZk+JSdfjE398w9Rkk7tcR02sCqMaMfjpRRGd83sxWjTbanfKVlIrCXKQ5+ZWTQFJ+/qp3Ho30Qq8f11SP9EmkrFPayo5ltGOWtFJPcIisBs8xseeCgwm/7mdgYbc76Jx1lewCwFjA90o8GfoFcmtnqyl1E7G3cU9X+nXD3fyODbZu4lD8dZyawVdRvAxr9oMizwDoWJ4mZ2QrWhfjhXF0WAu+Y4oKXIbyawYbu/ggyYt9BbttW77UP8EZMVA6jMUgU7yvVZ2a2PjDL3S8FrqRcruvK1nhg0Z4ba33q0anIpb1oI6q7z0YTjE1cBwIcCkw0bfrr5+73IoNqDeQpcOQ6Px+Y6u5ZDH0fIJtkHNaNZ8mzPzDS3dd39w3cvR8aW7ZFoWubhTz0peHJqNIXdcuv6v916APMDmNmFyRHZVT17d7IgHsrnmOfivu7jbs/A7yIvBy4+1vIc5ztHRiCwpPKaDWG9EETR2gsIGRcjiY3k9z93a7U2bTn5yJgRK6c10MG8+UU33FVujxl92Sr6buh0JSMD9Ek5mgzy95NsX8PyK0OLy4Poz0NfUO/H0D1u2nFs2h1un/Usz+yK542s9VM+2QytkT9C+TpOc/k0cvoHXk0u68Ozdo6zwK0V3DZLuTdgZiQHhN13K8wju0f49AmUYcZhdvzY+ZuwIycJzjPkpSFbtETHoAzgFFmlgnfG6jTYGa3A0e4e3GT7zDgT9EQH6Jd9nW5BLmRn9IYwmxkqLXiSSTkT6MX+FAXyixyGnrm/YH7aCi124AjTe685+gYPjMSubIfR89blS7P6chVPYuOLuYTo2MtRO7uu929PQT0kWiXucBB7v6Ymf0NxZvNpGJFPyYjr6PBB6S096ZziBQoNOASMzsBxXv2JPl2OrNG+j7AzWa2IprQHl+RbjJyla8LnObub5pcv4uIthoVaQEudve2kNPMFXp7RTtPM7PfIaPoY+R2HErnttoDWODuXfF6dSLqPxQYbY0NTifTeTL9KyQ7LyPZzyuc6UgePg8cGTI0IurbhvZfHJpbOb4JxZbnNz6egcLR2lD7T6def8w4HG2anRt1yQb/+1G/aot6lx6r6O4LzGxf4IIwipYD/ojCUrrKz9Gq5MtI7leM6+eZ2ReQEX93eFPeRJtWp6CVzSIXAn81swORAZ6tSE0Blo2+PxIZLmX6bGfgeDP7CHkPyo5kvBSFPr2C4kir+AlwsWnT2XJIZ5VtwgcgF+6xiBjgZwLbmNk09J6z/K6Ptl8GxZtnIZGjkZGSr/vpKHzrVSSXa8X1ccBNZrZ3s7oVODDyyzMG9cdjTSF8bWg19sn4vUpftNJpTfs/UOcI1muAW0O3PUnFwldV33b328zsKtQfXqJrJ6x1hbPoeMLIECQ/vWm897J6txpDzgX+YtpseV/h3kfN7ANy4T8x6TjY3cvkYTVTuNIKaGJ6BZAdTT4ChZkNQUZXdkrY40DvuO+yJuny9XrNtMG2DcWSjwBuMbPJkd+/CunfM7M9UL98H+0XWRcdTGJo8+ZgeoDwyp5JY4I11t3v6o5R6e4fmI4AHhUy1w4cFt6ZvsApoQPnI5k/Iu672eTlmxD94V2kq+9F+rL0vhIONXkEM45A4WaVbZ2ru5vZlUCbmT2K5KwZF5i8ZxlbI/k51t3fMLNhSE6zgxtmoDZeAxgWXsR8fqegMWxIPOPQinJ7RBZMR/DvjUI5XwUu8sIpUZHuOGCuu1eG1GUbObpNKNJPYpa2HTKauuPCSySWCNHZ33L3bp2nHytMk9y99tnEiXqY2cruPi8+n4I2OZ7wKVcrUSCMilXCa7NUkfr/ksd0xv09wABfXKMkkeghTCcEXuv/42NDc+X3Al5191peRjM7CtjIax6p2hMhQOuhOPSpaAY+rAfyTCT+L4iB6QF0Qk2i5xlscdQlip1tdo534lPCdYTf0mj8p/6/hAnP1CTk5UjGfyLRwIF2y/0hsCpMm+iHoxOFarHYHoBEIpFIJBKJRCLx2aEnPACJRCKRSCQSiUTiM0KaACQSiUQikUgkEksRaQKQSCQSiUQikUgsRaQJQCKRSCQSiUQisRSRJgCJRCKRSCQSicRSRJoAJBKJRCKRSCQSSxH/BS8J2UVd20Y+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21a866ed198>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the image libraries we need\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from imageio import imread\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "def plot_terrain(file_number=2):\n",
    "    # caption the plot\n",
    "    if file_number == 2:\n",
    "        area_name = 'the M酶svatn Austfjell area'\n",
    "        plot_number = 8\n",
    "    else:\n",
    "        raise ValueError(\"There is only the M酶svatn area, which is the second file, so only call it with '2'.\")\n",
    "    txt = 'Fig. ' + str(plot_number) + ' Terrain data used within this project. The above figure illustrates ' + area_name + ' in Norway. Data taken from USGS EarthExplorer [1].'\n",
    "    # read the file and generate the data\n",
    "    fileName = os.path.join(os.getcwd(), 'SRTM_data_Norway_' + str(file_number) + '.tif')\n",
    "    image = Image.open(fileName, mode='r')\n",
    "    image.mode = 'I'\n",
    "    #print(image.size) # width, height\n",
    "    x = np.linspace(0, 1, image.size[0])\n",
    "    y = np.linspace(0, 1, image.size[1])\n",
    "    X,Y = np.meshgrid(x,y)\n",
    "    Z = np.array(image)\n",
    "    Z = Z - np.min(Z)\n",
    "    Z = Z / np.max(Z)\n",
    "    \n",
    "    # plot the figure\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(X,Y,Z,cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    ax.set_zlim(-0.10, 1.20)\n",
    "    ax.set_title(\"Terrain Data of \" + area_name + \", Norway\")\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    ax.view_init(30, 45+90)\n",
    "    fig.text(.1,.1,txt)\n",
    "    #plt.savefig(os.path.join(plot_dir, 'terrain_' + area_name + '.png'), transparent=True, bbox_inches='tight')\n",
    "    plt.show()\n",
    "plot_terrain(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terrain_to_x_y_z(self, filenumber=2):\n",
    "    #setting up data points for real data\n",
    "    z = imread('SRTM_data_Norway_' + str(filenumber) + '.tif')\n",
    "    x = np.linspace(0, 1, len(z[1])).reshape(len(z[1]), 1)\n",
    "    y = np.linspace(0, 1, len(z)).reshape(len(z),1) # normalized data from 0 to 1\n",
    "    terrain_x, terrain_y = np.meshgrid(x,y)\n",
    "    terrain_z = z/np.max(z) # normalize\n",
    "    return terrain_x, terrain_y, terrain_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5188320, 2) (5188320,)\n",
      "(1297081, 2) (1297081,)\n"
     ]
    }
   ],
   "source": [
    "## massaging the Terrain data to the right shape\n",
    "mosvatn_x, mosvatn_y, mosvatn_z = terrain_to_x_y_z(2)\n",
    "X = np.vstack([mosvatn_x.ravel(), mosvatn_y.ravel()]).T\n",
    "y = np.reshape(mosvatn_z, X.shape[0])\n",
    "\n",
    "## for our split we will have to enable the shuffling, since it could just learn the position by time.\n",
    "## to get random samples from the entire space and not spatial correlated once we just enable the shuffling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# bring both training data to same shape \n",
    "# y_train2 = np.vstack([y_train, y_train]).T\n",
    "# y_test2 =  np.vstack([y_test, y_test]).T\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is from [project 1](https://github.com/lenlehm/Regression-and-Resampling). The function used is in the Appendix, but is the exact copy of project 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate degree, now at: 0 out of: 6\n",
      "Evaluate degree, now at: 1 out of: 6\n",
      "Evaluate degree, now at: 2 out of: 6\n",
      "Evaluate degree, now at: 3 out of: 6\n",
      "Evaluate degree, now at: 4 out of: 6\n",
      "Evaluate degree, now at: 5 out of: 6\n",
      "Evaluate degree, now at: 6 out of: 6\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lamda = 0.001\n",
    "mse_OLS_train, mse_OLS_test, r2_OLS_train, r2_OLS_test, mse_ridge_train, mse_ridge_test, r2_ridge_train, r2_ridge_test, mse_lasso_train, mse_lasso_test, r2_lasso_train, r2_lasso_test = Regression_Methods(X, y, lamda=lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Scores for the Regression methods, Lasso and Ridge used a 位 = 0.001\n",
      "\n",
      "|   Degree |    OLS Test |   OLS Train |   RIDGE Test |   RIDGE Train |   LASSO Test |   LASSO Train |\n",
      "|----------|-------------|-------------|--------------|---------------|--------------|---------------|\n",
      "|        0 | 0.0604247   | 0.06044     |  0.0604247   |   0.06044     |  0.0604247   |   0.06044     |\n",
      "|        1 | 2.56953e-29 | 2.57137e-29 |  1.6215e-19  |   1.62231e-19 |  1.65412e-05 |   1.65453e-05 |\n",
      "|        2 | 2.79996e-28 | 2.80209e-28 |  2.32916e-18 |   2.33249e-18 |  1.65412e-05 |   1.65453e-05 |\n",
      "|        3 | 1.26622e-26 | 1.26865e-26 |  1.41153e-17 |   1.41558e-17 |  1.65412e-05 |   1.65453e-05 |\n",
      "|        4 | 8.30612e-25 | 8.31962e-25 |  5.16924e-17 |   5.19319e-17 |  1.65412e-05 |   1.65453e-05 |\n",
      "|        5 | 1.13085e-22 | 1.13255e-22 |  1.49742e-16 |   1.50455e-16 |  1.65412e-05 |   1.65453e-05 |\n",
      "|        6 | 4.21588e-20 | 4.21981e-20 |  3.96656e-16 |   3.97967e-16 |  1.65412e-05 |   1.65453e-05 |\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE Scores for the Regression methods, Lasso and Ridge used a 位 = {}\\n\".format(lamda))\n",
    "print(tabulate([ [i, olsTest, olsTrain, ridgeTest, ridgeTrain, lassoTest, lassoTrain] for i, olsTest, olsTrain, ridgeTest, ridgeTrain, lassoTest, lassoTrain in zip(np.arange(len(mse_OLS_test)), mse_OLS_test, mse_OLS_train, mse_ridge_test, mse_ridge_train, mse_lasso_test, mse_lasso_train)], headers=['Degree', 'OLS Test', 'OLS Train','RIDGE Test','RIDGE Train','LASSO Test', 'LASSO Train'], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Scores for the Regression methods, Lasso and Ridge used a 位 = 0.001\n",
      "\n",
      "|   Degree |     OLS Test |   OLS Train |   RIDGE Test |   RIDGE Train |   LASSO Test |   LASSO Train |\n",
      "|----------|--------------|-------------|--------------|---------------|--------------|---------------|\n",
      "|        0 | -5.17956e-07 |           0 | -5.17956e-07 |  -8.88178e-16 | -5.17956e-07 |      0        |\n",
      "|        1 |  1           |           1 |  1           |   1           |  0.999726    |      0.999726 |\n",
      "|        2 |  1           |           1 |  1           |   1           |  0.999726    |      0.999726 |\n",
      "|        3 |  1           |           1 |  1           |   1           |  0.999726    |      0.999726 |\n",
      "|        4 |  1           |           1 |  1           |   1           |  0.999726    |      0.999726 |\n",
      "|        5 |  1           |           1 |  1           |   1           |  0.999726    |      0.999726 |\n",
      "|        6 |  1           |           1 |  1           |   1           |  0.999726    |      0.999726 |\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Scores for the Regression methods, Lasso and Ridge used a 位 = {}\\n\".format(lamda))\n",
    "print(tabulate([ [i, olsTest, olsTrain, ridgeTest, ridgeTrain, lassoTest, lassoTrain] for i, olsTest, olsTrain, ridgeTest, ridgeTrain, lassoTest, lassoTrain in zip(np.arange(len(r2_OLS_test)), r2_OLS_test, r2_OLS_train, r2_ridge_test, r2_ridge_train, r2_lasso_test, r2_lasso_train)], headers=['Degree', 'OLS Test', 'OLS Train','RIDGE Test','RIDGE Train','LASSO Test', 'LASSO Train'], tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding input layer with 128 neurons, using relu\n",
      "Adding layer with 64 neurons using relu\n",
      "Adding output layer with 2 outputs and identity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8924ff6014b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m        \u001b[0mval_stepwidth\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m        \u001b[0moptimizer\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m        lmbda                = 0.0001)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nNeural Network Accuracy on Test data (20% split): {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Studium_Robotics (M.Sc.)\\03_Semester 3 - Oslo ERASMUS\\01_Applied Data Analysis and Machine Learning\\Project 2\\neuralNet.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(self, x, target, learning_rate, epochs, batch_size, val_size, val_stepwidth, optimizer, lmbda)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                 \u001b[0mbatch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m                 \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_batch\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \"\"\"\n\u001b[0;32m    342\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_n_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## STANDARDIZED VERSION \n",
    "nn = NeuralNetwork(inputs             = X_train.shape[1],\n",
    "                   outputs            = 2, \n",
    "                   cost_function_name = 'mse')\n",
    "\n",
    "## NN Setup of: 128, 128, 128, 64 Neurons in the hidden layer and 2 neurons as output layer (binary classification)\n",
    "## All the layers are activated with the ReLU activation function, which mitigates the vanishing gradient problem\n",
    "nn.addLayer(activations = 'relu', neurons = 128)\n",
    "nn.addLayer(activations = 'relu', neurons = 64)\n",
    "nn.addLayer(activations = 'identity', neurons = 2, output = True)\n",
    "\n",
    "nn.train_network(X_train.T, y_train.T,\n",
    "       batch_size           = 64,\n",
    "       learning_rate        = 0.001,\n",
    "       epochs               = 50,\n",
    "       val_size             = 0.2,\n",
    "       val_stepwidth        = 10,\n",
    "       optimizer            = 'adam',\n",
    "       lmbda                = 0.0001)\n",
    "\n",
    "print(\"\\nNeural Network Accuracy on Test data (20% split): {}\\n\".format(nn.accuracy(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4150656 samples, validate on 1037664 samples\n",
      "Epoch 1/10\n",
      "4150656/4150656 [==============================] - 666s 161us/step - loss: 0.0587 - mean_squared_error: 0.0080 - val_loss: 0.0563 - val_mean_squared_error: 0.0071\n",
      "Epoch 2/10\n",
      "1404600/4150656 [=========>....................] - ETA: 12:57 - loss: 0.0544 - mean_squared_error: 0.0070"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=2, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Configure the model and start training\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "score = metrics.mean_squared_error(y_train.ravel(), train_pred_OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the table above, OLS outperforms Ridge and Lasso Regression siginificantly. <br /> \n",
    "All regression methods share their effect on the polynomial degree: The lower the degree the better the MSE score (excluding the 0th degree). <br /> \n",
    "OLS achieves by far the best results, however Ridge also gets great results and Lasso Regression is not doing excellent on this dataset. <br /> \n",
    "Since OLS and Ridge are doing a quite comparable job and Lasso is doing worse, the figure below (Fig. 5) seems almost constant over the degrees, but considering the table above, each degree added will reduce the MSE score by approximately a tenth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot that stuff\n",
    "txt = \"Fig. 9 Different Regression methods and their respective MSE score on the Terrain data set displayed\\n Ridge and Lasso utilized a regularization parameter 位 = \" + str(lamda)\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "plt.plot(np.arange(max_poly_degree)[1:], mse_OLS_test[1:], label=\"Test Accuracy OLS\")\n",
    "plt.plot(np.arange(max_poly_degree)[1:], mse_ridge_test[1:], label=\"Test Accuracy RIDGE\")\n",
    "plt.plot(np.arange(max_poly_degree)[1:], mse_lasso_test[1: ], label=\"Test Accuracy LASSO\")\n",
    "plt.title(\"MSE Accuracies across the different Regression techniques\")\n",
    "plt.ylabel(\"Accuracy Score (MSE)\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.xticks(np.arange(max_poly_degree))\n",
    "plt.legend()\n",
    "fig.text(.1, 0, txt)\n",
    "plt.savefig(os.path.join(plot_dir, 'Regression on Terrain.png'), transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work expanded on Logistic Regression and Neural Networks, especially Multi-Layered Perceptrons (MLP). <br />\n",
    "As seen in the first part of this work, where credit card data has been successfully classified, MLP slightly outperformed Logistic Regression, but took longer to converge along with a tedious hyperparameter search. <br /> \n",
    "Taken into account that my Notebook has limited computing power - Intel Core i7, 7th generation, 16GB RAM and 512GB SSD - there was the bare minimum of hyperparameter search done for the MLP. <br />\n",
    "\n",
    "Hyperparameters are essential parameters to this algorihtm, which need to be tuned to end up in an optimal solution. These hyperparameters include the learning rate $\\tau$, batch sizes, regularization parameter $\\lambda$, amount of hidden layers, amount of neurons within each layer, activation functions, epochs and optimizer (SGD or Adam). <br /> \n",
    "Hence, there is even more potential for MLP with the given credit card dataset, when performing a hyperparameter search by using either random search or Bayesian Analysis. [3] <br />\n",
    "\n",
    "In conclusion Logistic Regression is to be prefered for this classification dataset due to its simplicity and time benefits by almost the same accuracy. <br /> \n",
    "\n",
    "The neat benefit of neural networks is that they are also useful for Regression tasks, as shown in the second part of the project. By only changing the Loss function from Cross Entropy to MSE, the same algorithm can now predict a continuous value as seen in the terrain dataset above. <br /> \n",
    "\n",
    "Comparing the Neural Network as a regressor to the common regression methods such as Ordinary Least Squares, Ridge and Lasso Regression, one can tell that the common regression methods, especially OLS and Ridge Regression achieved remarkably good results (see the Table four cells above). <br />\n",
    "Nevertheless, the neural network still outperformed both of those Regression methods, since it can model even more complex shapes. <br />\n",
    "\n",
    "To put it in a nutshell, neural networks are flexible and adaptive tools, which in turn need a lot of fine tuning and hyperparameter search. For both tasks, Regression and Classification, it challenged the performance of its competitors such as OLS, Ridge and Logistic Regression. <br />\n",
    "These benefits however, come at the cost of computational power and complexity for searching optimal parametrization values. <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Conclusions\n",
    "\n",
    "This work shows that it is possible to get similiar results for Logistic Regression and Artificial Neural Network of Yeh and Lien. [4] <br /> \n",
    "When it comes to classify the credit card data, the neural networks were similiar in accuracy than its rival, Logistic Regression, only with the drawback of a more complex implementation as well as sensitive hyperparameters. <br /> \n",
    "\n",
    "Considering Regression on the terrain dataset, Neural Networks are also comparable to the OLS and Ridge Regression in terms of accuracy. Again, its complexity and sensitivity are more of a problem for these easier problems. <br /> \n",
    "\n",
    "## ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Further Work \n",
    "\n",
    "As the section about the Classification dataset in the Introduction already states, the dataset itself has some values that are not documented. Manually checking and cleaning the dataset that it follows its documentation would most probably lead to better results. <br /> \n",
    "Going further, one could perform feature engineering to enhance the dataset by adding some more predictor variables to it and thus boosting the accuracies for the classification algorithms at hand. <br /> \n",
    "Leveraging this technique along with utilizing other classification algorithms such as Gradient Boosting Machines, i.e. AdaBoost, it would be interesting to compare those methods in terms of accuracies and performance. According to the authors [4], these methods revealed promising results and digging deeper into these techniques could yield even better results. <br /> \n",
    "Another interesting approach would be combining this dataset with another credit card default dataset, such as the [German one from before 2000](https://github.com/olethrosdc/ml-society-science/tree/master/data/credit) to end up in one huge dataset, or comparing the different nations to one another and deriving some insights with respect to the banks handing out credits. <br /> \n",
    "\n",
    "When it comes to Regression, it would be more interesting to compare the common Regression methods to the neural networks for higher dimensional datasets such as the [Boston House prices](https://www.kaggle.com/vikrishnan/boston-house-prices). Since the common Regression methods could pretty much perfectly predict the elevation of the terrain data, having a multi-dimensional dataset would definitely reveal some more interesting properties for both, Neural Networks and Ridge Regression. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Shanahan, M. (2015). *The Techonological Singularity*. Camebridge: The MIT Press <br />\n",
    "[2] Saintano, M. (2015). *Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence*. Retrieved from [Observer](https://observer.com/2015/08/stephen-hawking-elon-musk-and-bill-gates-warn-about-artificial-intelligence/) on 8th October 2019 <br />\n",
    "[3] Baeuml, B. (2019). *Advanced Deep Learning for Robotics*. Munich: [Lecture at Technical University Munich (TUM)](https://github.com/bbaeuml/ss19-advanced-dl-for-robotics/blob/master/docs/adlr-2-advanced-networks.pdf), Summer Term 2019 (password: TUM19ADLR) <br />\n",
    "[4] Yeh, C. et al. (2009). *The comparisons of data mining techniques for the predictive\n",
    "accuracy of probability of default of credit card clients*. ELSEVIER, [Source](https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf) Retrieved from ELSEVIER on 8th October 2019 <br />\n",
    "[5] Hjorth-Jensen, M. (2019). *Applied Data Analysis and Machine Learning*. Oslo: Lecture hold at University of Oslo (UiO), [Source](https://compphysics.github.io/MachineLearning/doc/web/course.html) <br />\n",
    "[6] Hastie, T. et al. (2009). *The Elements of Statistical Learning*. Camebridge: Springer <br />\n",
    "[7] Knoll, A. (2019). *Cognitive Systems*. Lecture hold at Technical University Munich (TUM), [Source](https://github.com/lenlehm/Classification-and-Regression/blob/master/E04-Deep_Learning.pdf) included in Github Repository <br />\n",
    "[8] Murphy, K. P. et al. (2007). *Machine Learning: A probabilistic Perspective*. Camebridge: The MIT Press. <br />\n",
    "[9] Diederik K. et al. (2015). *Adam: A Method for Stochastic Optimization*. Cornell University, ArXiv: https://arxiv.org/abs/1412.6980, retrieved 8th October, 2019 <br />\n",
    "[10] Glorot, X. et al. (2010). *Understanding the difficulty of training deep feedforward neural networks*. Cornell University, ArXiv: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf, retrieved 8th October, 2019. <br />\n",
    "[10] Bishop, C.M. et al. (2011). *Pattern Recognition and Machine Learning*. Cambridge: Springer. <br />\n",
    "[11] EarthExplorer website: https://earthexplorer.usgs.gov/, Used Dataset: Norway, last visited 05.09.2019 <br />\n",
    "[12] Grover P. (2018): *5 Regression Loss Functions All Machine Learners Should Know*. Medium: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0, retrieved 28th October, 2019. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "\n",
    "Some more plots for the terrain data $\\lambda$ parametrization. <br /> \n",
    "All of the plots below are showing the Mean Squared Error (MSE) of Ridge or Lasso, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the code, which has been used in project 1 only without noisy targets. <br />\n",
    "This code was used for the Terrain data Regression Analysis, which was shown towards the end of this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from RegressionMethods import CreateDesignMatrix_X\n",
    "from sklearn import linear_model, metrics\n",
    "\n",
    "def Regression_Methods(X, y, max_poly_degree=7, lamda=0.001):\n",
    "    '''\n",
    "    INPUT:\n",
    "    ----------\n",
    "    X: numpy ndarray or pandas Dataframe\n",
    "        entire input data which it counts to analyse using regression\n",
    "    y: numpy array or pandas Series\n",
    "        corresponding targets for the input data rows\n",
    "    max_ploy_degree: integer (default = 7 - too long computation time and no accuracy benefit)\n",
    "        indicator of which polynomial degree to fit to the dataset\n",
    "    lamda: float (default = 0.001 - empirical test have proven this to be quite good)\n",
    "        regularization strength parameter for Ridge and Lasso Regression\n",
    "    \n",
    "    OUTPUT:\n",
    "    ----------\n",
    "    train/test_metric: numpy ndarray\n",
    "        Metrics (either MSE or R2) for the corresponding regression method\n",
    "        There are in total 12 return values with train and test accuracy for all the 3 methods\n",
    "    '''\n",
    "    # OLS train and test scores\n",
    "    mse_OLS_train = np.zeros(max_poly_degree)\n",
    "    mse_OLS_test  = np.zeros(max_poly_degree)\n",
    "    r2_OLS_train  = np.zeros(max_poly_degree)\n",
    "    r2_OLS_test   = np.zeros(max_poly_degree)\n",
    "\n",
    "    # Ridge train and test scores\n",
    "    mse_ridge_train = np.zeros(max_poly_degree)\n",
    "    mse_ridge_test  = np.zeros(max_poly_degree)\n",
    "    r2_ridge_train  = np.zeros(max_poly_degree)\n",
    "    r2_ridge_test   = np.zeros(max_poly_degree)\n",
    "\n",
    "    # Lasso train and test scores\n",
    "    mse_lasso_train = np.zeros(max_poly_degree)\n",
    "    mse_lasso_test  = np.zeros(max_poly_degree)\n",
    "    r2_lasso_train  = np.zeros(max_poly_degree)\n",
    "    r2_lasso_test   = np.zeros(max_poly_degree)\n",
    "    \n",
    "    # Split the Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # bring both training data to same shape \n",
    "    y_train = np.vstack([y_train, y_train]).T\n",
    "    y_test =  np.vstack([y_test, y_test]).T\n",
    "\n",
    "    # now check the proper parametrization and benchmark the algorithms against each other\n",
    "    for degree in range(max_poly_degree):\n",
    "        # put everything that DOES NOT depend on lamda in this loop --> entire OLS & design Matrices\n",
    "        print(\"Evaluate degree, now at: \" + str(degree) + ' out of: ' + str(max_poly_degree-1))\n",
    "        # Train design Matrix to fit our regression model\n",
    "        X = CreateDesignMatrix_X(X_train.ravel(), y_train.ravel(), degree)\n",
    "        # Test design Matrix to evaluate the Test Set\n",
    "        designX_test = CreateDesignMatrix_X(X_test.ravel(), y_test.ravel(), degree)\n",
    "\n",
    "        beta_OLS = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y_train.ravel())#reshape(-1,1))\n",
    "        beta_ridge = np.dot(np.linalg.inv(np.dot(np.transpose(X),X) + lamda * np.eye(X.shape[1])), np.dot(np.transpose(X), y_train.ravel()))#reshape(-1, 1)))\n",
    "\n",
    "        ## OLS prediction\n",
    "        train_pred_OLS = X.dot(beta_OLS)\n",
    "        test_pred_OLS  = designX_test.dot(beta_OLS)\n",
    "\n",
    "        ## Ridge Prediction\n",
    "        train_pred_ridge = X.dot(beta_ridge)\n",
    "        test_pred_ridge = designX_test.dot(beta_ridge)\n",
    "\n",
    "        ## LASSO\n",
    "        polynom    = PolynomialFeatures(degree=degree)\n",
    "        XY         = polynom.fit_transform(np.array([X_train.ravel(), y_train.ravel()]).T)\n",
    "        lasso      = linear_model.Lasso(fit_intercept=True, alpha=lamda)\n",
    "        lasso.fit(XY, y_train.reshape(-1, 1))\n",
    "        test_feed  = polynom.fit_transform(np.array([X_test.ravel(), y_test.ravel()]).T)\n",
    "        train_feed = polynom.fit_transform(np.array([X_train.ravel(), y_train.ravel()]).T)\n",
    "        ## prediction\n",
    "        train_pred_lasso = lasso.predict(train_feed)\n",
    "        test_pred_lasso  = lasso.predict(test_feed)    \n",
    "\n",
    "\n",
    "        ## GET THE SCORES - MSE AND R2 ---------------- OLS\n",
    "        mse_OLS_train[degree] = metrics.mean_squared_error(y_train.ravel(), train_pred_OLS)\n",
    "        ## OLS is doing this one:  np.mean( np.mean( (train_target.ravel() - train_pred_OLS)**2, axis=1, keepdims=True) )\n",
    "        mse_OLS_test[degree]  = metrics.mean_squared_error(y_test.ravel(), test_pred_OLS)\n",
    "\n",
    "        r2_OLS_train[degree]  = metrics.r2_score(y_train.ravel(), train_pred_OLS)\n",
    "        r2_OLS_test[degree]   = metrics.r2_score(y_test.ravel(), test_pred_OLS)\n",
    "\n",
    "        ## RIDGE\n",
    "        mse_ridge_train[degree] = metrics.mean_squared_error(y_train.ravel(), train_pred_ridge)\n",
    "        mse_ridge_test[degree]  = metrics.mean_squared_error(y_test.ravel(), test_pred_ridge)\n",
    "        r2_ridge_train[degree]  = metrics.r2_score(y_train.ravel(), train_pred_ridge)\n",
    "        r2_ridge_test[degree]   = metrics.r2_score(y_test.ravel(), test_pred_ridge)\n",
    "\n",
    "        ## LASSO\n",
    "        mse_lasso_train[degree] = metrics.mean_squared_error(y_train.ravel(), train_pred_lasso)\n",
    "        mse_lasso_test[degree]  = metrics.mean_squared_error(y_test.ravel(), test_pred_lasso)\n",
    "        r2_lasso_train[degree]  = metrics.r2_score(y_train.ravel(), train_pred_lasso)\n",
    "        r2_lasso_test[degree]   = metrics.r2_score(y_test.ravel(), test_pred_lasso)\n",
    "        \n",
    "    return mse_OLS_train, mse_OLS_test, r2_OLS_train, r2_OLS_test, mse_ridge_train, mse_ridge_test, r2_ridge_train, r2_ridge_test, mse_lasso_train, mse_lasso_test, r2_lasso_train, r2_lasso_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "def plot_roc_curve(FPR, TPR, plot_number, text=\"ROC curve\"):  \n",
    "    txt = \"Fig. \" + str(plot_number) + text\n",
    "    fig = plt.figure(figsize=(9, 7))\n",
    "    plt.plot(FPR, TPR, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    fig.text(.1, 0,txt)\n",
    "    plt.savefig(os.path.join(plot_dir, 'ROC ' + str(plot_number) + '.png'), transparent=True, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
